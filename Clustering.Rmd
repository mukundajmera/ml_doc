---
title: "Clustering"
author: "Mukund Ajmera"
date: "August 17, 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
#Type of Clustering
* **Hard Clustering**
In hard clustering, each data point either belongs to a cluster completely or not.
For example, in the above example each customer is put into one group out of the 10 groups.
* **Soft Clustering**
Instead of putting each data point into a separate cluster, a probability or likelihood of that data point to be in those clusters is assigned.
For example, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store

#Types of clustering algorithms

The means that can be used for achieving this goal are plenty. Every methodology follows a different set of rules for defining the ‘similarity’ among data points. 
1. **Connectivity models** As per the name suggests, these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. These models can follow two approaches. First approach, they start with classifying all data points into separate clusters & then aggregating them as the distance decreases. Second approach, all data points are classified as a single cluster and then partitioned as the distance increases. Examples hierarchical clustering algorithm and its variants.

2. **Centroid models**: These are iterative clustering algorithms in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters.
Example K-Means clustering algorithm is a popular algorithm that falls into this category. In these models, the no. of clusters required at the end have to be mentioned beforehand, which makes it important to have prior knowledge of the dataset.

3. **Distribution models**: These clustering models are based on the notion of how probable is it that all data points in the cluster belong to the same distribution. These models often suffer from overfitting.
Example of these models is Expectation-maximization algorithm which uses multivariate normal distributions.

4. **Density Models**: These clustering models search the data space for areas of varied density of data points in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Examples of density models are DBSCAN and OPTICS.


#K - Means
This algorithm works in these 5 steps :

1. Specify the desired number of clusters K : Let us choose k=2 for these 5 data points in 2-D space.

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-2.png)

2. Randomly assign each data point to a cluster : Let’s assign three points in cluster 1 shown using red color and two points in cluster 2 shown using grey color.

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-2-1.png)

3. Compute cluster centroids : The centroid of data points in the red cluster is shown using red cross and those in grey cluster using grey cross.

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-3.png)

4. Re-assign each point to the closest cluster centroid : Note that only the data point at the bottom is assigned to the red cluster even though its closer to the centroid of grey cluster. Thus, we assign that data point into grey cluster

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-4.png)

5. Re-compute cluster centroids : Now, re-computing the centroids for both the clusters.

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-5.png)

* **Note** Repeat steps 4 and 5 until no improvements are possible : Similarly, we’ll repeat the 4th and 5th steps until we’ll reach global optima. When there will be no further switching of data points between two clusters for two successive repeats. It will mark the termination of the algorithm if not explicitly mentioned.

##Hierarchical Clustering
Hierarchical clustering is an algorithm that builds hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left.

The results of hierarchical clustering can be shown using **dendrogram**.

Example 
![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-6.png)


###Two important things that you should know about hierarchical clustering are:

1. This algorithm has been implemented above using bottom up approach. It is also possible to follow top-down approach starting with all data points assigned in the same cluster and recursively performing splits till each data point is assigned a separate cluster.
2. The decision of merging two clusters is taken on the basis of closeness of these clusters. There are multiple metrics for deciding the closeness of two clusters :
        * Euclidean distance: ||a-b||2 = √(Σ(ai-bi))
        * Manhattan distance: ||a-b||1 = Σ|ai-bi|
        * Maximum distance:||a-b||INFINITY = maxi|ai-bi|
        * Mahalanobis distance: √((a-b)T S-1 (-b))   {where, s : covariance matrix}
        
#Applications of Clustering

Some of the most popular applications of clustering are:

* Recommendation engines
* Market segmentation
* Social network analysis
* Search result grouping
* Medical imaging
* Image segmentation
* Anomaly detection
        
##Resources
1. Analytics Vidhya [link](https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)
2. Tutorials point [link](https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_algorithms.htm)

