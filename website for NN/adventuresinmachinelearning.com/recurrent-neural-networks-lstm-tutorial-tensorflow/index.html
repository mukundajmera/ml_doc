<!DOCTYPE html>
<html class="no-js" lang="en-AU" prefix="og: http://ogp.me/ns#">
<head>
<script async src="http://adventuresinmachinelearning.com/wp-content/cache/abtf/proxy/98/31/1e/98311e1088e466ab7425e6580ae2c43e.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-8092137086954180",
    enable_page_level_ads: true
  });
</script>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="http://adventuresinmachinelearning.com/xmlrpc.php" />
<title>Recurrent neural networks and LSTM tutorial in Python and TensorFlow - Adventures in Machine Learning</title>
<script rel="abtf">!function(){function a(a,b,c){return a.call.apply(a.bind,arguments)}function b(a,b,c){if(!a)throw Error();if(2<arguments.length){var d=Array.prototype.slice.call(arguments,2);return function(){var c=Array.prototype.slice.call(arguments);return Array.prototype.unshift.apply(c,d),a.apply(b,c)}}return function(){return a.apply(b,arguments)}}function c(d,e,f){return c=Function.prototype.bind&&-1!=Function.prototype.bind.toString().indexOf("native code")?a:b,c.apply(null,arguments)}function d(a,b){this.a=a,this.m=b||a,this.c=this.m.document}function e(a,b,c,d){if(b=a.c.createElement(b),c)for(var e in c)c.hasOwnProperty(e)&&("style"==e?b.style.cssText=c[e]:b.setAttribute(e,c[e]));return d&&b.appendChild(a.c.createTextNode(d)),b}function f(a,b,c){a=a.c.getElementsByTagName(b)[0],a||(a=document.documentElement),a.insertBefore(c,a.lastChild)}function g(a){a.parentNode&&a.parentNode.removeChild(a)}function h(a,b,c){b=b||[],c=c||[];for(var d=a.className.split(/\s+/),e=0;e<b.length;e+=1){for(var f=!1,g=0;g<d.length;g+=1)if(b[e]===d[g]){f=!0;break}f||d.push(b[e])}for(b=[],e=0;e<d.length;e+=1){for(f=!1,g=0;g<c.length;g+=1)if(d[e]===c[g]){f=!0;break}f||b.push(d[e])}a.className=b.join(" ").replace(/\s+/g," ").replace(/^\s+|\s+$/,"")}function i(a,b){for(var c=a.className.split(/\s+/),d=0,e=c.length;d<e;d++)if(c[d]==b)return!0;return!1}function j(a){if("string"==typeof a.f)return a.f;var b=a.m.location.protocol;return"about:"==b&&(b=a.a.location.protocol),"https:"==b?"https:":"http:"}function k(a){return a.m.location.hostname||a.a.location.hostname}function l(a,b,c){function d(){j&&g&&h&&(j(i),j=null)}b=e(a,"link",{rel:"stylesheet",href:b,media:"all"});var g=!1,h=!0,i=null,j=c||null;da?(b.onload=function(){g=!0,d()},b.onerror=function(){g=!0,i=Error("Stylesheet failed to load"),d()}):setTimeout(function(){g=!0,d()},0),f(a,"head",b)}function m(a,b,c,d){var f=a.c.getElementsByTagName("head")[0];if(f){var g=e(a,"script",{src:b}),h=!1;return g.onload=g.onreadystatechange=function(){h||this.readyState&&"loaded"!=this.readyState&&"complete"!=this.readyState||(h=!0,c&&c(null),g.onload=g.onreadystatechange=null,"HEAD"==g.parentNode.tagName&&f.removeChild(g))},f.appendChild(g),setTimeout(function(){h||(h=!0,c&&c(Error("Script load timeout")))},d||5e3),g}return null}function n(){this.a=0,this.c=null}function o(a){return a.a++,function(){a.a--,q(a)}}function p(a,b){a.c=b,q(a)}function q(a){0==a.a&&a.c&&(a.c(),a.c=null)}function r(a){this.a=a||"-"}function s(a,b){this.c=a,this.f=4,this.a="n";var c=(b||"n4").match(/^([nio])([1-9])$/i);c&&(this.a=c[1],this.f=parseInt(c[2],10))}function t(a){return w(a)+" "+(a.f+"00")+" 300px "+u(a.c)}function u(a){var b=[];a=a.split(/,\s*/);for(var c=0;c<a.length;c++){var d=a[c].replace(/['"]/g,"");-1!=d.indexOf(" ")||/^\d/.test(d)?b.push("'"+d+"'"):b.push(d)}return b.join(",")}function v(a){return a.a+a.f}function w(a){var b="normal";return"o"===a.a?b="oblique":"i"===a.a&&(b="italic"),b}function x(a){var b=4,c="n",d=null;return a&&((d=a.match(/(normal|oblique|italic)/i))&&d[1]&&(c=d[1].substr(0,1).toLowerCase()),(d=a.match(/([1-9]00|normal|bold)/i))&&d[1]&&(/bold/i.test(d[1])?b=7:/[1-9]00/.test(d[1])&&(b=parseInt(d[1].substr(0,1),10)))),c+b}function y(a,b){this.c=a,this.f=a.m.document.documentElement,this.h=b,this.a=new r("-"),this.j=!1!==b.events,this.g=!1!==b.classes}function z(a){a.g&&h(a.f,[a.a.c("wf","loading")]),B(a,"loading")}function A(a){if(a.g){var b=i(a.f,a.a.c("wf","active")),c=[],d=[a.a.c("wf","loading")];b||c.push(a.a.c("wf","inactive")),h(a.f,c,d)}B(a,"inactive")}function B(a,b,c){a.j&&a.h[b]&&(c?a.h[b](c.c,v(c)):a.h[b]())}function C(){this.c={}}function D(a,b,c){var d,e=[];for(d in b)if(b.hasOwnProperty(d)){var f=a.c[d];f&&e.push(f(b[d],c))}return e}function E(a,b){this.c=a,this.f=b,this.a=e(this.c,"span",{"aria-hidden":"true"},this.f)}function F(a){f(a.c,"body",a.a)}function G(a){return"display:block;position:absolute;top:-9999px;left:-9999px;font-size:300px;width:auto;height:auto;line-height:normal;margin:0;padding:0;font-variant:normal;white-space:nowrap;font-family:"+u(a.c)+";"+("font-style:"+w(a)+";font-weight:"+(a.f+"00")+";")}function H(a,b,c,d,e,f){this.g=a,this.j=b,this.a=d,this.c=c,this.f=e||3e3,this.h=f||void 0}function I(a,b,c,d,e,f,g){this.v=a,this.B=b,this.c=c,this.a=d,this.s=g||"BESbswy",this.f={},this.w=e||3e3,this.u=f||null,this.o=this.j=this.h=this.g=null,this.g=new E(this.c,this.s),this.h=new E(this.c,this.s),this.j=new E(this.c,this.s),this.o=new E(this.c,this.s),a=new s(this.a.c+",serif",v(this.a)),a=G(a),this.g.a.style.cssText=a,a=new s(this.a.c+",sans-serif",v(this.a)),a=G(a),this.h.a.style.cssText=a,a=new s("serif",v(this.a)),a=G(a),this.j.a.style.cssText=a,a=new s("sans-serif",v(this.a)),a=G(a),this.o.a.style.cssText=a,F(this.g),F(this.h),F(this.j),F(this.o)}function J(){if(null===fa){var a=/AppleWebKit\/([0-9]+)(?:\.([0-9]+))/.exec(window.navigator.userAgent);fa=!!a&&(536>parseInt(a[1],10)||536===parseInt(a[1],10)&&11>=parseInt(a[2],10))}return fa}function K(a,b,c){for(var d in ea)if(ea.hasOwnProperty(d)&&b===a.f[ea[d]]&&c===a.f[ea[d]])return!0;return!1}function L(a){var b,c=a.g.a.offsetWidth,d=a.h.a.offsetWidth;(b=c===a.f.serif&&d===a.f["sans-serif"])||(b=J()&&K(a,c,d)),b?ca()-a.A>=a.w?J()&&K(a,c,d)&&(null===a.u||a.u.hasOwnProperty(a.a.c))?N(a,a.v):N(a,a.B):M(a):N(a,a.v)}function M(a){setTimeout(c(function(){L(this)},a),50)}function N(a,b){setTimeout(c(function(){g(this.g.a),g(this.h.a),g(this.j.a),g(this.o.a),b(this.a)},a),0)}function O(a,b,c){this.c=a,this.a=b,this.f=0,this.o=this.j=!1,this.s=c}function P(a){0==--a.f&&a.j&&(a.o?(a=a.a,a.g&&h(a.f,[a.a.c("wf","active")],[a.a.c("wf","loading"),a.a.c("wf","inactive")]),B(a,"active")):A(a.a))}function Q(a){this.j=a,this.a=new C,this.h=0,this.f=this.g=!0}function R(a,b,d,e,f){var g=0==--a.h;(a.f||a.g)&&setTimeout(function(){var a=f||null,i=e||null||{};if(0===d.length&&g)A(b.a);else{b.f+=d.length,g&&(b.j=g);var j,k=[];for(j=0;j<d.length;j++){var l=d[j],m=i[l.c],n=b.a,o=l;if(n.g&&h(n.f,[n.a.c("wf",o.c,v(o).toString(),"loading")]),B(n,"fontloading",o),n=null,null===ga)if(window.FontFace){var o=/Gecko.*Firefox\/(\d+)/.exec(window.navigator.userAgent),p=/OS X.*Version\/10\..*Safari/.exec(window.navigator.userAgent)&&/Apple/.exec(window.navigator.vendor);ga=o?42<parseInt(o[1],10):!p}else ga=!1;n=ga?new H(c(b.g,b),c(b.h,b),b.c,l,b.s,m):new I(c(b.g,b),c(b.h,b),b.c,l,b.s,a,m),k.push(n)}for(j=0;j<k.length;j++)k[j].start()}},0)}function S(a,b,c){var d=[],e=c.timeout;z(b);var d=D(a.a,c,a.c),f=new O(a.c,b,e);for(a.h=d.length,b=0,c=d.length;b<c;b++)d[b].load(function(b,c,d){R(a,f,b,c,d)})}function T(a,b){this.c=a,this.a=b}function U(a,b,c){var d=j(a.c);return a=(a.a.api||"fast.fonts.net/jsapi").replace(/^.*http(s?):(\/\/)?/,""),d+"//"+a+"/"+b+".js"+(c?"?v="+c:"")}function V(a,b){this.c=a,this.a=b}function W(a,b,c){a?this.c=a:this.c=b+ha,this.a=[],this.f=[],this.g=c||""}function X(a,b){for(var c=b.length,d=0;d<c;d++){var e=b[d].split(":");3==e.length&&a.f.push(e.pop());var f="";2==e.length&&""!=e[1]&&(f=":"),a.a.push(e.join(f))}}function Y(a){if(0==a.a.length)throw Error("No fonts to load!");if(-1!=a.c.indexOf("kit="))return a.c;for(var b=a.a.length,c=[],d=0;d<b;d++)c.push(a.a[d].replace(/ /g,"+"));return b=a.c+"?family="+c.join("%7C"),0<a.f.length&&(b+="&subset="+a.f.join(",")),0<a.g.length&&(b+="&text="+encodeURIComponent(a.g)),b}function Z(a){this.f=a,this.a=[],this.c={}}function $(a){for(var b=a.f.length,c=0;c<b;c++){var d=a.f[c].split(":"),e=d[0].replace(/\+/g," "),f=["n4"];if(2<=d.length){var g,h=d[1];if(g=[],h)for(var h=h.split(","),i=h.length,j=0;j<i;j++){var k;if(k=h[j],k.match(/^[\w-]+$/)){var l=la.exec(k.toLowerCase());if(null==l)k="";else{if(k=l[2],k=null==k||""==k?"n":ka[k],l=l[1],null==l||""==l)l="4";else var m=ja[l],l=m?m:isNaN(l)?"4":l.substr(0,1);k=[k,l].join("")}}else k="";k&&g.push(k)}0<g.length&&(f=g),3==d.length&&(d=d[2],g=[],d=d?d.split(","):g,0<d.length&&(d=ia[d[0]])&&(a.c[e]=d))}for(a.c[e]||(d=ia[e])&&(a.c[e]=d),d=0;d<f.length;d+=1)a.a.push(new s(e,f[d]))}}function _(a,b){this.c=a,this.a=b}function aa(a,b){this.c=a,this.a=b}function ba(a,b){this.c=a,this.f=b,this.a=[]}var ca=Date.now||function(){return+new Date},da=!!window.FontFace;r.prototype.c=function(a){for(var b=[],c=0;c<arguments.length;c++)b.push(arguments[c].replace(/[\W_]+/g,"").toLowerCase());return b.join(this.a)},H.prototype.start=function(){var a=this.c.m.document,b=this,c=ca(),d=new Promise(function(d,e){function f(){ca()-c>=b.f?e():a.fonts.load(t(b.a),b.h).then(function(a){1<=a.length?d():setTimeout(f,25)},function(){e()})}f()}),e=new Promise(function(a,c){setTimeout(c,b.f)});Promise.race([e,d]).then(function(){b.g(b.a)},function(){b.j(b.a)})};var ea={D:"serif",C:"sans-serif"},fa=null;I.prototype.start=function(){this.f.serif=this.j.a.offsetWidth,this.f["sans-serif"]=this.o.a.offsetWidth,this.A=ca(),L(this)};var ga=null;O.prototype.g=function(a){var b=this.a;b.g&&h(b.f,[b.a.c("wf",a.c,v(a).toString(),"active")],[b.a.c("wf",a.c,v(a).toString(),"loading"),b.a.c("wf",a.c,v(a).toString(),"inactive")]),B(b,"fontactive",a),this.o=!0,P(this)},O.prototype.h=function(a){var b=this.a;if(b.g){var c=i(b.f,b.a.c("wf",a.c,v(a).toString(),"active")),d=[],e=[b.a.c("wf",a.c,v(a).toString(),"loading")];c||d.push(b.a.c("wf",a.c,v(a).toString(),"inactive")),h(b.f,d,e)}B(b,"fontinactive",a),P(this)},Q.prototype.load=function(a){this.c=new d(this.j,a.context||this.j),this.g=!1!==a.events,this.f=!1!==a.classes,S(this,new y(this.c,a),a)},T.prototype.load=function(a){function b(){if(f["__mti_fntLst"+d]){var c,e=f["__mti_fntLst"+d](),g=[];if(e)for(var h=0;h<e.length;h++){var i=e[h].fontfamily;void 0!=e[h].fontStyle&&void 0!=e[h].fontWeight?(c=e[h].fontStyle+e[h].fontWeight,g.push(new s(i,c))):g.push(new s(i))}a(g)}else setTimeout(function(){b()},50)}var c=this,d=c.a.projectId,e=c.a.version;if(d){var f=c.c.m;m(this.c,U(c,d,e),function(e){e?a([]):(f["__MonotypeConfiguration__"+d]=function(){return c.a},b())}).id="__MonotypeAPIScript__"+d}else a([])},V.prototype.load=function(a){var b,c,d=this.a.urls||[],e=this.a.families||[],f=this.a.testStrings||{},g=new n;for(b=0,c=d.length;b<c;b++)l(this.c,d[b],o(g));var h=[];for(b=0,c=e.length;b<c;b++)if(d=e[b].split(":"),d[1])for(var i=d[1].split(","),j=0;j<i.length;j+=1)h.push(new s(d[0],i[j]));else h.push(new s(d[0]));p(g,function(){a(h,f)})};var ha="//fonts.googleapis.com/css",ia={latin:"BESbswy","latin-ext":"çöüğş",cyrillic:"йяЖ",greek:"αβΣ",khmer:"កខគ",Hanuman:"កខគ"},ja={thin:"1",extralight:"2","extra-light":"2",ultralight:"2","ultra-light":"2",light:"3",regular:"4",book:"4",medium:"5","semi-bold":"6",semibold:"6","demi-bold":"6",demibold:"6",bold:"7","extra-bold":"8",extrabold:"8","ultra-bold":"8",ultrabold:"8",black:"9",heavy:"9",l:"3",r:"4",b:"7"},ka={i:"i",italic:"i",n:"n",normal:"n"},la=/^(thin|(?:(?:extra|ultra)-?)?light|regular|book|medium|(?:(?:semi|demi|extra|ultra)-?)?bold|black|heavy|l|r|b|[1-9]00)?(n|i|normal|italic)?$/,ma={Arimo:!0,Cousine:!0,Tinos:!0};_.prototype.load=function(a){var b=new n,c=this.c,d=new W(this.a.api,j(c),this.a.text),e=this.a.families;X(d,e);var f=new Z(e);$(f),l(c,Y(d),o(b)),p(b,function(){a(f.a,f.c,ma)})},aa.prototype.load=function(a){var b=this.a.id,c=this.c.m;b?m(this.c,(this.a.api||"https://use.typekit.net")+"/"+b+".js",function(b){if(b)a([]);else if(c.Typekit&&c.Typekit.config&&c.Typekit.config.fn){b=c.Typekit.config.fn;for(var d=[],e=0;e<b.length;e+=2)for(var f=b[e],g=b[e+1],h=0;h<g.length;h++)d.push(new s(f,g[h]));try{c.Typekit.load({events:!1,classes:!1,async:!0})}catch(a){}a(d)}},2e3):a([])},ba.prototype.load=function(a){var b=this.f.id,c=this.c.m,d=this;b?(c.__webfontfontdeckmodule__||(c.__webfontfontdeckmodule__={}),c.__webfontfontdeckmodule__[b]=function(b,c){for(var e=0,f=c.fonts.length;e<f;++e){var g=c.fonts[e];d.a.push(new s(g.name,x("font-weight:"+g.weight+";font-style:"+g.style)))}a(d.a)},m(this.c,j(this.c)+(this.f.api||"//f.fontdeck.com/s/css/js/")+k(this.c)+"/"+b+".js",function(b){b&&a([])})):a([])};var na=new Q(window);na.a.c.custom=function(a,b){return new V(b,a)},na.a.c.fontdeck=function(a,b){return new ba(b,a)},na.a.c.monotype=function(a,b){return new T(b,a)},na.a.c.typekit=function(a,b){return new aa(b,a)},na.a.c.google=function(a,b){return new _(b,a)};var oa={load:c(na.load,na)};"function"==typeof define&&define.amd?define(function(){return oa}):"undefined"!=typeof module&&module.exports?module.exports=oa:(window.WebFont=oa,window.WebFontConfig&&na.load(window.WebFontConfig))}();window.Abtf=function(window){var Abtf={cnf:{},h:function(cnf,css){this.cnf=cnf,cnf.proxy&&window.Abtf.proxy_setup(cnf.proxy),this.cnf.js&&!this.cnf.js[1]&&this.js(this.cnf.js[0]);var noref=!("undefined"==typeof this.cnf.noref||!this.cnf.noref);noref||this.ref(),"undefined"!=typeof cnf.gwf&&cnf.gwf[0]&&!cnf.gwf[1]&&("a"===cnf.gwf[0]?this.async(cnf.gwf[2],"webfont"):"undefined"!=typeof WebFont&&("string"==typeof cnf.gwf[0]&&(cnf.gwf[0]=eval("("+cnf.gwf[0]+")")),WebFont.load(cnf.gwf[0])))},f:function(a){a&&this.css&&this.css(),this.cnf.js&&this.cnf.js[1]&&this.js(this.cnf.js[0]),"undefined"!=typeof this.cnf.gwf&&this.cnf.gwf[0]&&this.cnf.gwf[1]&&("a"===this.cnf.gwf[0]?this.async(this.cnf.gwf[2],"webfont"):"undefined"!=typeof WebFont&&WebFont.load(this.cnf.gwf[0]))},ready:function(a,b,c){b=document,c="addEventListener",b[c]?b[c]("DocumentContentLoaded",a):window.attachEvent("onload",a)},ref:function(){"undefined"!=typeof window.console&&console.log("\n%c100","font: 1em sans-serif; color: white; background-color: #079c2d;padding:2px;","Google PageSpeed Score optimized using https://goo.gl/C1gw96\n\nTest your website: https://pagespeed.pro/tests\n\n")},async:function(a,b){!function(c){var d=c.createElement("script");d.src=a,b&&(d.id=b),d.async=!0;var e=c.getElementsByTagName("script")[0];if(e)e.parentNode.insertBefore(d,e);else{var f=document.head||document.getElementsByTagName("head")[0];f.appendChild(d)}}(document)}},SITE_URL,BASE_URL_REGEX;return Abtf}(window);!function(a,b){var c,d,e=!1,f=!1,g=!1,h=!1,i=!1,j=!1,k=!1,l=!1,m=[],n=[],o=[],p={},q=[];a.Abtf.proxy_setup=function(a){if("undefined"==typeof b)var b=!1;if(c=a.url||b,e=a.js||!1,f=a.css||!1,g=a.cdn||!1,g&&q.push(g),h=a.js_include||!1,i=a.js_exclude||!1,j=a.css_include||!1,k=a.css_exclude||!1,a.preload){l=!0;for(var r=0;r<a.preload.length;r++)"regex"===a.preload[r][0]?(o.push([a.preload[r][2],a.preload[r][3],a.preload[r][1]]),a.preload[r][4]&&(p[a.preload[r][0]]=a.preload[r][4],q.indexOf(a.preload[r][4])===-1&&q.push(a.preload[r][4]))):(m.push(a.preload[r][0]),n.push(a.preload[r][1]),a.preload[r][4]&&(p[a.preload[r][0]]=a.preload[r][4],q.indexOf(a.preload[r][4])===-1&&q.push(a.preload[r][4])));d=a.base||!1}if(0===q.length)q=!1;else for(var s=q.length,r=0;r<s;r++)q[r]=v(q[r])};var r={Element:"undefined"!=typeof Element&&Element,Document:"undefined"!=typeof Document&&Document},s={append:{},insert:{}};for(var t in r)r.hasOwnProperty(t)&&r[t]&&(s.append[t]=r[t].prototype.appendChild,s.insert[t]=r[t].prototype.insertBefore);var u=document.createElement("a");u.href=document.location.href;var v=function(a){var b=document.createElement("a");return b.href=a,b},w=function(a,b){return"css"===b?c.replace("{PROXY:URL}",escape(a)).replace("{PROXY:TYPE}",escape(b)):"js"===b?c.replace("{PROXY:URL}",escape(a)).replace("{PROXY:TYPE}",escape(b)):void 0},x=function(a,c){if(l){var e=m.indexOf(a);if(e>-1)var f=n[e];else if(o.length>0)for(var g,h,i=o.length,j=0;j<i;j++){h=!1;try{g=new RegExp(o[j][0],o[j][1]||"")}catch(a){h=!0}if(!h&&g.test(a)){o[j][2]?f=o[j][2]:o[j][3]&&(a=o[j][3]);break}}if(f){if("undefined"!=typeof p[a])var k=p[a];else var k=d;k+=f.substr(0,2)+"/",k+=f.substr(2,2)+"/",k+=f.substr(4,2)+"/",k+=f;if("js"===c){if(k+=".js","undefined"!=typeof b.cachedScriptUrl){var q=v(k).href;k=b.cachedScriptUrl(q)}}else"css"===c&&(k+=".css");return k}}if("js"===c&&"undefined"!=typeof b.cachedScriptUrl){var r=v(a).href;if(a=b.cachedScriptUrl(r),a!==r)return a}return w(a,c)},y=function(a,b){var c="object"==typeof a&&"undefined"!=typeof a.href?a:v(a);if("blob:"===c.protocol)return!1;if(q&&b!==!0)for(var d=q.length,e=0;e<d;e++)if(c.href.indexOf(q[e].href)!==-1)return!1;return c.host!==u.host},z=function(a){var b="object"==typeof a&&"undefined"!=typeof a.href?a:v(a);if("blob:"===b.protocol)return!0;if(h){for(var c=!1,d=h.length,e=0;e<d;e++)if(b.href.indexOf(h[e])!==-1){c=!0;break}if(!c)return!0}if(i)for(var d=i.length,e=0;e<d;e++)if(b.href.indexOf(i[e])!==-1)return!0;return!1},A=function(a){var b="object"==typeof a&&"undefined"!=typeof a.href?a:v(a);if("blob:"===b.protocol)return!1;if(q)for(var c=q.length,d=0;d<c;d++)if(b.href.indexOf(q[d].href)!==-1)return!1;return b.host!==u.host},B=function(a){var b="object"==typeof a&&"undefined"!=typeof a.href?a:v(a);if("blob:"===b.protocol)return!0;if(j){for(var c=!1,d=j.length,e=0;e<d;e++)if(b.href.indexOf(j[e])!==-1){c=!0;break}if(!c)return!0}if(k)for(var d=k.length,e=0;e<d;e++)if(b.href.indexOf(k[e])!==-1)return!0;return!1},C=function(a){if(a.nodeName)if("SCRIPT"===a.nodeName.toUpperCase()){if(!e)return!1;if("abtf"===a.getAttribute("rel"))return a.removeAttribute("rel"),!1;if(a.src){var c=v(a.src);if(z(c))return!1;if(!y(c)){if("undefined"!=typeof b.cachedScriptUrl){if("blob:"===c.protocol)return!1;url=b.cachedScriptUrl(c.href),url!==c.href&&(a.src=url)}return!1}return"js"}}else if("LINK"===a.nodeName.toUpperCase()&&"stylesheet"===a.rel.toLowerCase()){if(!f)return!1;if("abtf"===a.getAttribute("rel"))return a.removeAttribute("rel"),!1;if(a.href){var c=v(a.href);return!B(c)&&(!!A(c)&&"css")}}return!1},D=function(a){var b=C(a);if(!b)return!1;var c=v("css"===b?a.href:a.src).href,d=x(c,b);"css"===b?a.href=d:"js"===b&&(a.src=d)},E={appendChild:function(a,b){return D(b),s.append[a].call(this,b)},insertBefore:function(a,b,c){return D(b),s.insert[a].call(this,b,c)}};for(var t in r)r.hasOwnProperty(t)&&r[t]&&!function(a){r[a].prototype.appendChild=function(b){return E.appendChild.call(this,a,b)},r[a].prototype.insertBefore=function(b,c){return E.insertBefore.call(this,a,b,c)}}(t);a.Abtf.proxifyScript=function(a){return y(a,!0)?w(a,"js"):a}}(window,window.Abtf);!function(a,b){a.Abtf.css=function(){var a,b=this.cnf.css;if([[["all"],"//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css"],[["all"],"http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/css/prism.css"],[["all"],"http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/plugins/line-numbers/prism-line-numbers.css"]]!==b&&b){var c=!!document.getElementById("AbtfCSS")&&document.getElementById("AbtfCSS").nextSibling;for(i in b)"object"==typeof b[i]&&(a=b[i][0].join(","),this.loadCSS(b[i][1],c,a))}}}(window,window.Abtf);!function(a,b){a.Abtf.loadCSS=function(c,d,e,f){function g(a){return j.body?a():void setTimeout(function(){g(a)})}function h(){function a(){b.raf(function(){k.media=e||"all",f&&f()})}n||(n=!0,k.addEventListener&&k.removeEventListener("load",h),"undefined"!=typeof b.cnf.delay&&parseInt(b.cnf.delay)>0?setTimeout(a,b.cnf.delay):a())}var i,j=a.document,k=j.createElement("link");if(d)i=d;else{var l=(j.body||j.getElementsByTagName("head")[0]).childNodes;i=l[l.length-1]}var m=j.styleSheets;k.rel="stylesheet",k.href=c,k.media="only x",g(function(){i.parentNode.insertBefore(k,d?i:i.nextSibling)});var n=!1,o=function(a){if(!n){for(var b=k.href,c=m.length;c--&&!n;)if(m[c].href===b)return a();setTimeout(function(){o(a)})}};return k.addEventListener?k.addEventListener("load",h):k.onload=h,o(h),k},a.Abtf.raf=function(a){"function"==typeof requestAnimationFrame?requestAnimationFrame(a):"function"==typeof mozRequestAnimationFrame?mozRequestAnimationFrame(a):"function"==typeof webkitRequestAnimationFrame?webkitRequestAnimationFrame(a):"function"==typeof msRequestAnimationFrame?msRequestAnimationFrame(a):b.ready(a)}}(window,window.Abtf);Abtf.h({"gwf":[{"google":{"families":["Open Sans:400,400italic,700,600:latin","Open Sans:300italic,400italic,600italic,300,400,600:latin"]}}],"proxy":{"url":"http:\/\/adventuresinmachinelearning.com?url={PROXY:URL}&type={PROXY:TYPE}&abtf-proxy=d40a8cf7252d97d8e5892d84a6ab4551","js":true,"css":false,"js_exclude":["mathjax"]},"css":[[["all"],"//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css"],[["all"],"http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/css/prism.css"],[["all"],"http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/plugins/line-numbers/prism-line-numbers.css"]]});</script><style type="text/css" rel="abtf" id="AbtfCSS">html,
body,
div,
span,
h1,
h2,
h3,
h4,
p,
a,
img,
i,
ul,
li,
form,
label,
article,
aside,
figure,
header,
nav {
  margin: 0px;
  padding: 0px;
  border: 0px;
  font-style: inherit;
  font-variant: inherit;
  font-weight: inherit;
  font-stretch: inherit;
  font-size: inherit;
  line-height: inherit;
  font-family: inherit;
  vertical-align: baseline;
}

article,
aside,
figure,
header,
nav {
  display: block;
}

ul {
  list-style: none;
}

html {
  font-size: 100%;
  text-size-adjust: none;
}

body {
  font-family: "Open Sans", Helvetica, Arial, sans-serif;
  font-size: 0.875rem;
  line-height: 1.6;
  background: rgb(247, 247, 247);
  word-wrap: break-word;
}

.mh-container,
.mh-container-inner {
  width: 100%;
  max-width: 1080px;
  margin: 0px auto;
  position: relative;
}

.mh-container-outer {
  margin: 25px auto;
  box-shadow: rgba(50, 50, 50, 0.168627) 0px 0px 10px;
}

.mh-wrapper {
  padding: 25px;
  background: rgb(255, 255, 255);
}

.mh-content {
  width: 65.83%;
  overflow: hidden;
}

.mh-sidebar {
  width: 31.66%;
  float: left;
}

.mh-right-sb #main-content {
  float: left;
  margin-right: 2.5%;
}



.clearfix {
  display: block;
}

.clearfix::after {
  content: ".";
  display: block;
  clear: both;
  visibility: hidden;
  line-height: 0;
  height: 0px;
  margin: 0px;
  padding: 0px;
}

h1 {
  font-size: 2rem;
}

h2 {
  font-size: 1.5rem;
}

h3 {
  font-size: 1.25rem;
}

h4 {
  font-size: 1.125rem;
}

h1,
h2,
h3,
h4 {
  font-family: "Open Sans", Helvetica, Arial, sans-serif;
  color: rgb(0, 0, 0);
  line-height: 1.3;
  font-weight: 700;
}

i {
  font-style: italic;
}

a {
  color: rgb(0, 0, 0);
  text-decoration: none;
}

.screen-reader-text {
  position: absolute;
  top: -9999rem;
  left: -9999rem;
}

.mh-header {
  background: rgb(255, 255, 255);
}

.mh-site-logo {
  padding: 20px;
  overflow: hidden;
}

.mh-header-text {
  margin: 5px 0px;
  text-transform: uppercase;
}

.mh-header-title {
  font-size: 2rem;
  line-height: 1;
}

.mh-header-tagline {
  display: inline-block;
  font-size: 0.875rem;
  line-height: 1;
  color: rgb(230, 73, 70);
  padding-top: 10px;
  margin-top: 10px;
  border-top: 1px solid rgb(229, 229, 229);
}

.mh-main-nav-wrap {
  background: rgb(42, 42, 42);
}

.mh-main-nav {
  text-transform: uppercase;
  border-bottom: 5px solid rgb(230, 73, 70);
}

.mh-main-nav li {
  float: left;
  position: relative;
  font-weight: 700;
}

.mh-main-nav li a {
  display: block;
  color: rgb(255, 255, 255);
  padding: 10px 20px;
  border-left: 1px solid rgba(255, 255, 255, 0.0980392);
}

.mh-main-nav li:first-child a {
  border: none;
}

.mh-meta {
  font-size: 0.8125rem;
}

.mh-meta,
.mh-meta a {
  color: rgb(151, 151, 151);
}

.mh-meta span {
  margin-right: 10px;
}

.mh-meta .fa {
  margin-right: 5px;
}

.mh-loop-item {
  padding-bottom: 1.25rem;
  margin-bottom: 1.25rem;
  border-bottom: 1px solid rgb(235, 235, 235);
}

.mh-loop-header {
  margin-bottom: 0.625rem;
}

.mh-loop-meta {
  margin-top: 5px;
}

.mh-loop-excerpt {
  overflow: hidden;
}

.mh-loop-thumb {
  float: left;
  margin-right: 20px;
}

.mh-loop-thumb img {
  width: 100%;
  max-width: 235px;
}

img {
  max-width: 100%;
  height: auto;
  vertical-align: bottom;
}

input {
  font-size: 12px;
  padding: 5px;
  border: 1px solid rgba(0, 0, 0, 0.0980392);
  vertical-align: middle;
  background: rgb(245, 245, 245);
}

input[type="submit"] {
  display: inline-block;
  min-width: 150px;
  font-weight: 700;
  color: rgb(255, 255, 255);
  padding: 10px 15px;
  background: rgb(230, 73, 70);
  text-transform: uppercase;
  border: 0px;
  -webkit-appearance: none;
}

.search-form input {
  font-size: 11px;
  line-height: 1;
  color: rgb(31, 30, 30);
  text-transform: uppercase;
}

.search-form .search-submit {
  display: none;
}

.search-form .search-field {
  position: relative;
  padding: 10px;
  margin: 0px;
  border: 1px solid rgb(235, 235, 235);
  background: rgb(255, 255, 255);
  -webkit-appearance: none;
  border-radius: 0px;
}

.mh-widget .search-form {
  display: block;
  margin: 0px auto;
  padding: 5%;
  background: rgb(245, 245, 245);
}

.mh-widget .search-form .search-field {
  display: block;
  margin: 0px auto;
  width: 90%;
}

.mh-widget {
  margin-bottom: 25px;
  overflow: hidden;
}

.mh-widget-title {
  position: relative;
  font-size: 1rem;
  padding-bottom: 5px;
  margin-bottom: 1.25rem;
  text-transform: uppercase;
  border-bottom: 3px solid rgb(230, 73, 70);
}

.widget_categories li {
  border-bottom: 1px dotted rgb(235, 235, 235);
}

.widget_categories li a {
  display: block;
  padding: 5px 0px;
}

.widget_categories li:first-child a {
  padding-top: 0px;
}

.widget_recent_entries li {
  display: block;
  padding: 5px 0px;
  border-bottom: 1px dotted rgb(229, 229, 229);
}

.widget_recent_entries li:first-child {
  padding-top: 0px;
}

@media screen and (max-width: 1475px) {
  .mh-container-inner {
    width: 100%;
  }

  .mh-container {
    width: 95%;
  }
}

@media screen and (max-width: 1120px) {
  .mh-main-nav li a {
    padding: 10px 15px;
  }

  .mh-main-nav li {
    font-size: 12px;
  }
}

@media screen and (max-width: 900px) {
  .mh-meta-comments {
    display: none;
  }

  .mh-widget {
    margin-bottom: 20px;
  }

  .mh-wrapper,
  .mh-site-logo {
    padding: 20px;
  }

  #mh-mobile .mh-container {
    width: 100%;
  }

  .mh-container-outer {
    margin: 0px auto;
  }
}

@media screen and (max-width: 767px) {
  .mh-sidebar {
    margin-top: 20px;
  }

  .mh-header-title,
  .entry-title {
    font-size: 1.5rem;
  }

  .mh-site-logo {
    text-align: center;
  }

  .mh-custom-header,
  .mh-content,
  .mh-sidebar,
  .mh-right-sb #main-content {
    float: none;
    width: 100%;
    margin: 0px;
  }
}

@media screen and (max-width: 620px) {
  .mh-loop-excerpt {
    display: none;
  }

  .mh-loop-meta {
    display: block;
    font-size: 0.6875rem;
  }

  .mh-loop-title {
    font-size: 0.875rem;
  }

  .mh-loop-thumb {
    max-width: 80px;
  }
}

html {
  margin-top: 32px !important;
}

@media screen and (max-width: 782px) {
  html {
    margin-top: 46px !important;
  }
}

body {
  position: relative;
}</style><script rel="abtf">Abtf.css();</script>
<script type='text/x-mathjax-config'>
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true}});
</script>

<!-- This site is optimized with the Yoast SEO plugin v7.7.2 - https://yoast.com/wordpress/plugins/seo/ -->
<meta name="description" content="Learn all about recurrent neural networks and LSTMs in this comprehensive tutorial, and also how to implement an LSTM in TensorFlow for text prediction"/>
<link rel="canonical" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Recurrent neural networks and LSTM tutorial in Python and TensorFlow - Adventures in Machine Learning" />
<meta property="og:description" content="Learn all about recurrent neural networks and LSTMs in this comprehensive tutorial, and also how to implement an LSTM in TensorFlow for text prediction" />
<meta property="og:url" content="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" />
<meta property="og:site_name" content="Adventures in Machine Learning" />
<meta property="article:section" content="Deep learning" />
<meta property="article:published_time" content="2017-10-09T20:34:37+00:00" />
<meta property="article:modified_time" content="2018-04-03T01:55:08+00:00" />
<meta property="og:updated_time" content="2018-04-03T01:55:08+00:00" />
<meta property="og:image" content="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" />
<meta property="og:image:width" content="772" />
<meta property="og:image:height" content="252" />
<meta property="og:image:alt" content="Recurrent LSTM tutorial - unrolled RNN" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:description" content="Learn all about recurrent neural networks and LSTMs in this comprehensive tutorial, and also how to implement an LSTM in TensorFlow for text prediction" />
<meta name="twitter:title" content="Recurrent neural networks and LSTM tutorial in Python and TensorFlow - Adventures in Machine Learning" />
<meta name="twitter:image" content="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" />
<!-- / Yoast SEO plugin. -->

<link rel='dns-prefetch' href='//cdn.mathjax.org' />
<link rel='dns-prefetch' href='//fonts.googleapis.com' />
<link rel='dns-prefetch' href='//netdna.bootstrapcdn.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Adventures in Machine Learning &raquo; Feed" href="http://adventuresinmachinelearning.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Adventures in Machine Learning &raquo; Comments Feed" href="http://adventuresinmachinelearning.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Adventures in Machine Learning &raquo; Recurrent neural networks and LSTM tutorial in Python and TensorFlow Comments Feed" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/feed/" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/adventuresinmachinelearning.com\/wp-includes\/js\/wp-emoji-release.min.js"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='https://api.w.org/' href='http://adventuresinmachinelearning.com/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://adventuresinmachinelearning.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://adventuresinmachinelearning.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 4.9.8" />
<link rel='shortlink' href='http://adventuresinmachinelearning.com/?p=432' />
<link rel="alternate" type="application/json+oembed" href="http://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fadventuresinmachinelearning.com%2Frecurrent-neural-networks-lstm-tutorial-tensorflow%2F" />
<link rel="alternate" type="text/xml+oembed" href="http://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fadventuresinmachinelearning.com%2Frecurrent-neural-networks-lstm-tutorial-tensorflow%2F&#038;format=xml" />
<meta name="generator" content="Easy Digital Downloads v2.8.10" />
                <!-- auto ad code generated with Easy Google AdSense plugin v1.0.1 -->
                <script async src="http://adventuresinmachinelearning.com/wp-content/cache/abtf/proxy/98/31/1e/98311e1088e466ab7425e6580ae2c43e.js"></script>
                <script>
                (adsbygoogle = window.adsbygoogle || []).push({
                     google_ad_client: "ca-pub-8092137086954180",
                     enable_page_level_ads: true
                });
                </script>      
                <!-- / Easy Google AdSense plugin --><script data-cfasync="false" src="http://adventuresinmachinelearning.com/wp-content/cache/abtf/proxy/27/64/d5/2764d5c0249fea99220c6c1a7381bfe6.js" data-sumo-platform="wordpress" data-sumo-site-id="b7b9f7005ff6d7005228ea00f998170022e7be007bd51c00750dfd000b485800" async></script><!--[if lt IE 9]>
<script src="http://adventuresinmachinelearning.com/wp-content/themes/mh-magazine-lite/js/css3-mediaqueries.js"></script>
<![endif]-->
<style type="text/css" media="screen">body{position:relative}#dynamic-to-top{display:none;overflow:hidden;width:auto;z-index:90;position:fixed;bottom:20px;right:20px;top:auto;left:auto;font-family:sans-serif;font-size:1em;color:#fff;text-decoration:none;padding:17px 16px;border:1px solid #000;background:#272727;-webkit-background-origin:border;-moz-background-origin:border;-icab-background-origin:border;-khtml-background-origin:border;-o-background-origin:border;background-origin:border;-webkit-background-clip:padding-box;-moz-background-clip:padding-box;-icab-background-clip:padding-box;-khtml-background-clip:padding-box;-o-background-clip:padding-box;background-clip:padding-box;-webkit-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-ms-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-moz-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-o-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-khtml-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-icab-box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );box-shadow:0 1px 3px rgba( 0, 0, 0, 0.4 ), inset 0 0 0 1px rgba( 0, 0, 0, 0.2 ), inset 0 1px 0 rgba( 255, 255, 255, .4 ), inset 0 10px 10px rgba( 255, 255, 255, .1 );-webkit-border-radius:30px;-moz-border-radius:30px;-icab-border-radius:30px;-khtml-border-radius:30px;border-radius:30px}#dynamic-to-top:hover{background:#4e9c9c;background:#272727 -webkit-gradient( linear, 0% 0%, 0% 100%, from( rgba( 255, 255, 255, .2 ) ), to( rgba( 0, 0, 0, 0 ) ) );background:#272727 -webkit-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 -khtml-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 -moz-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 -o-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 -ms-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 -icab-linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );background:#272727 linear-gradient( top, rgba( 255, 255, 255, .2 ), rgba( 0, 0, 0, 0 ) );cursor:pointer}#dynamic-to-top:active{background:#272727;background:#272727 -webkit-gradient( linear, 0% 0%, 0% 100%, from( rgba( 0, 0, 0, .3 ) ), to( rgba( 0, 0, 0, 0 ) ) );background:#272727 -webkit-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 -moz-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 -khtml-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 -o-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 -ms-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 -icab-linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) );background:#272727 linear-gradient( top, rgba( 0, 0, 0, .1 ), rgba( 0, 0, 0, 0 ) )}#dynamic-to-top,#dynamic-to-top:active,#dynamic-to-top:focus,#dynamic-to-top:hover{outline:none}#dynamic-to-top span{display:block;overflow:hidden;width:14px;height:12px;background:url( http://adventuresinmachinelearning.com/wp-content/plugins/dynamic-to-top/css/images/up.png )no-repeat center center}</style>
<!-- BEGIN GADWP v4.9.6.2 Universal Tracking - https://deconf.com/google-analytics-dashboard-wordpress/ -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-93928649-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- END GADWP Universal Tracking -->

</head>
<body id="mh-mobile" class="post-template-default single single-post postid-432 single-format-standard edd-test-mode mh-right-sb" itemscope="itemscope" itemtype="http://schema.org/WebPage">
<div id="fb-root"></div>
<script>
  window.fbAsyncInit = function() {
    FB.init({appId: 'your_app_id', status: true, cookie: true,
             xfbml: true});
  };
  (function() {
    var e = document.createElement('script'); e.async = true;
    e.src = document.location.protocol +
      '//connect.facebook.net/en_US/all.js';
    document.getElementById('fb-root').appendChild(e);
  }());
</script>
<div class="mh-container mh-container-outer">
<div class="mh-header-mobile-nav clearfix"></div>
<header class="mh-header" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
	<div class="mh-container mh-container-inner mh-row clearfix">
		<div class="mh-custom-header clearfix">
<div class="mh-site-identity">
<div class="mh-site-logo" role="banner" itemscope="itemscope" itemtype="http://schema.org/Brand">
<div class="mh-header-text">
<a class="mh-header-text-link" href="http://adventuresinmachinelearning.com/" title="Adventures in Machine Learning" rel="home">
<h2 class="mh-header-title">Adventures in Machine Learning</h2>
<h3 class="mh-header-tagline">Learn and explore machine learning</h3>
</a>
</div>
</div>
</div>
</div>
	</div>
	<div class="mh-main-nav-wrap">
		<nav class="mh-navigation mh-main-nav mh-container mh-container-inner clearfix" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
			<div class="menu"><ul>
<li class="page_item page-item-2"><a href="http://adventuresinmachinelearning.com/about/">About</a></li>
<li class="page_item page-item-331"><a href="http://adventuresinmachinelearning.com/contact/">Contact</a></li>
<li class="page_item page-item-335"><a href="http://adventuresinmachinelearning.com/ebook-newsletter-sign/">Ebook / newsletter sign-up</a></li>
</ul></div>
		</nav>
	</div>
</header><div class="mh-wrapper clearfix">
	<div id="main-content" class="mh-content" role="main" itemprop="mainContentOfPage"><article id="post-432" class="post-432 post type-post status-publish format-standard has-post-thumbnail hentry category-deep-learning category-lstms category-recurrent-neural-networks category-tensorflow">
	<header class="entry-header clearfix"><h1 class="entry-title">Recurrent neural networks and LSTM tutorial in Python and TensorFlow</h1><p class="mh-meta entry-meta">
<span class="entry-meta-date updated"><i class="fa fa-clock-o"></i><a href="http://adventuresinmachinelearning.com/2017/10/">October 9, 2017</a></span>
<span class="entry-meta-author author vcard"><i class="fa fa-user"></i><a class="fn" href="http://adventuresinmachinelearning.com/author/andyt81/">Andy</a></span>
<span class="entry-meta-categories"><i class="fa fa-folder-open-o"></i><a href="http://adventuresinmachinelearning.com/category/deep-learning/" rel="category tag">Deep learning</a>, <a href="http://adventuresinmachinelearning.com/category/deep-learning/lstms/" rel="category tag">LSTMs</a>, <a href="http://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/" rel="category tag">Recurrent neural networks</a>, <a href="http://adventuresinmachinelearning.com/category/deep-learning/tensorflow/" rel="category tag">TensorFlow</a></span>
<span class="entry-meta-comments"><i class="fa fa-comment-o"></i><a class="mh-comment-scroll" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#mh-comments">9</a></span>
</p>
	</header>
		<div class="entry-content clearfix">
<figure class="entry-thumbnail">
<img src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-678x252.png" alt="Recurrent LSTM tutorial - unrolled RNN" title="Recurrent neural network" />
<figcaption class="wp-caption-text">Unrolled recurrent neural network</figcaption>
</figure>
<p>In the deep learning journey so far on this website, I&#8217;ve introduced <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">dense neural networks</a> and <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">convolutional neural networks</a> (CNNs) which explain how to perform classification tasks on static images.  We&#8217;ve seen good results, especially with CNN&#8217;s. However, what happens if we want to analyze dynamic data? What about videos, voice recognition or sequences of text? There are ways to do some of this using CNN&#8217;s, but the most popular method of performing classification and other analysis on <em>sequences</em> of data is recurrent neural networks.  This tutorial will be a very comprehensive introduction to recurrent neural networks and a subset of such networks &#8211; long-short term memory networks (or LSTM networks). I&#8217;ll also show you how to implement such networks in TensorFlow &#8211; including the data preparation step. It&#8217;s going to be a long one, so settle in and enjoy these pivotal networks in deep learning &#8211; at the end of this post, you&#8217;ll have a very solid understanding of recurrent neural networks and LSTMs. By the way, if you&#8217;d like to learn how to build LSTM networks in Keras, see <a href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/" target="_blank" rel="noopener">this tutorial</a>.</p>
<p>As always, all the code for this post can be found on <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github repository</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, I&#8217;d recommend this inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.887814&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-learning-recurrent-neural-networks-in-python%2F">Deep Learning: Recurrent Neural Networks in Python</a><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.887814&#038;type=2&#038;subid=0" width="1" height="1" border="0"><noscript><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.887814&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></noscript></p>
<hr />
<h1>An introduction to recurrent neural networks</h1>
<p>A recurrent neural network, at its most fundamental level, is simply a type of densely connected neural network (for an introduction to such networks, <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">see my tutorial</a>). However, the key difference to normal feed forward networks is the introduction of <em>time</em> &#8211; in particular, the output of the hidden layer in a recurrent neural network is <em>fed back </em><em>into itself</em>. Diagrams help here, so observe:</p>
<figure id="attachment_537" style="width: 363px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg" class="size-full wp-image-537" alt="Recurrent LSTM tutorial - RNN diagram with nodes" width="363" height="229" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg 363w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN-300x189.jpg 300w" sizes="(max-width: 363px) 100vw, 363px"><noscript><img class="size-full wp-image-537" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg" alt="Recurrent LSTM tutorial - RNN diagram with nodes" width="363" height="229" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg 363w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN-300x189.jpg 300w" sizes="(max-width: 363px) 100vw, 363px" /></noscript><figcaption class="wp-caption-text">Recurrent neural network diagram with nodes shown</figcaption></figure>
<p>In the diagram above, we have a simple recurrent neural network with three input nodes.  These input nodes are fed into a hidden layer, with sigmoid activations, as per any normal <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">densely connected neural network</a>. What happens next is what is interesting &#8211; the output of the hidden layer is then <em>fed back</em> into the same hidden layer. As you can see the hidden layer outputs are passed through a conceptual <em>delay </em>block to allow the input of $\textbf{h}^{t-1}$ into the hidden layer.  What is the point of this? Simply, the point is that we can now model <em>time </em>or sequence-dependent data.</p>
<p>A particularly good example of this is predicting text sequences.  Consider the following text string: &#8220;A girl walked into a bar, and she said &#8216;Can I have a drink please?&#8217;.  The bartender said &#8216;Certainly {}&#8221;. There are many options for what could fill in the {} symbol in the above string, for instance, &#8220;miss&#8221;, &#8220;ma&#8217;am&#8221; and so on. However, other words could also fit, such as &#8220;sir&#8221;, &#8220;Mister&#8221; etc. In order to get the correct gender of the noun, the neural network needs to &#8220;recall&#8221; that two previous words designating the likely gender (i.e. &#8220;girl&#8221; and &#8220;she&#8221;) were used. This type of flow of information through time (or sequence) in a recurrent neural network is shown in the diagram below, which <em>unrolls </em>the sequence:</p>
<figure id="attachment_541" style="width: 555px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" class=" wp-image-541" alt="Recurrent LSTM tutorial - unrolled RNN" width="555" height="181" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png 772w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-300x98.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-768x251.png 768w" sizes="(max-width: 555px) 100vw, 555px"><noscript><img class=" wp-image-541" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" alt="Recurrent LSTM tutorial - unrolled RNN" width="555" height="181" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png 772w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-300x98.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-768x251.png 768w" sizes="(max-width: 555px) 100vw, 555px" /></noscript><figcaption class="wp-caption-text">Unrolled recurrent neural network</figcaption></figure>
<p>On the left-hand side of the above diagram, we have basically the same diagram as the first (the one which shows all the nodes explicitly). What the previous diagram neglected to show explicitly was that we in fact only ever supply finite length sequences to such networks &#8211; therefore we can <em>unroll </em>the network as shown on the right-hand side of the diagram above. This unrolled network shows how we can supply a stream of data to the recurrent neural network. For instance, first, we supply the word vector for &#8220;A&#8221; (more about word vectors later) to the network <em>F</em> &#8211; the output of the nodes in <em>F </em>are fed into the &#8220;next&#8221; network and also act as a stand-alone output ($h_0$).  The next network (though it is really the same network) <em>F</em> at time <em>t=1</em> takes the next word vector for &#8220;girl&#8221; and the previous output $h_0$ into its hidden nodes, producing the next output $h_1$ and so on.</p>
<p>As discussed above, the words themselves i.e. &#8220;A&#8221;, &#8220;girl&#8221; etc. aren&#8217;t input directly into the neural network. Neither are their one-hot vector type representations &#8211; rather, an embedding vector is used for each word. An embedding vector is an efficient vector representation of the word (often between 50-300 in length), which should maintain some meaning or context of the word. Word embedding won&#8217;t be entered into detail here, as I have covered it extensively in other posts &#8211; <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec word embedding tutorial in Python and TensorFlow</a>, <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">A Word2Vec Keras tutorial</a> and <a href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" target="_blank" rel="noopener">Python gensim Word2Vec tutorial with TensorFlow and Keras</a>. It is an interesting topic and well worth the time investigating.</p>
<p>Now, back to recurrent neural networks themselves. Recurrent neural networks are very flexible. In the implementation shown above, we have a many-to-many model &#8211; in other words, we have the input sequence &#8220;A girl walked into a bar&#8230;&#8221; and many outputs &#8211; $h_0$ to $h_t$. We could also have multiple other configurations.  Another option is one-to-many i.e. supplying one input, say &#8220;girl&#8221; and predicting multiple outputs $h_0$ to $h_t$ (i.e. trying to generate sentences based on a single starting word). A further configuration is many-to-one i.e. supplying many words as input, like the sentence &#8220;A girl walked into a bar, and she said &#8216;Can I have a drink please?&#8217;.  The bartender said &#8216;Certainly {}&#8221; and predicting the next word i.e. {}. The diagram below shows an example one-to-many and many-to-one configuration, respectively (the words next to the outputs are the target words which we would supply during training).</p>
<figure id="attachment_546" style="width: 406px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many.png" class=" wp-image-546" alt="Recurrent neural network LSTM - one-to-many configuration" width="406" height="227" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many.png 502w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many-300x168.png 300w" sizes="(max-width: 406px) 100vw, 406px"><noscript><img class=" wp-image-546" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many.png" alt="Recurrent neural network LSTM - one-to-many configuration" width="406" height="227" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many.png 502w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-one-to-many-300x168.png 300w" sizes="(max-width: 406px) 100vw, 406px" /></noscript><figcaption class="wp-caption-text">Recurrent neural network &#8211; one-to-many configuration</figcaption></figure>
<figure id="attachment_547" style="width: 429px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one.png" class=" wp-image-547" alt="Recurrent neural network LSTM - many-to-one configuration" width="429" height="238" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one.png 507w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one-300x166.png 300w" sizes="(max-width: 429px) 100vw, 429px"><noscript><img class=" wp-image-547" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one.png" alt="Recurrent neural network LSTM - many-to-one configuration" width="429" height="238" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one.png 507w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/RNN-many-to-one-300x166.png 300w" sizes="(max-width: 429px) 100vw, 429px" /></noscript><figcaption class="wp-caption-text">Recurrent neural network &#8211; many-to-one configuration</figcaption></figure>
<p>There are also different many-to-many configurations that can be constructed &#8211; but you get the idea: recurrent neural networks are quite flexible. One last thing to note &#8211; the weights of the connections between time steps are <em>shared</em> i.e. there isn&#8217;t a different set of weights for each time step.</p>
<p>Now you have a pretty good idea of what recurrent neural networks are, it is time to point out their dominant problem.</p>
<h2>The problem with basic recurrent neural networks</h2>
<p>Vanilla recurrent neural networks aren&#8217;t actually used very often in practice. Why? The main reason is the vanishing gradient problem. For recurrent neural networks, ideally, we would want to have long memories, so the network can connect data relationships at significant distances in time. That sort of network could make real progress in understanding how language and narrative works, how stock market events are correlated and so on. However, the more time steps we have, the more chance we have of back-propagation gradients either accumulating and exploding or vanishing down to nothing.</p>
<p>Consider the following representation of a recurrent neural network:</p>
<p>$$\textbf{h}_t = \sigma (\textbf{Ux}_t + \textbf{Vh}_{t-1})$$</p>
<p>Where <strong><em>U </em></strong>and <strong><em>V</em></strong><em> </em>are the weight matrices connecting the inputs and the recurrent outputs respectively. We then often will perform a softmax of all the $\textbf{h}_t$ outputs (if we have some sort of many-to-many or one-to-many configuration). Notice, however, that if we go back three time steps in our recurrent neural network, we have the following:</p>
<p>$$\textbf{h}_t = \sigma (\textbf{Ux}_t + \textbf{V}(\sigma(\textbf{Ux}_{t-1} + \textbf{V}(\sigma(\textbf{Ux}_{t-2})))$$</p>
<p>From the above you can see, as we work our way back in time, we are essentially adding deeper and deeper layers to our network. This causes a problem &#8211; consider the gradient of the error with respect to the weight matrix <em><strong>U</strong></em> during backpropagation through time, it looks something along the lines of this:</p>
<p>$$\frac{\partial E_3}{\partial U} = \frac{\partial E_3}{\partial out_3}\frac{\partial out_3}{\partial h_3}\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial U}$$</p>
<p>The equation above is only a rough approximation of what is going on during backpropagation through time, but it will suffice for our purposes (for more on back-propagation, see my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">comprehensive neural networks tutorial</a>). Each of these gradients will involve calculating the gradient of the sigmoid function. The problem with the sigmoid function occurs when the input values are such that the output is close to either 0 or 1 &#8211; at this point, the gradient is very small, see the plot below.</p>
<figure id="attachment_559" style="width: 389px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png" class="size-full wp-image-559" alt="Recurrent neural network and LSTM tutorial - sigmoid gradient" width="389" height="266" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png 389w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient-300x205.png 300w" sizes="(max-width: 389px) 100vw, 389px"><noscript><img class="size-full wp-image-559" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png" alt="Recurrent neural network and LSTM tutorial - sigmoid gradient" width="389" height="266" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png 389w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient-300x205.png 300w" sizes="(max-width: 389px) 100vw, 389px" /></noscript><figcaption class="wp-caption-text">Sigmoid gradient</figcaption></figure>
<p>As you can observe, the values of the gradient (orange line) are always &lt;0.25 and get to very low values when the output gets close to 0 or 1. What does this mean? It means that when you multiply many sigmoid gradients together you are multiplying many values which are potentially much less than zero &#8211; this leads to a vanishing gradient $\frac{\partial E}{\partial U}$. Because the gradient will become basically zero when dealing with many prior time steps, the weights won&#8217;t adjust to take into account these values, and therefore the network won&#8217;t learn relationships separated by significant periods of time. This makes vanilla recurrent neural networks not very useful. If you&#8217;d like to learn more about the vanishing gradient problem, see my dedicated post about it <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/">here</a>.</p>
<p>We could use <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener">ReLU activation functions</a> to reduce this problem, though not eliminate it. However, the most popular way of dealing with this issue in recurrent neural networks is by using long-short term memory (LSTM) networks, which will be introduced in the next section.</p>
<h1>Introduction to LSTM networks</h1>
<p>To reduce the vanishing (and exploding) gradient problem, and therefore allow deeper networks and recurrent neural networks to perform well in practical settings, there needs to be a way to reduce the multiplication of gradients which are less than zero. The LSTM cell is a specifically designed unit of logic that will help reduce the vanishing gradient problem sufficiently to make recurrent neural networks more useful for long-term memory tasks i.e. text sequence predictions. The way it does so is by creating an internal memory state which<em> </em>is simply <em>added</em> to the processed input, which greatly reduces the multiplicative effect of small gradients. The time dependence and effects of previous inputs are controlled by an interesting concept called a <em>forget </em><em>gate</em>, which determines which states are remembered or forgotten. Two other gates, the <em>input gate</em> and <em>output</em><em> gate</em>, are also featured in LSTM cells.</p>
<p>Let&#8217;s first have a look at LSTM cells more carefully, then I&#8217;ll discuss how they help reduce the vanishing gradient problem.</p>
<h2>The structure of an LSTM cell</h2>
<p>The structure of a typical LSTM cell is shown in the diagram below:</p>
<figure id="attachment_564" style="width: 592px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png" class=" wp-image-564" alt="Recurrent neural network LSTM tutorial - LSTM cell diagram" width="592" height="285" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png 669w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram-300x144.png 300w" sizes="(max-width: 592px) 100vw, 592px"><noscript><img class=" wp-image-564" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png" alt="Recurrent neural network LSTM tutorial - LSTM cell diagram" width="592" height="285" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png 669w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram-300x144.png 300w" sizes="(max-width: 592px) 100vw, 592px" /></noscript><figcaption class="wp-caption-text">LSTM cell diagram</figcaption></figure>
<p>The data flow is from left-to-right in the diagram above, with the current input $x_t$ and the previous cell output $h_{t-1}$ concatenated together and entering the top &#8220;data rail&#8221;. Here&#8217;s where things get interesting.</p>
<h3>The input gate</h3>
<p>First, the input is squashed between -1 and 1 using a <em>tanh</em> activation function. This can be expressed by:</p>
<p>$$g = tanh(b^g + x_tU^g + h_{t-1}V^g)$$</p>
<p>Where $U^g$ and $V^g$ are the weights for the input and previous cell output, respectively, and $b^g$ is the input bias. Note that the exponents <i>g</i> are not a raised power, but rather signify that these are the input weights and bias values (as opposed to the input gate, forget gate, output gate etc.).</p>
<p>This squashed input is then multiplied element-wise by the output of the <em>input gate</em>. The input gate is basically a hidden layer of sigmoid activated nodes, with weighted $x_t$ and $h_{t-1}$ input values, which outputs values of between 0 and 1 and when multiplied element-wise by the input determines which inputs are switched on and off. In other words, it is a kind of input filter or gate. The expression for the input gate is:</p>
<p>$$i = \sigma(b^i + x_tU^i + h_{t-1}V^i)$$</p>
<p>The output of the input stage of the LSTM cell can be expressed below, where the $\circ$ operator expresses element-wise multiplication:</p>
<p>$$g \circ i$$</p>
<p>As you can observe, the input gate output <em>i</em> acts as the weights for the squashed input <em>g</em>.  We now move onto the next stage of the LSTM cell &#8211; the internal state and the forget gate.</p>
<h3>The internal state and the forget gate</h3>
<p>This stage in the LSTM is where most of the magic happens. As can be observed, there is a new variable<em> </em>$s_t$ which is the inner state of the LSTM cell. This state is delayed by one-time step and is ultimately added to the $g \circ i$ input to provide an internal recurrence loop to learn the relationship between inputs separated by time. Two things to notice &#8211; first, there is a forget gate here &#8211; this forget gate is again a sigmoid activated set of nodes which is element-wise multiplied by $s_{t-1}$ to determine which previous states should be remembered (i.e. forget gate output close to 1) and which should be forgotten (i.e. forget gate output close to 0). This allows the LSTM cell to learn appropriate context. Consider the sentence &#8220;Clare took Helen to Paris and she was very grateful&#8221; &#8211; for the LSTM cell to learn who &#8220;she&#8221; refers to, it needs to forget the subject &#8220;Clare&#8221; and replace it with the subject &#8220;Helen&#8221;. The forget gate can facilitate such operations and is expressed as:<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet.png" class="wp-image-569 alignleft" alt="Recurrent neural network LSTM tutorial - forget gate snippet" width="85" height="202" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet.png 193w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet-126x300.png 126w" sizes="(max-width: 85px) 100vw, 85px"><noscript><img class="wp-image-569 alignleft" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet.png" alt="Recurrent neural network LSTM tutorial - forget gate snippet" width="85" height="202" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet.png 193w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Forget-gate-snippet-126x300.png 126w" sizes="(max-width: 85px) 100vw, 85px" /></noscript></p>
<p>$$f = \sigma(b^f + x_tU^f + h_{t-1}V^f)$$</p>
<p>The output of the element-wise product of the previous state and the forget gate is expressed as $s_{t-1} \circ f$. Again, the forget gate output acts as weights for the internal state. The second thing to notice about this stage is that the forget-gate-&#8220;filtered&#8221; state is simply added to the input, rather than multiplied by it, or mixed with it via weights and a sigmoid activation function as occurs in a standard recurrent neural network. This is important to reduce the issue of vanishing gradients. The output from this stage, $s_t$ is expressed by:</p>
<p>$$s_t = s_{t-1} \circ f + g \circ i$$</p>
<p>The final stage of the LSTM cell is the output gate.</p>
<h3>The output gate</h3>
<p>The final stage of the LSTM cell is the output gate. The output gate has two components &#8211; another <em>tanh </em>squashing function and an output sigmoid gating function. The output sigmoid gating function, like the other gating functions in the cell, is multiplied by the squashed state $s_t$ to determine which values of the state are output from the cell. As you can tell, the LSTM cell is very flexible, with gating functions controlling what is input, what is &#8220;remembered&#8221; in the internal state variable, and finally what is output from the LSTM cell. <img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet.png" class="wp-image-575 alignleft" alt="Recurrent neural network LSTM tutorial - output gate snippet" width="115" height="203" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet.png 262w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet-169x300.png 169w" sizes="(max-width: 115px) 100vw, 115px"><noscript><img class="wp-image-575 alignleft" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet.png" alt="Recurrent neural network LSTM tutorial - output gate snippet" width="115" height="203" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet.png 262w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Output-gate-snippet-169x300.png 169w" sizes="(max-width: 115px) 100vw, 115px" /></noscript></p>
<p>The output gate is expressed as:</p>
<p>$$o = \sigma(b^o + x_tU^o + h_{t-1}V^o)$$</p>
<p>So the final output of the cell can be expressed as:</p>
<p>$$h_t = tanh(s_t) \circ o$$</p>
<p>The next question is, how does the LSTM cell reduce the vanishing gradient problem?</p>
<h2>Reducing the vanishing gradient problem</h2>
<p>Recall before that the issue with vanilla recurrent neural networks is that calculating the gradient to update the weights involves cascading terms like:</p>
<p>$$\frac {\partial h_n}{\partial h_{n-1}} \frac {\partial h_{n-1}}{\partial h_{n-2}} \frac {\partial h_{n-2}}{\partial h_{n-3}} &#8230;$$</p>
<p>This is a problem because of the sigmoid derivative, which is present in all of the partial derivatives above, being &lt;0.25 (often greatly so). There is also a factorial of the weights involved, so if they are consistently &lt;1, we get a similar result &#8211; a vanishing gradient.</p>
<p>In an LSTM cell, the recurrency of the internal state of the LSTM cell involves, as shown above, an addition &#8211; like so:</p>
<p>$$s_t = s_{t-1} \circ f + g \circ i$$</p>
<p>If we take the partial derivative of this recurrency like we did above for a vanilla recurrent neural network, we find the following:</p>
<p>$$\frac{\partial s_t}{\partial s_{t-1}} = f$$</p>
<p>Notice that the $g \circ i$ term drops away and we are just left with a repeated multiplication of $f$. So for three time steps, we would have $f x f x f$. Notice that if the output of $f=1$, there will be no decay of the gradient. Generally, the bias of the sigmoid in $f$ is made large at the beginning of training so that $f$ starts out as 1 , meaning that all past input states will be &#8220;remembered&#8221; in the cell. During training, the forget gate will reduce or eliminate the memory of certain components of the state $s_{t-1}$.</p>
<p>This might be a bit confusing, so I&#8217;ll explain another way before we move on. Imagine if we let in a single input during the first time step, but then we block all future inputs (by setting the input gate to output zeros) and remember all previous states (by setting the forget gate to output ones). We would have a kind of circulating memory of $s_t$ which never decays i.e. $s_t$ = $s_{t-1}$. A back-propagated error &#8220;entering&#8221; this loop would also never decay. With the vanilla recurrent neural network, however, if we did the same thing our back-propagated error would be continuously degraded by the gradient of the activation function of the hidden nodes, and therefore eventually decay to zero.</p>
<p>Hopefully, that helps you to understand, at least in part, why LSTM cells are a great solution to the vanishing gradient problem, and therefore why they are currently used so extensively. Now, so far, we have been dealing with the data in the LSTM cells as if they were single values (i.e. scalars), however, in reality, they are tensors or vectors, and this can get confusing. So in the next section, I&#8217;ll spend a bit of time explaining the tensor sizes we can expect to be flowing around our unrolled LSTM networks.</p>
<h2>The dimensions of data inside an LSTM cell</h2>
<p>In the example code that is going to be discussed below, we are going to be performing text prediction. Now, as discussed in <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">previous tutorials on the Word2Vec algorithm</a>, words are input into neural networks using meaningful word vectors i.e. the word &#8220;cat&#8221; might be represented by, say, a 650 length vector. This vector is encoded in such a way as to capture some aspect of the meaning of the word (where meaning is usually construed as the context the word is usually found in). So each word input into our LSTM network below will be a 650 length vector. Next, because we will be inputting a sequence of words into our unrolled LSTM network, for each input row we will be inputting 35 of these word vectors. So the input for each row will be (35 x 650) in size. Finally, with TensorFlow, we can process batches of data via multi-dimensional tensors (to learn more about basic TensorFlow, see <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">this TensorFlow tutorial</a>). If we have a batch size of 20, our <em>training</em> input data will be (20 x 35 x 650). For future reference, the way I have presented the tensor size here (i.e. (20 x 35 x 650)) is called a &#8220;batch-major&#8221; arrangement, where the batch size is the first dimension of the tensor. We could also alternatively arrange the data in &#8220;time-major&#8221; format, which would be (35 x 20 x 650) &#8211; same data, just a different arrangement.</p>
<p>Now, the next thing to consider is that each of the input, forget and output gates, along with the inner state variable $s_t$ and the squashing functions, are not single functions with single/scalar weights. Rather, they comprise the hidden layer of the network and therefore include multiple nodes, connecting weights, bias values and so on. It is up to us to set the size of the hidden layer. The output from the unrolled LSTM network will, therefore, include the size of the hidden layer. The size of the output from the unrolled LSTM network with a size 650 hidden layer, and a 20 length batch-size and 35 time steps will be (20, 35, 650). Often, the output of an unrolled LSTM will be partially flattened and fed into a softmax layer for classification &#8211; so, for instance, the first two dimensions of the tensor are flattened to give a softmax layer input size of (700, 650). The output of the softmax is then matched against the expected training outputs during training. The diagram below shows all this:</p>
<figure id="attachment_756" style="width: 503px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3.png" class="wp-image-756 " alt="TensorFlow LSTM network architecture" width="503" height="512" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3.png 657w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3-295x300.png 295w" sizes="(max-width: 503px) 100vw, 503px"><noscript><img class="wp-image-756 " src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3.png" alt="TensorFlow LSTM network architecture" width="503" height="512" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3.png 657w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/10/LSTM-many-to-many-classifier-3-295x300.png 295w" sizes="(max-width: 503px) 100vw, 503px" /></noscript><figcaption class="wp-caption-text">LSTM network architecture</figcaption></figure>
<p>As can be observed in the architecture above (which we will be creating in the code below), it is possible to stack layers of LSTM cells on top of each other &#8211; this increases the model complexity and predictive power but at the expense of training times and difficulties. The architecture shown above is what we will implement in TensorFlow in the next section. Note the small batch size &#8211; this is to allow a more stochastic gradient descent which will avoid settling in local minima during many training iterations (see <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener">here</a>).</p>
<h1>Creating an LSTM network in TensorFlow</h1>
<p>We are now going to create an LSTM network in TensorFlow. The code will loosely follow the TensorFlow team tutorial found <a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb" target="_blank" rel="noopener">here</a>, but with updates and my own substantial modifications. The text dataset that will be used and is a common benchmarking corpus is the <a href="https://catalog.ldc.upenn.edu/ldc99t42" target="_blank" rel="noopener">Penn Tree Bank</a> (PTB) dataset. As usual, all the code for this post can be found on the <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">AdventuresinML Github site</a>. To run this code, you&#8217;ll first have to download and extract the .tgz file from <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz" target="_blank" rel="noopener">here</a>. First off, we&#8217;ll go through the data preparation part of the code.</p>
<h2>Preparing the data</h2>
<p>This code will use, verbatim, the following functions from the <a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb" target="_blank" rel="noopener">previously mentioned TensorFlow tutorial</a>: <em>read_words, build_vocab </em>and <em>file_to_word_ids. </em>I won&#8217;t go into these functions in detail, but basically, they first split the given text file into separate words and sentence based characters (i.e. end-of-sentence &lt;eos&gt;). Then, each unique word is identified and assigned a unique integer. Finally, the original text file is converted into a list of these unique integers, where each word is substituted with its new integer identifier. This allows the text data to be consumed in the neural network.</p>
<p>The code below shows how these functions are used in my code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def load_data():
    # get the data paths
    train_path = os.path.join(data_path, &quot;ptb.train.txt&quot;)
    valid_path = os.path.join(data_path, &quot;ptb.valid.txt&quot;)
    test_path = os.path.join(data_path, &quot;ptb.test.txt&quot;)

    # build the complete vocabulary, then convert text data to list of integers
    word_to_id = build_vocab(train_path)
    train_data = file_to_word_ids(train_path, word_to_id)
    valid_data = file_to_word_ids(valid_path, word_to_id)
    test_data = file_to_word_ids(test_path, word_to_id)
    vocabulary = len(word_to_id)
    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))

    print(train_data[:5])
    print(word_to_id)
    print(vocabulary)
    print(&quot; &quot;.join([reversed_dictionary[x] for x in train_data[:10]]))
    return train_data, valid_data, test_data, vocabulary, reversed_dictionary</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, we simply setup the directory paths for the train, validation and test datasets respectively. Then, <em>build_vocab</em>() is invoked on the training data to create a dictionary that has each word as a key, and a unique integer as the associated value. Here is a sample of what the <em>word_to_id</em> dictionary looks like:</p>
<blockquote><p>{&#8216;write-off&#8217;: 7229, &#8216;ports&#8217;: 8314, &#8216;fundamentals&#8217;: 4478, &#8216;toronto-based&#8217;: 5034, &#8216;head&#8217;: 638, &#8216;fairness&#8217;: 6417,&#8230;</p></blockquote>
<p>Next, we convert the text data for each file into a list of integers using the <em>word_to_id</em> dictionary. The first 5 items of the list <em>train_data </em>looks like:</p>
<blockquote>[9970, 9971, 9972, 9974, 9975]</blockquote>
<p>I&#8217;ve also created a reverse dictionary which allows you to go the other direction &#8211; from a unique integer identifier to the corresponding word. This will be used later when we are reconstructing the outputs of our LSTM network back into plain English sentences.</p>
<p>The next step is to develop an input data pipeline that allows the extraction of batches of data in an efficient manner.</p>
<h2>Creating an input data pipeline</h2>
<p>As discussed in my <a href="http://adventuresinmachinelearning.com/introduction-tensorflow-queuing/" target="_blank" rel="noopener">TensorFlow queues and threads</a> tutorial, the use of a feed dictionary to supply data to your model during training, while common in tutorials, is not efficient &#8211; as can be read <a href="https://www.tensorflow.org/performance/performance_guide#input_pipeline_optimization" target="_blank" rel="noopener">here</a> on the TensorFlow site. Rather, it is more efficient to use TensorFlow queues and threading. Note, that there is a new way of doing things, using the Dataset API, which won&#8217;t be used in this tutorial, but I will perhaps update it in the future to include this new way of doing things. I&#8217;ve packaged up this code in a function called <em>batch_producer</em> &#8211; this function extracts batches of <em>x, y</em> training data &#8211; the <em>x </em>batch is formatted as the time stepped text data. The y batch is the same data, except delayed one time step. So, for instance, a single <em>x, y</em> sample in a batch, with the number of time steps being 8, looks like:</p>
<ul>
<li><em>x = </em>&#8220;A girl walked into a bar, and she&#8221;</li>
<li>y = &#8220;girl walked into a bar, and she said&#8221;</li>
</ul>
<p>Remember that <em>x </em>and <em>y</em> will be batches of integer data, with the size (<em>batch_size</em>, <em>num_steps</em>), not text as shown above &#8211; however, I have shown the above <em>x </em>and <em>y </em>sample in text form to aid understanding. So, as demonstrated in the model architecture diagram above, we are producing a many-to-many LSTM model, where the model will be trained to predict the very next word in the sequence <em>for each</em> word in the number of time steps.</p>
<p>Here&#8217;s what the code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def batch_producer(raw_data, batch_size, num_steps):
    raw_data = tf.convert_to_tensor(raw_data, name=&quot;raw_data&quot;, dtype=tf.int32)

    data_len = tf.size(raw_data)
    batch_len = data_len // batch_size
    data = tf.reshape(raw_data[0: batch_size * batch_len],
                      [batch_size, batch_len])

    epoch_size = (batch_len - 1) // num_steps

    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()
    x = data[:, i * num_steps:(i + 1) * num_steps]
    x.set_shape([batch_size, num_steps])
    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]
    y.set_shape([batch_size, num_steps])
    return x, y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the code above, first, the raw text data is converted into an <em>int32</em> tensor. Next, the length of the full data set is calculated and stored in <em>data_len</em> and this is then divided by the batch size in an <em>integer division (//)</em> to get the number of full batches of data available within the dataset. The next line reshapes the <em>raw_data </em>tensor (restricted in size to the number of full batches of data i.e. 0 to <em>batch_size * batch_len</em>) into a (<em>batch_size, batch_len</em>) shape. The next line sets the number of iterations in each epoch &#8211; usually, this is set so that all the training data is passed through the algorithm in each epoch. This is what occurs here &#8211; the number of batches in the data (<em>batch_len</em>) is integer divided by the number of time steps &#8211; this gives the number of time-step-sized batches that are available to be iterated through in a single epoch.</p>
<p>The next line sets up an input range producer queue &#8211; this is a simple queue which allows the asynchronous and threaded extraction of data batches from a pre-existing dataset. For more on threads and queues, check out <a href="http://adventuresinmachinelearning.com/introduction-tensorflow-queuing/">my tutorial</a>. Basically, each time more data is required in the training of the model, a new integer is extracted between 0 and <em>epoch_size</em> &#8211; this is then used in the following lines to extract a batch of data asynchronously from the <em>data</em> tensor. With the <em>shuffle</em> argument set to False, this integer simply cycles from 0 to <em>epoch_size</em> and then resets back at 0 to repeat.</p>
<p>To produce the <em>x, y</em> batches of data, data slices are extracted from the data tensor based on the dequeued integer <em>i</em>. To see how this works, it is easier to imagine a dummy dataset of integers up to 20 &#8211; [0, 1, 2, 3, 4, 5, 6, &#8230;, 19, 20]. Let&#8217;s say we set the batch size to 3, and the number of steps to 2. The variables <em>batch_len </em>and <em>epoch_size </em>will therefore be equal to 6 and 2, respectively. The dummy reshaped data will look like:</p>
<p>$$\begin{bmatrix}<br />
1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\<br />
7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 \\<br />
13 &amp; 14 &amp; 15 &amp; 16 &amp; 17 &amp; 18 \\<br />
\end{bmatrix}$$</p>
<p>For the first data batch extraction, <em>i = 0</em>, therefore the extracted <em>x</em> for our dummy dataset will be <em>data[:, 0:2]</em>:</p>
<p>$$\begin{bmatrix}<br />
1 &amp; 2\\<br />
7 &amp; 8\\<br />
13 &amp; 14\\<br />
\end{bmatrix}$$</p>
<p>The extracted <em>y</em> will be <em>data[:, 1:3]</em>:</p>
<p>$$\begin{bmatrix}<br />
2 &amp; 3\\<br />
8 &amp; 9\\<br />
14 &amp; 15\\<br />
\end{bmatrix}$$</p>
<p>As can be observed, each row of the extracted <em>x </em>and <em>y </em>tensors will be an individual sample of length <em>num_steps</em> and the number of rows is the batch length. By organizing the data in this fashion, it is straight-forward to extract batch data while still maintaining the correct sentence sequence within each data sample.</p>
<h2>Creating the model</h2>
<p>In this code example, in order to have nice encapsulation and better-looking code, I&#8217;ll be building the model in <a href="https://docs.python.org/3/tutorial/classes.html" target="_blank" rel="noopener">Python classes</a>. The first class is a simple class that contains the input data:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Input(object):
    def __init__(self, batch_size, num_steps, data):
        self.batch_size = batch_size
        self.num_steps = num_steps
        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps
        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We pass this object important input data information such as batch size, the number of recurrent time steps and finally the raw data file we wish to extract batch data from. The previously explained <em>batch_producer</em> function, when called, will return our input data batch <em>x</em> and the associated time step + 1 target data batch, <em>y</em>.</p>
<p>The next step is to create our LSTM model. Again, I&#8217;ve used a Python class to hold all the information and TensorFlow operations:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the main model
class Model(object):
    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,
                 dropout=0.5, init_scale=0.05):
        self.is_training = is_training
        self.input_obj = input
        self.batch_size = input.batch_size
        self.num_steps = input.num_steps</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first part of initialization is pretty self-explanatory, with the input data information and batch producer operation found in <em>input_obj</em>. Another important input is the boolean <em>is_training</em> &#8211; this allows the model instance to be created either as a model setup for training, or alternatively setup for validation or testing only.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the word embeddings
with tf.device(&quot;/cpu:0&quot;):
    embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))
    inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The block of code above creates the word embeddings. As previously discussed and shown in <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">my tutorial</a>, word embedding creates meaningful vectors to represent each word. First, we initialize the embedding variable with size (vocab_size, hidden_size) which creates the &#8220;lookup table&#8221; where each row represents a word in the dataset, and the set of columns is the embedding vector. In this case, our embedding vector length is set equal to the size of our LSTM hidden layer.</p>
<p>The next line performs a lookup action on the embedding tensor, where each word in the input data set is matched with a row in the embedding tensor, with the matched embedding vector being returned within <em>inputs.</em></p>
<p>In this model, the embedding layer / vectors will be learned during the model training &#8211; however, if we so desired, we could also pre-learn embedding vectors using another model and upload these into our models. I&#8217;ve shown how to do this in <a href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" target="_blank" rel="noopener">my gensim tutorial</a> if you want to check it out.</p>
<p>The next step adds a <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" target="_blank" rel="noopener">drop-out</a> wrapper to the input data &#8211; this helps prevent overfitting by continually changing the structure of the network connections:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if is_training and dropout &lt; 1:
    inputs = tf.nn.dropout(inputs, dropout)</code></pre> <div class="code-embed-infos"> </div> </div>
<h3>Creating the LSTM network</h3>
<p>The next step is to setup the initial state TensorFlow placeholder. This placeholder will be loaded with the initial state of the LSTM cells for each training batch. At the beginning of each training epoch, the input data will reset to the beginning of the text data set, so we want to reset the state variables to zero. However, during the multiple training batches executed in each epoch, we want to load the final state variables from the previous training batch into our LSTM cells for the current training batch. This keeps a certain continuity of state in our model, as we are progressing linearly through our text data set. We define the placeholder by:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># set up the state storage / extraction
self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The second argument to the placeholder function is the size of the variable &#8211; (num_layers, 2, batch_size, hidden_size) and requires some explanation. If we consider an individual LSTM cell, for each training sample it processes it has two other inputs &#8211; the previous output from the cell ($h_{t-1}$) and the previous state variable ($s_{t-1}$). These two inputs, <em>h</em> and <em>s, </em>are what is required to load the full state data into an LSTM cell. Remember also that <em>h</em> and <em>s</em> for each sample are actually vectors with the size equal to the hidden layer size. Therefore, for all the samples in the batch, for a single LSTM cell we have state data required of shape (2, batch_size, hidden_size). Finally, if we have stacked LSTM cell layers, we need state variables for each layer &#8211; <em>num_layers. </em>This gives the final shape of the state variables: (num_layers, 2, batch_size, hidden_size).</p>
<p>The next two steps involve setting up this state data variable in the format required to feed it into the TensorFlow LSTM data structure:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">state_per_layer_list = tf.unstack(self.init_state, axis=0)
rnn_tuple_state = tuple(
            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])
             for idx in range(num_layers)]
        )</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The TensorFlow LSTM cell can accept the state as a tuple if a flag is set to True (more on this later). The <em>tf.unstack</em> command creates a number of tensors, each of shape (2, batch_size, hidden_size), from the <em>init_state </em>tensor, one for each stacked LSTM layer <em>(num_layer)</em>. These tensors are then loaded into a specific TensorFlow data structure<em>, LSTMStateTuple</em>, which is the required for input into the LSTM cells.</p>
<p>Next, we create an LSTM cell which will be &#8220;unrolled&#8221; over the number of time steps. Following this, we apply a drop-out wrapper to again protect against overfitting. Notice that we set the forget bias values to be equal to 1.0, which helps guard against repeated low forget gate outputs causing vanishing gradients, as explained above:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create an LSTM cell to be unrolled
cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)
# add a dropout wrapper if training
if is_training and dropout &lt; 1:
    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, if we include many layers of stacked LSTM cells in the model, we need to use another TensorFlow object called <em>MultiRNNCell </em>which performs the requisite cell stacking / layering:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if num_layers &gt; 1:
    cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that we tell <em>MultiRNNCell </em>to expect the state variables in the form of a <em>LSTMStateTuple</em> by setting the flag <em>state_is_tuple</em> to True.</p>
<p>The final step in creating the LSTM network structure is to create a dynamic RNN object in TensorFlow. This object will dynamically perform the unrolling of the LSTM cell over each time step.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The <em>dynamic_rnn </em>object takes our defined LSTM cell as the first argument, and the embedding vector tensor <em>inputs</em> as the second argument. The final argument, <em>initial_state</em> is where we load our time-step zero state variables, that we created earlier, into the unrolled LSTM network.</p>
<p>This operation creates two outputs, the first is the output from all the unrolled LSTM cells, and will have a shape of (batch_size, num_steps, hidden_size). This data will be flattened in the next step to feed into a softmax classification layer. The second output, <em>state</em>, is the (s, h) state tuple taken from the final time step of the LSTM cells. This <em>state</em> operation / tuple will be extracted during each batch training operation to be used as inputs (via <em>init_state</em>) into the next training batch.</p>
<h3>Creating the softmax, loss and optimizer operations</h3>
<p>Next we have to flatten the outputs so that we can feed them into our proposed softmax classification layer. We can use the -1 notation to reshape our output tensor, with the second axis set to be equal to the hidden layer size:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># reshape to (batch_size * num_steps, hidden_size)
output = tf.reshape(output, [-1, hidden_size])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next we setup our softmax weight variables and the standard $xw+b$ operation:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))
softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))
logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that the <em>logits</em> operation is simply the output of our tensor multiplication &#8211; we haven&#8217;t yet added the softmax operation &#8211; this will occur in the loss calculations below (and also in our ancillary accuracy calculations).</p>
<p>Following this, we have to setup our loss or cost function which will be used to train our LSTM network. In this case, we will use the specialized <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss" target="_blank" rel="noopener">TensorFlow sequence to sequence loss function</a>. This loss function allows one to calculate (a potentially) weighted cross entropy loss over a sequence of values. The first argument to this loss function is the <em>logits</em> argument, which requires tensors with the shape (batch_size, num_steps, vocab_size) &#8211; so we&#8217;ll need to reshape our logits tensor. The second argument to the loss function is the <em>targets </em>tensor which has a shape (batch_size, num_steps) with each value being an integer (which corresponds to a unique word in our case) &#8211; in other words, this tensor contains the true values of the word sequence that we want our LSTM network to predict. The third important argument is the weights tensor, of shape (batch_size, num_steps), which allows you to weight different samples or time steps with respect to the loss i.e. you might want the loss to favor the latter time steps rather than the earlier ones. No weighting is applied in this model, so a tensor of ones is passed to this argument.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Reshape logits to be a 3-D tensor for sequence loss
logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])

# Use the contrib sequence loss and average over the batches
loss = tf.contrib.seq2seq.sequence_loss(
            logits,
            self.input_obj.targets,
            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),
            average_across_timesteps=False,
            average_across_batch=True)
# Update the cost
self.cost = tf.reduce_sum(loss)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>There are two more important arguments for this function &#8211; <em>average_across_timesteps </em>and <em>average_across_batch</em>. If <em>average_across_timesteps </em>is set to True, the cost will be summed across the time dimension, if <em>average_across_batch</em> is True, then the cost will be summed across the batch dimension. In this case we are favoring the latter option.</p>
<p>Finally, we produce the <em>cost</em> operation which reduces the loss to a single scalar value &#8211; we could also do something similar by setting <em>average_across_timesteps</em><em> </em>to True &#8211; however, I am keeping things consistent with the TensorFlow tutorial.</p>
<p>In the next few steps, we set up some operations to calculate the accuracy off predictions over the batch samples:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># get the prediction accuracy
self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))
self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)
correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))
self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First we apply a softmax operation to get the predicted probabilities of each word for each output of the LSTM network. We then make the network predictions equal to those words with the highest softmax probability by using the <em>argmax</em> function. These predictions are then compared to the actual target words and then averaged to get the accuracy.</p>
<p>Now we move onto constructing the optimization operations &#8211; in this case we aren&#8217;t using a simple &#8220;out of the box&#8221; optimizer &#8211; rather we are doing a few manipulations to improve results:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if not is_training:
   return
self.learning_rate = tf.Variable(0.0, trainable=False)

tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)
optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
self.train_op = optimizer.apply_gradients(
            zip(grads, tvars),
            global_step=tf.contrib.framework.get_or_create_global_step())</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First off, if the model has been created for predictions, validations or testing only, these operations do not need to be created. The first step if the model is being used for training, is to create a learning rate variable. This will be used so that we can decrease the learning rate during training &#8211; this improves the final outcome of the model.</p>
<p>Next we wish to clip the size of the gradients in our network during back-propagation &#8211; this is recommended in recurrent neural networks to improve outcomes. Clipping values of between 1 and 5 are commonly used. Finally, we create the optimizer operation, using the <em>learning_rate </em>variable, and apply the clipped gradients.. Then a gradient descent step is performed &#8211; assigning this operation to <em>train_op</em>. This operation, <em>train_op</em>, will be called for each training batch.</p>
<p>The final two lines of the model creation involve the updating of the <em>learning_rate</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">self.new_lr = tf.placeholder(tf.float32, shape=[])
self.lr_update = tf.assign(self.learning_rate, self.new_lr)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, a placeholder is created which will be input via the <em>feed_dict</em> argument when running the training, <em>new_lr</em>. This new learning rate is then assigned to <em>learning_rate</em> via a <em>tf.assign</em> operation. This operation, <em>lr_update,</em> will be run at the beginning of each epoch.</p>
<p>Now that the model structure is fully created, we can move onto the training loops:</p>
<h2>Training the LSTM model</h2>
<p>The training function will take as input the training data, along with various model parameters (batch sizes, number of steps etc.). The first part of the function looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,
          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93):
    # setup data and models
    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)
    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,
              num_layers=num_layers)
    init_op = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First we create an Input object instance and a Model object instance, passing in the necessary parameters. Because the TensorFlow graph is being created during the initialization of these objects, the TensorFlow global variable initializer operation can only be properly run <em>after</em> the creation of these instances.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">orig_decay = lr_decay
with tf.Session() as sess:
    # start threads
    sess.run([init_op])
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    saver = tf.train.Saver()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next we start the session, and run the variable initializer operation. Because we are using queuing in the Input object, we also need to create a thread coordinator and start the running of the threads (for more information, see <a href="http://adventuresinmachinelearning.com/introduction-tensorflow-queuing/" target="_blank" rel="noopener">this tutorial</a>). If you skip this step, or put it before the creation of <em>training_input</em>, your program will hang. Finally, a saver instance is created as we want to store model training checkpoints and the final trained model.</p>
<p>Next, the epochal training loop is entered into:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">for epoch in range(num_epochs):
    new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)
    m.assign_lr(sess, learning_rate * new_lr_decay)
    current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))
    for step in range(training_input.epoch_size):
        if step % 50 != 0:
            cost, _, current_state = sess.run([m.cost, m.train_op, m.state],
                                                             feed_dict={m.init_state: current_state})
        else:
            cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],
                                                      feed_dict={m.init_state: current_state})
            print(&quot;Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}&quot;.format(epoch, step, cost, acc))
    # save a model checkpoint
    saver.save(sess, data_path + &#039;\\&#039; + model_save_name, global_step=epoch)
# do a final save
saver.save(sess, data_path + &#039;\\&#039; + model_save_name + &#039;-final&#039;)
# close threads
coord.request_stop()
coord.join(threads)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step in every epoch is to calculate the learning rate decay factor, which gradually decreases after <em>max_lr_epoch</em> number of epochs has been reached. This learning rate decay factor, <em>new_lr_decay</em>, is multiplied by the learning rate and assigned to the model by calling the Model method <em>assign_lr</em>. This method looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def assign_lr(self, session, lr_value):
    session.run(self.lr_update, feed_dict={self.new_lr: lr_value})</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As can be observed, this function simply runs the <em>lr_update </em>operation which was explained in the prior section.</p>
<p>The next step is to create a zeroed initial state tensor for our LSTM model &#8211; we assign this zeroed tensor to the variable <em>current_state</em>. Then each training operation is looped through within our specified epoch size. Every iteration we run the following operations: <em>m.train_op</em> and <em>m.state</em>. The <em>train_op </em>operation, as previously shown, calculates the clipped gradients of the model and takes a batched step to minimize the cost. The <em>state </em>operation returns the <em>state </em>of the final unrolled LSTM cell which we will require to input as the state for the next training batch &#8211; note that it replaces the contents of the <em>current_state</em> variable. This <em>current_state </em>variable is inserted into the <em>m.init_state </em>placeholder via the <em>feed_dict.</em></p>
<p>Every 50 iterations we also extract the current cost of the model in training, as well as the accuracy against the current training batch, to provide printed feedback during training. The outputs look like this:</p>
<blockquote><p>Epoch 9, Step 1850, cost: 96.185, accuracy: 0.198<br />
Epoch 9, Step 1900, cost: 94.755, accuracy: 0.235</p></blockquote>
<p>Finally, at the end of each epoch, we use the <em>saver </em>object to save a model checkpoint, and finally at the end of the training a final save of the state of the model is performed.</p>
<h3>Expected training outcomes</h3>
<p>The expected cost and accuracy progress through the epochs depends on the multitude of parameters supplied to the models and also the results of the random initialization of the variables. Training time is also dependent on whether you are using only CPUs, or whether you are using GPUs too (note, I have not tested the code on the Github repository with GPUs).</p>
<p>My model achieved an average cost and <em>training batch</em> accuracy on the order of 110-120 and 30%, respectively, after 38 epochs with the following paramters:</p>
<p>Hidden size:650, Number of steps:35, Initialization scale:0.05, Batch size:20, Number of stacked LSTM layers:2, Keep probability / dropout: 0.5</p>
<p>You are probably thinking the accuracy isn&#8217;t very high, and you are correct, however further training and a larger hidden layer would provide better final accuracy values. To perform further training on a larger network you really need to be using GPUs to accelerate the training &#8211; I&#8217;ll do this in a future post and present the results.</p>
<h2>Testing the model</h2>
<p>To test the model on the test or validation data, I&#8217;ve created another function called <em>test</em> which looks like so:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def test(model_path, test_data, reversed_dictionary):
    test_input = Input(batch_size=20, num_steps=35, data=test_data)
    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,
              num_layers=2)
    saver = tf.train.Saver()
    with tf.Session() as sess:
        # start threads
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))
        # restore the trained model
        saver.restore(sess, model_path)
        # get an average accuracy over num_acc_batches
        num_acc_batches = 30
        check_batch_idx = 25
        acc_check_thresh = 5
        accuracy = 0
        for batch in range(num_acc_batches):
            if batch == check_batch_idx:
                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],
                                                               feed_dict={m.init_state: current_state})
                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]
                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]
                print(&quot;True values (1st line) vs predicted values (2nd line):&quot;)
                print(&quot; &quot;.join(true_vals_string))
                print(&quot; &quot;.join(pred_string))
            else:
                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})
            if batch &gt;= acc_check_thresh:
                accuracy += acc
        print(&quot;Average accuracy: {:.3f}&quot;.format(accuracy / (num_acc_batches-acc_check_thresh)))
        # close threads
        coord.request_stop()
        coord.join(threads)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We start with creating an Input and Model class that matches our training Input and Model classes. It is important that key parameters match the training model, such as the hidden size, number of steps, batch size etc. We are going to load our saved model variables into the computational graph created by the test Model instance, and if the dimensions don&#8217;t match TensorFlow will throw an error.</p>
<p>Next we create a <em>tf.train.Saver() </em>operation &#8211; this will load all our saved model variables into our test model when we run the line <em>saver.restore(sess, model_path). </em>After dealing with all of the threads and creating a zeroed state variable, we setup some variables which relate to how we are going to assess the accuracy and look at some specific instances of predicted strings. Because we have to &#8220;warm up&#8221; the model by feeding it some data to get good state variables, we only measure the accuracy after a certain number of batches i.e. <em>acc_check_thresh.</em></p>
<p>When the batch number is equal to <em>check_batch_idx</em> the code runs the <em>m.predict</em> operation to extract the predictions for the particular batch of data. The first prediction of the batch is passed through the reverse dictionary to convert them back to actual words (along with the batch target words) and then compared with what should have been predicted via printing.</p>
<p>Using the trained model, we can see the following output:</p>
<p>True values (1st line) vs <em>predicted values</em> (2nd line):<br />
stock market is headed many traders were afraid to trust stock prices quoted on the big board &lt;eos&gt; the futures halt was even &lt;unk&gt; by big board floor traders &lt;eos&gt; it &lt;unk&gt; things up said<br />
<em>market market is n&#8217;t for traders say willing to buy the prices &lt;eos&gt; &lt;eos&gt; the big board &lt;eos&gt; the dow market is a worse &lt;eos&gt; the board traders traders &lt;eos&gt; the &#8216;s the to to</em><br />
Average accuracy: 0.283</p>
<p>The accuracy isn&#8217;t fantastic, but you can see the network is matching the &#8220;gist&#8221; of the sentence i.e. not producing all of the exact words but matching the general subject matter. As I mentioned above, in a future post I&#8217;ll present the data from a model trained for longer using GPUs.</p>
<p>I hope you enjoyed the post &#8211; it&#8217;s been a long one, but I hope that this gives you a solid foundation in understanding recurrent neural networks and LSTMs and how to implement them in TensorFlow. If you&#8217;d like to learn how to build LSTM networks in Keras, see <a href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/" target="_blank" rel="noopener">this tutorial</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, I&#8217;d recommend this inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.887814&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-learning-recurrent-neural-networks-in-python%2F">Deep Learning: Recurrent Neural Networks in Python</a><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.887814&#038;type=2&#038;subid=0" width="1" height="1" border="0"><noscript><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.887814&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></noscript></p>
<hr />
<p>&nbsp;</p>
	</div></article><nav class="mh-post-nav mh-row clearfix" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
<div class="mh-col-1-2 mh-post-nav-item mh-post-nav-prev">
<a href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" rel="prev"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-80x60.jpg" width="80" height="60" class="attachment-mh-magazine-lite-small size-mh-magazine-lite-small wp-post-image" alt="gensim Word2Vec - nearest words" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-80x60.jpg 80w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-326x245.jpg 326w" sizes="(max-width: 80px) 100vw, 80px"><noscript><img width="80" height="60" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-80x60.jpg" class="attachment-mh-magazine-lite-small size-mh-magazine-lite-small wp-post-image" alt="gensim Word2Vec - nearest words" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-80x60.jpg 80w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-326x245.jpg 326w" sizes="(max-width: 80px) 100vw, 80px" /></noscript><span>Previous</span><p>Python gensim Word2Vec tutorial with TensorFlow and Keras</p></a></div>
<div class="mh-col-1-2 mh-post-nav-item mh-post-nav-next">
<a href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" rel="next"><img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-80x60.jpg" width="80" height="60" class="attachment-mh-magazine-lite-small size-mh-magazine-lite-small wp-post-image" alt="PyTorch tutorial - fully connected neural network example architecture" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-80x60.jpg 80w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-326x245.jpg 326w" sizes="(max-width: 80px) 100vw, 80px"><noscript><img width="80" height="60" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-80x60.jpg" class="attachment-mh-magazine-lite-small size-mh-magazine-lite-small wp-post-image" alt="PyTorch tutorial - fully connected neural network example architecture" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-80x60.jpg 80w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-326x245.jpg 326w" sizes="(max-width: 80px) 100vw, 80px" /></noscript><span>Next</span><p>A PyTorch tutorial &#8211; deep learning in Python</p></a></div>
</nav>
		<div id="mh-comments" class="mh-comments-wrap">
			<h4 class="mh-widget-title">
				<span class="mh-widget-title-inner">
					9 Comments				</span>
			</h4>
			<ol class="commentlist mh-comment-list">
						<li id="comment-3510" class="comment even thread-even depth-1 mh-comment-item">
			<article id="div-comment-3510" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/3a77c14da85afad0feedbeaa0e3b5698-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/3a77c14da85afad0feedbeaa0e3b5698-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Dime</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3510">
							October 19, 2017 at 12:23 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Kudos!</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3510' onclick='return addComment.moveForm( "comment-3510", "3510", "respond", "432" )' aria-label='Reply to Dime'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
		<li id="comment-3782" class="comment odd alt thread-odd thread-alt depth-1 mh-comment-item">
			<article id="div-comment-3782" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/94d859232490671b374e96a7dc75500f-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/94d859232490671b374e96a7dc75500f-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Kumar</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3782">
							November 8, 2017 at 2:24 am						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>I learned a lot from this tutorial. Thank! Just one question. In the image &#8220;LSTM sample many-to-many classifier&#8221;, should the indices go from x0&#8230;x35, likewise h0&#8230;h35. In the current illustration, I do not understand why there is feedback within a batch (i.e., across rows &#8211; which is of size 20). Please clarify. Thanks.</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3782' onclick='return addComment.moveForm( "comment-3782", "3782", "respond", "432" )' aria-label='Reply to Kumar'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
		<li id="comment-3791" class="comment even thread-even depth-1 mh-comment-item">
			<article id="div-comment-3791" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/743478e2f9af0950950800b9d96dbc74-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/743478e2f9af0950950800b9d96dbc74-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Lew</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3791">
							November 10, 2017 at 9:07 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Can&#8217;t find the code on your Github site.  How can I access it?<br />
Cheers</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3791' onclick='return addComment.moveForm( "comment-3791", "3791", "respond", "432" )' aria-label='Reply to Lew'>Reply</a>                </div>
			</article><ul class="children">
		<li id="comment-3792" class="comment byuser comment-author-andyt81 bypostauthor odd alt depth-2 mh-comment-item">
			<article id="div-comment-3792" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/433e00225f87d7ffde96af7389d36310-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/433e00225f87d7ffde96af7389d36310-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Andy</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3792">
							November 10, 2017 at 9:56 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Hi Lew &#8211; the file is called lstm_tutorial.py @ <a href="https://github.com/adventuresinML/adventures-in-ml-code" rel="nofollow">https://github.com/adventuresinML/adventures-in-ml-code</a></p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3792' onclick='return addComment.moveForm( "comment-3792", "3792", "respond", "432" )' aria-label='Reply to Andy'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li id="comment-3842" class="comment even thread-odd thread-alt depth-1 mh-comment-item">
			<article id="div-comment-3842" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/8863037a6154ef65c29aa468da7d2132-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/8863037a6154ef65c29aa468da7d2132-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Mark</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3842">
							November 28, 2017 at 7:08 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Another excellent post Andy, thanks for taking the time.<br />
I will make an attempt on gpu also, so that when you post the update to this I can see how on track I was.</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3842' onclick='return addComment.moveForm( "comment-3842", "3842", "respond", "432" )' aria-label='Reply to Mark'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
		<li id="comment-3962" class="comment odd alt thread-even depth-1 mh-comment-item">
			<article id="div-comment-3962" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/ca1ab00ac2b7dae053a25b15ae705af6-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/ca1ab00ac2b7dae053a25b15ae705af6-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Jon</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-3962">
							January 6, 2018 at 12:09 am						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Hi Andy,</p>
<p>Thank you for all the time you&#8217;ve spent making these neural network tutorials, they have helped me a great deal. I have a question, though, that I haven&#8217;t seen specifically answered anywhere and was wondering if you would be able to help.</p>
<p>Do you know if TensorFlow LSTMs can handle multiple datastreams at once? Working from your example, it would be like having 4 35&#215;650 inputs at once, for a 20x4x35x650 input. I know I could convert 4x35x650 into 140&#215;650 but I was wondering if there was a more elegant way to do this that would allow all four inputs to be considered at once instead of in some sort of sequential fashion.</p>
<p>Thanks for your time</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-3962' onclick='return addComment.moveForm( "comment-3962", "3962", "respond", "432" )' aria-label='Reply to Jon'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
		<li id="comment-4052" class="comment even thread-odd thread-alt depth-1 mh-comment-item">
			<article id="div-comment-4052" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/efc8b734d0ec4f90b49ef18115a6252e-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/efc8b734d0ec4f90b49ef18115a6252e-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Jack</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-4052">
							February 6, 2018 at 3:31 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Hi Andy, I have the same question that Kumar asked before:<br />
&gt; In the image “LSTM sample many-to-many classifier”, should the indices go from x0…x35, likewise h0…h35. In the current illustration, I do not understand why there is feedback within a batch (i.e., across rows – which is of size 20)</p>
<p>Could you please help me on this? Thanks! 🙂</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-4052' onclick='return addComment.moveForm( "comment-4052", "4052", "respond", "432" )' aria-label='Reply to Jack'>Reply</a>                </div>
			</article><ul class="children">
		<li id="comment-4054" class="comment byuser comment-author-andyt81 bypostauthor odd alt depth-2 mh-comment-item">
			<article id="div-comment-4054" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/433e00225f87d7ffde96af7389d36310-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/433e00225f87d7ffde96af7389d36310-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Andy</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-4054">
							February 6, 2018 at 8:16 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Hi Jack and Kumar &#8211; you are correct, I have updated the diagram now. Thanks for picking it up</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-4054' onclick='return addComment.moveForm( "comment-4054", "4054", "respond", "432" )' aria-label='Reply to Andy'>Reply</a>                </div>
			</article><ul class="children">
		<li id="comment-4058" class="comment even depth-3 mh-comment-item">
			<article id="div-comment-4058" class="mh-comment-body">
				<footer class="mh-comment-footer clearfix">
					<figure class="mh-comment-gravatar">
						<img src="http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/images/1x1.trans.gif" data-lazy-src="http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/efc8b734d0ec4f90b49ef18115a6252e-s80.jpg" alt class="avatar avatar-80" width="80" height="80"><noscript><img alt="" src='http://adventuresinmachinelearning.com/wp-content/uploads/gravatar/efc8b734d0ec4f90b49ef18115a6252e-s80.jpg' class="avatar avatar-80" width="80" height="80" /></noscript>					</figure>
					<div class="mh-meta mh-comment-meta">
						<div class="vcard author mh-comment-meta-author">
							<span class="fn">Jack</span>
						</div>
						<a class="mh-comment-meta-date" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/#comment-4058">
							February 7, 2018 at 2:40 pm						</a>
					</div>
				</footer>
								<div class="entry-content mh-comment-content">
					<p>Thank you for the tutorial and the answer!</p>
				</div>
				<div class="mh-meta mh-comment-meta-links"><a rel='nofollow' class='comment-reply-link' href='#comment-4058' onclick='return addComment.moveForm( "comment-4058", "4058", "respond", "432" )' aria-label='Reply to Jack'>Reply</a>                </div>
			</article></li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
			</ol>
		</div>	<div id="respond" class="comment-respond">
		<h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="/recurrent-neural-networks-lstm-tutorial-tensorflow/#respond" style="display:none;">Cancel reply</a></small></h3>			<form action="http://adventuresinmachinelearning.com/wp-comments-post.php" method="post" id="commentform" class="comment-form" novalidate>
				<p class="comment-notes">Your email address will not be published.</p><p class="comment-form-comment"><label for="comment">Comment</label><br/><textarea id="comment" name="comment" cols="45" rows="5" aria-required="true"></textarea></p><p class="comment-form-author"><label for="author">Name</label><span class="required">*</span><br/><input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">Email</label><span class="required">*</span><br/><input id="email" name="email" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">Website</label><br/><input id="url" name="url" type="text" value="" size="30" /></p>
<p class="form-submit"><input name="submit" type="submit" id="submit" class="submit" value="Post Comment" /> <input type='hidden' name='comment_post_ID' value='432' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
</p><input type='hidden' id='ct_checkjs_ac1dd209cbcc5e5d1c6e28598e8cbbe8' name='ct_checkjs' value='0' /><script type='text/javascript'>setTimeout(function(){var ct_input_name = 'ct_checkjs_ac1dd209cbcc5e5d1c6e28598e8cbbe8';if (document.getElementById(ct_input_name) !== null) {var ct_input_value = document.getElementById(ct_input_name).value;document.getElementById(ct_input_name).value = document.getElementById(ct_input_name).value.replace(ct_input_value, '972931588');}}, 1000);</script>			</form>
			</div><!-- #respond -->
		</div>
	<aside class="mh-widget-col-1 mh-sidebar" itemscope="itemscope" itemtype="http://schema.org/WPSideBar"><div id="text-6" class="mh-widget widget_text"><h4 class="mh-widget-title"><span class="mh-widget-title-inner">Popular tutorials</span></h4>			<div class="textwidget"><ul>
<li><a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">Neural Networks Tutorial – A Pathway to Deep Learning</a></li>
<li><a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/">Python TensorFlow Tutorial – Build a Neural Network</a></li>
<li><a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/">Convolutional Neural Networks Tutorial in TensorFlow</a></li>
<li><a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras tutorial &#8211; build a convolutional neural network in 11 lines</a></li>
<li><a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec word embedding tutorial in Python and TensorFlow</a></li>
</ul>
</div>
		</div><div id="categories-2" class="mh-widget widget_categories"><h4 class="mh-widget-title"><span class="mh-widget-title-inner">Categories</span></h4>		<ul>
	<li class="cat-item cat-item-22"><a href="http://adventuresinmachinelearning.com/category/amazon-aws/" >Amazon AWS</a>
</li>
	<li class="cat-item cat-item-15"><a href="http://adventuresinmachinelearning.com/category/deep-learning/cntk/" >CNTK</a>
</li>
	<li class="cat-item cat-item-11"><a href="http://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/" >Convolutional Neural Networks</a>
</li>
	<li class="cat-item cat-item-2"><a href="http://adventuresinmachinelearning.com/category/deep-learning/" >Deep learning</a>
</li>
	<li class="cat-item cat-item-16"><a href="http://adventuresinmachinelearning.com/category/nlp/gensim/" >gensim</a>
</li>
	<li class="cat-item cat-item-23"><a href="http://adventuresinmachinelearning.com/category/deep-learning/gpus/" >GPUs</a>
</li>
	<li class="cat-item cat-item-12"><a href="http://adventuresinmachinelearning.com/category/deep-learning/keras/" >Keras</a>
</li>
	<li class="cat-item cat-item-18"><a href="http://adventuresinmachinelearning.com/category/deep-learning/lstms/" >LSTMs</a>
</li>
	<li class="cat-item cat-item-3"><a href="http://adventuresinmachinelearning.com/category/deep-learning/neural-networks/" >Neural networks</a>
</li>
	<li class="cat-item cat-item-13"><a href="http://adventuresinmachinelearning.com/category/nlp/" >NLP</a>
</li>
	<li class="cat-item cat-item-10"><a href="http://adventuresinmachinelearning.com/category/optimisation/" >Optimisation</a>
</li>
	<li class="cat-item cat-item-19"><a href="http://adventuresinmachinelearning.com/category/deep-learning/pytorch/" >PyTorch</a>
</li>
	<li class="cat-item cat-item-17"><a href="http://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/" >Recurrent neural networks</a>
</li>
	<li class="cat-item cat-item-24"><a href="http://adventuresinmachinelearning.com/category/reinforcement-learning/" >Reinforcement learning</a>
</li>
	<li class="cat-item cat-item-9"><a href="http://adventuresinmachinelearning.com/category/deep-learning/tensorflow/" >TensorFlow</a>
</li>
	<li class="cat-item cat-item-25"><a href="http://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/" >Weight initialization</a>
</li>
	<li class="cat-item cat-item-14"><a href="http://adventuresinmachinelearning.com/category/nlp/word2vec/" >Word2Vec</a>
</li>
		</ul>
</div><div id="mc4wp_form_widget-2" class="mh-widget widget_mc4wp_form_widget"><h4 class="mh-widget-title"><span class="mh-widget-title-inner">Newsletter + free eBook</span></h4><script type="text/javascript">(function() {
	if (!window.mc4wp) {
		window.mc4wp = {
			listeners: [],
			forms    : {
				on: function (event, callback) {
					window.mc4wp.listeners.push({
						event   : event,
						callback: callback
					});
				}
			}
		}
	}
})();
</script><!-- MailChimp for WordPress v4.1.0 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form id="mc4wp-form-2" class="mc4wp-form mc4wp-form-165" method="post" data-id="165" data-name="Main sign-up" ><div class="mc4wp-form-fields"><p>
	<label>Email address: </label>
	<input type="email" name="EMAIL" placeholder="Your email address" required />
</p>
<p>
  
</p>
<p>
	<input type="submit" value="Sign up" />
</p><div style="display: none;"><input type="text" name="_mc4wp_honeypot" value="" tabindex="-1" autocomplete="off" /></div><input type="hidden" name="_mc4wp_timestamp" value="1536643235" /><input type="hidden" name="_mc4wp_form_id" value="165" /><input type="hidden" name="_mc4wp_form_element_id" value="mc4wp-form-2" /></div><div class="mc4wp-response"></div></form><!-- / MailChimp for WordPress Plugin --></div><div id="text-7" class="mh-widget widget_text"><h4 class="mh-widget-title"><span class="mh-widget-title-inner">Find us on Facebook</span></h4>			<div class="textwidget"><div class="fb-page" data-href="https://www.facebook.com/adventuresinml/" data-small-header="true" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="true"><blockquote cite="https://www.facebook.com/adventuresinml/" class="fb-xfbml-parse-ignore"><a href="https://www.facebook.com/adventuresinml/">Adventures in Machine Learning</a></blockquote></div></div>
		</div></aside></div>
	<footer class="mh-footer" itemscope="itemscope" itemtype="http://schema.org/WPFooter">
		<div class="mh-container mh-container-inner mh-footer-widgets mh-row clearfix">
							<div class="mh-col-1-4 mh-widget-col-1 mh-footer-area mh-footer-1">
					<div id="text-8" class="mh-footer-widget widget_text">			<div class="textwidget"><p>Note: some posts contain Udemy affiliate links</p>
</div>
		</div>				</div>
														</div>
	</footer>
<div class="mh-copyright-wrap">
	<div class="mh-container mh-container-inner clearfix">
		<p class="mh-copyright">Copyright &copy; 2018 | WordPress Theme by <a href="https://www.mhthemes.com/" rel="nofollow">MH Themes</a></p>
	</div>
</div>
</div><!-- .mh-container-outer -->
<script type='text/javascript'>function ctSetCookie(c_name, value, def_value){document.cookie = c_name + '=' + escape(value) + '; path=/';}ctSetCookie('ct_checkjs', '972931588', '0');</script><script type="text/javascript">(function() {function addEventListener(element,event,handler) {
	if(element.addEventListener) {
		element.addEventListener(event,handler, false);
	} else if(element.attachEvent){
		element.attachEvent('on'+event,handler);
	}
}function maybePrefixUrlField() {
	if(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {
		this.value = "http://" + this.value;
	}
}

var urlFields = document.querySelectorAll('.mc4wp-form input[type="url"]');
if( urlFields && urlFields.length > 0 ) {
	for( var j=0; j < urlFields.length; j++ ) {
		addEventListener(urlFields[j],'blur',maybePrefixUrlField);
	}
}/* test if browser supports date fields */
var testInput = document.createElement('input');
testInput.setAttribute('type', 'date');
if( testInput.type !== 'date') {

	/* add placeholder & pattern to all date fields */
	var dateFields = document.querySelectorAll('.mc4wp-form input[type="date"]');
	for(var i=0; i<dateFields.length; i++) {
		if(!dateFields[i].placeholder) {
			dateFields[i].placeholder = 'YYYY-MM-DD';
		}
		if(!dateFields[i].pattern) {
			dateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';
		}
	}
}

})();</script><style type="text/css" media="all">.code-syntax,.code-syntax code{font-family:Menlo,Consolas,'Liberation Mono',monospace;font-size:14px;color:#009}.code-syntax{padding:.8em 1.6em;overflow-x:auto !important;border:1px solid #CCC;border-radius:4px}</style><style type="text/css" media="all"> code[class*="language-"],pre[class*="language-"]{color:#393A34;font-family:Menlo,Consolas,"Liberation Mono",monospace;direction:ltr;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;font-size:14px;line-height:1.6em;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*="language-"]::-moz-selection,pre[class*="language-"] ::-moz-selection,code[class*="language-"]::-moz-selection,code[class*="language-"] ::-moz-selection{background:#b3d4fc}pre[class*="language-"]::selection,pre[class*="language-"] ::selection,code[class*="language-"]::selection,code[class*="language-"] ::selection{background:#b3d4fc}pre[class*="language-"]{padding:1em;margin:.5em 0;overflow:auto;border:1px solid #dddddd;background-color:white}:not(pre) > code[class*="language-"],pre[class*="language-"]{}:not(pre) > code[class*="language-"]{padding:.2em;padding-top:1px;padding-bottom:1px;background:#f8f8f8;border:1px solid #dddddd}.token.comment,.token.prolog,.token.doctype,.token.cdata{color:#999988;font-style:italic}.token.namespace{opacity:.7}.token.string,.token.attr-value{color:#e3116c}.token.punctuation,.token.operator{color:#393A34}.token.entity,.token.url,.token.symbol,.token.number,.token.boolean,.token.variable,.token.constant,.token.property,.token.regex,.token.inserted{color:#36acaa}.token.atrule,.token.keyword,.token.attr-name,.language-autohotkey .token.selector{color:#00a4db}.token.function,.token.deleted,.language-autohotkey .token.tag{color:#9a050f}.token.tag,.token.selector,.language-autohotkey .token.keyword{color:#00009f}.token.important,.token.function,.token.bold{font-weight:bold}.token.italic{font-style:italic}</style><style type="text/css" media="all">@charset "UTF-8";#edd_checkout_form_wrap label:after,.edd_clearfix:after{visibility:hidden;float:none;text-indent:-9999px;content:".";clear:both}#edd_login_form label,#edd_register_form label,.edd-amazon-logout a,input.edd_submit_plain{cursor:pointer}.edd-icon{display:inline-block;fill:currentColor;position:relative;top:-.0625em;vertical-align:middle;width:1em;height:1em}.edd-icon-lock{top:-.125rem}.edd-icon-spin{display:inline-block;-moz-animation:edd-icon-spin 2s infinite linear;-o-animation:edd-icon-spin 2s infinite linear;-webkit-animation:edd-icon-spin 2s infinite linear;animation:edd-icon-spin 2s infinite linear}@-moz-keyframes edd-icon-spin{0%{-moz-transform:rotate(0)}100%{-moz-transform:rotate(359deg)}}@-webkit-keyframes edd-icon-spin{0%{-webkit-transform:rotate(0)}100%{-webkit-transform:rotate(359deg)}}@-o-keyframes edd-icon-spin{0%{-o-transform:rotate(0)}100%{-o-transform:rotate(359deg)}}@-ms-keyframes edd-icon-spin{0%{-ms-transform:rotate(0)}100%{-ms-transform:rotate(359deg)}}@keyframes edd-icon-spin{0%{transform:rotate(0)}100%{transform:rotate(359deg)}}.edd_clearfix:after{display:block}#edd_checkout_cart{text-align:left;width:100%;border:none;margin:0 0 21px;table-layout:auto}#edd_checkout_cart td,#edd_checkout_cart th{text-align:left;border:1px solid #eee;color:#666;padding:.5em 1.387em}#edd_checkout_cart .edd_cart_header_row th{background:#fafafa;padding:1.387em}#edd_checkout_cart .edd_cart_discount_row th,#edd_checkout_cart .edd_cart_tax_row th{background:0 0}#edd_checkout_cart th{font-weight:700}#edd_checkout_cart td{line-height:25px;vertical-align:middle;background:#fff}#edd_checkout_cart td.edd_cart_actions,#edd_checkout_cart td:last-child,#edd_checkout_cart th.edd_cart_actions,#edd_checkout_cart th.edd_cart_total,#edd_checkout_cart th:last-child{text-align:right}#edd_checkout_cart td img{float:left;margin:0 8px 0 0;background:0 0;padding:0;border:none}#edd_checkout_cart input.edd-item-quantity{width:3em;padding:2px}#edd_checkout_cart .edd_discount{display:inline-block;margin-left:5px}.edd_discount_remove{display:inline-block;width:10px;height:11px;background:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/xit.gif) no-repeat;position:relative;top:3px}.edd_discount_remove:hover{background-position:-10px 0}#edd_checkout_cart br{display:none}#edd_checkout_cart a.edd-cart-saving-button{font-weight:400;text-decoration:none}#edd_checkout_form_wrap legend{display:block;font-size:120%;line-height:1;font-weight:700;width:100%;margin:0 0 21px;padding:0}#edd_checkout_form_wrap label{font-weight:700;display:block;position:relative;line-height:100%;font-size:95%;margin:0 0 5px}#edd_checkout_form_wrap label:after{display:block;height:0}#edd_checkout_form_wrap span.edd-description{color:#666;font-size:80%;display:block;margin:0 0 5px}#edd_checkout_form_wrap input.edd-input,#edd_checkout_form_wrap textarea.edd-input{display:inline-block;width:70%}#edd_checkout_form_wrap select.edd-select{display:block;width:60%}#edd_checkout_form_wrap select.edd-select.edd-select-small{display:inline;width:auto}#edd_checkout_form_wrap input.edd-input.error,#edd_checkout_form_wrap textarea.edd-input.error{border-color:#c4554e}#edd_checkout_form_wrap>p{margin:0 0 21px}#edd_checkout_form_wrap span.edd-required-indicator{color:#b94a48;display:inline}#edd_checkout_form_wrap input[type=text],#edd_checkout_form_wrap input[type=email],#edd_checkout_form_wrap input[type=password],#edd_checkout_form_wrap input[type=tel],#edd_checkout_form_wrap textarea{padding:4px 6px}#edd_checkout_form_wrap input[type=radio]{border:none;margin-right:5px}#edd_checkout_form_wrap input[type=checkbox]{display:inline-block;margin:0 5px 0 0}#edd_checkout_form_wrap input[type=checkbox]+label,#edd_checkout_form_wrap input[type=checkbox]+label:after{display:inline}#edd_checkout_form_wrap .edd-payment-icons{height:32px;display:block;margin:0 0 8px}#edd_checkout_form_wrap .edd-payment-icons img.payment-icon{max-height:32px;width:auto;margin:0 3px 0 0;float:left;background:0 0;padding:0;border:none;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none}#edd_checkout_form_wrap #edd-payment-mode-wrap label{display:inline-block;margin:0 20px 0 0}#edd_checkout_form_wrap #edd-payment-mode-wrap .edd-payment-mode-label{font-weight:700;display:inline-block;position:relative;margin-bottom:5px}#edd_checkout_form_wrap fieldset{border:1px solid #eee;padding:1.387em;margin:0 0 21px}#edd_checkout_form_wrap #edd_discount_code,#edd_checkout_form_wrap #edd_purchase_submit,#edd_checkout_form_wrap #edd_register_account_fields{padding:0;border:none}#edd_checkout_form_wrap fieldset fieldset{margin:0;border:none;padding:0}#edd_checkout_form_wrap #edd-login-account-wrap,#edd_checkout_form_wrap #edd-new-account-wrap,#edd_checkout_form_wrap #edd_final_total_wrap,#edd_checkout_form_wrap #edd_show_discount,#edd_checkout_form_wrap .edd-cart-adjustment{background:#fafafa;color:#666;padding:.5em 1.387em}#edd_checkout_form_wrap #edd-discount-code-wrap,#edd_checkout_form_wrap #edd_final_total_wrap,#edd_checkout_form_wrap #edd_show_discount{border:1px solid #eee}#edd_checkout_form_wrap .edd-cart-adjustment{padding:1.387em}#edd_checkout_form_wrap .edd-cart-adjustment input.edd-input,#edd_checkout_form_wrap .edd-cart-adjustment input.edd-submit{display:inline-block}#edd_checkout_form_wrap .edd-cart-adjustment input.edd-submit{padding:3px 12px;margin-bottom:2px}#edd_checkout_form_wrap #edd_purchase_final_total p,#edd_checkout_form_wrap fieldset#edd_register_account_fields p.edd_login_password,#edd_checkout_form_wrap fieldset#edd_register_account_fields p.edd_register_password{margin:0}#edd_checkout_form_wrap #edd-discount-error-wrap{width:100%;display:inline-block;margin:1em 0 0}#edd_checkout_form_wrap #edd-login-account-wrap,#edd_checkout_form_wrap #edd-new-account-wrap{margin:-1.387em -1.387em 21px;border-left:none;border-right:none;border-top:none}#edd_checkout_form_wrap #edd_payment_mode_select,#edd_checkout_form_wrap fieldset#edd_register_fields #edd_checkout_user_info{margin-bottom:21px}#edd_checkout_form_wrap fieldset#edd_register_account_fields legend{padding-top:11px}#edd_checkout_form_wrap fieldset#edd_cc_fields{border:1px solid #f0f0f0;background:#f9f9f9;position:relative}#edd_checkout_form_wrap fieldset#edd_cc_fields legend{border:none;padding:0}#edd_checkout_form_wrap fieldset p:last-child{margin-bottom:0}#edd_checkout_form_wrap fieldset#edd_cc_fields #edd-card-number-wrap{margin-top:5px}#edd_checkout_form_wrap #edd_purchase_final_total{margin:21px 0}#edd_secure_site_wrapper{padding:4px 4px 4px 0;font-weight:700}#edd_checkout_form_wrap input.edd-input.card-number.valid{background-image:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/tick.png);background-repeat:no-repeat;background-position:98% 50%}#edd_checkout_form_wrap span.exp-divider{display:inline}#edd_checkout_form_wrap span.card-type{position:absolute;top:-2px;right:0;width:43px;height:32px;background-size:43px 32px!important}#edd_checkout_form_wrap span.card-type.off{display:none}#edd_checkout_form_wrap span.card-type.visa{background:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/icons/visa.png) no-repeat}#edd_checkout_form_wrap span.card-type.mastercard{background:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/icons/mastercard.png) no-repeat}#edd_checkout_form_wrap span.card-type.discover{background:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/icons/discover.png) no-repeat}#edd_checkout_form_wrap span.card-type.amex{background:url(http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/templates/images/icons/americanexpress.png) no-repeat}#edd_checkout_form_wrap .edd-cart-ajax{-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none}.edd-amazon-profile-wrapper{font-size:12px}.edd-amazon-profile-name{font-weight:600}.edd-amazon-logout{font-size:10px;line-height:12px}#edd-amazon-address-box,#edd-amazon-wallet-box{height:228px;width:350px}#edd-amazon-address-box{margin-bottom:15px}@media only screen and (min-width:768px){#edd-amazon-address-box,#edd-amazon-wallet-box{width:100%;height:228px}}.edd_purchase_submit_wrapper{position:relative}.edd_purchase_submit_wrapper a.edd-add-to-cart{text-decoration:none;display:none;position:relative;overflow:hidden}.edd_purchase_submit_wrapper a.edd-add-to-cart.edd-has-js{display:inline-block}.edd_purchase_submit_wrapper .edd-cart-ajax{display:none;position:relative;left:-35px}.edd-submit.button.edd-ajax-loading{padding-right:30px}.edd-add-to-cart .edd-add-to-cart-label{opacity:1;filter:alpha(opacity=100)}.edd-loading,.edd-loading:after{border-radius:50%;display:block;width:1.5em;height:1.5em}.edd-loading{-webkit-animation:edd-spinning 1.1s infinite linear;animation:edd-spinning 1.1s infinite linear;border-top:.2em solid rgba(255,255,255,.2);border-right:.2em solid rgba(255,255,255,.2);border-bottom:.2em solid rgba(255,255,255,.2);border-left:.2em solid #fff;font-size:.75em;position:absolute;left:calc(50% - .75em);top:calc(50% - .75em);opacity:0;filter:alpha(opacity=0);-ms-transform:translateZ(0);transform:translateZ(0)}.edd-discount-loader.edd-loading,.edd-loading-ajax.edd-loading,a.edd-add-to-cart.white .edd-loading{border-top-color:rgba(0,0,0,.2);border-right-color:rgba(0,0,0,.2);border-bottom-color:rgba(0,0,0,.2);border-left-color:#000}.edd-loading-ajax.edd-loading{display:inline-block;position:relative;top:0;left:.25em;vertical-align:middle;opacity:1}#edd_checkout_form_wrap .edd-cart-adjustment .edd-apply-discount.edd-submit{display:inline-block}.edd-discount-loader.edd-loading{display:inline-block;position:relative;left:auto;vertical-align:middle;width:1.25em;height:1.25em}@-webkit-keyframes edd-spinning{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes edd-spinning{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.edd-loading,a.edd-add-to-cart .edd-add-to-cart-label{-webkit-transition:.1s opacity!important;-moz-transition:.1s opacity!important;-ms-transition:.1s opacity!important;-o-transition:.1s opacity!important;transition:.1s opacity!important}.edd-add-to-cart[data-edd-loading] .edd-add-to-cart-label{opacity:0;filter:alpha(opacity=0)}.edd-add-to-cart[data-edd-loading] .edd-loading,.edd-discount-loader.edd-loading{opacity:1;filter:alpha(opacity=100)}.edd-cart-added-alert{color:#567622;display:block;position:absolute}.edd-cart-ajax,body.edd_receipt_page:before{position:relative}.edd_form input.edd-input.required,.edd_form select.edd-select.required{color:#000}body.edd_receipt_page{background-color:#fff;color:#141412;margin:0;font-family:Helvetica,sans-serif;font-size:12px}#edd_user_history .edd_purchase_status.cancelled,#edd_user_history .edd_purchase_status.failed,#edd_user_history .edd_purchase_status.pending,#edd_user_history .edd_purchase_status.revoked,table#edd_purchase_receipt .edd_receipt_payment_status.cancelled,table#edd_purchase_receipt .edd_receipt_payment_status.failed,table#edd_purchase_receipt .edd_receipt_payment_status.pending,table#edd_purchase_receipt .edd_receipt_payment_status.revoked{color:#f73f2e}body.edd_receipt_page #edd_receipt_wrapper{width:660px;margin:0 auto;padding:50px 0}body.edd_receipt_page table{display:table;width:100%;border-bottom:1px solid #ededed;border-collapse:collapse;border-spacing:0;font-size:14px;line-height:2;margin:0 0 20px}body.edd_receipt_page td,body.edd_receipt_page th{display:table-cell;text-align:left;border-top:1px solid #ededed;padding:6px 10px;font-weight:400}body.edd_receipt_page th{font-weight:700;text-transform:uppercase}body.edd_receipt_page h3{font-size:22px;margin:40px 0 5px;clear:both;display:block;font-weight:700}body.edd_receipt_page li{list-style:none}table#edd_purchase_receipt,table#edd_purchase_receipt_products{width:100%}table#edd_purchase_receipt td,table#edd_purchase_receipt th,table#edd_purchase_receipt_products td,table#edd_purchase_receipt_products th{text-align:left}table#edd_purchase_receipt_products li{list-style:none;margin:0 0 8px 10px}table#edd_purchase_receipt ul,table#edd_purchase_receipt_products ul.edd_purchase_receipt_files{margin:0;padding:0}table#edd_purchase_receipt li.edd_download_file{list-style:none;margin:0 0 8px}table#edd_purchase_receipt_products .edd_purchase_receipt_product_notes{font-style:italic}table#edd_purchase_receipt_products .edd_purchase_receipt_product_name{font-weight:700}table#edd_purchase_receipt_products .edd_bundled_product_name{font-style:italic;font-weight:700}#edd_user_history{text-align:left;width:100%;border-top:1px solid #f0f0f0;border-bottom:none}#edd_user_history td,#edd_user_history th{text-align:left;padding:3px 5px;border-bottom:1px solid #f0f0f0;border-top:none}#edd_user_history th{font-weight:700;background:#f5f5f5}#edd_user_history td{line-height:25px;vertical-align:middle}#edd_login_form legend,#edd_register_form legend{font-size:120%;margin-bottom:1em}#edd_login_form fieldset,#edd_register_form fieldset{border:none}#edd_login_form .edd-input,#edd_register_form .edd-input{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}#edd_profile_editor_form p{margin-bottom:8px}#edd_profile_editor_form label{display:inline-block}#edd_profile_editor_form .edd-profile-emails{list-style-type:none;display:inline-table;margin-left:0;margin-bottom:0}#edd_profile_editor_form .edd-profile-email{width:auto}#edd_profile_editor_form .edd-profile-email .actions{display:none}#edd_profile_editor_form .edd-profile-email:hover>span{display:inline-block}.edd_added_to_cart_alert{padding:5px;font-size:14px;border:1px solid #046a9e;background:#9ecce2;color:#333;margin:8px 0}.edd_added_to_cart_alert a.edd_alert_checkout_link{color:#000!important}input.edd_submit_plain{background:0 0!important;border:none!important;padding:0!important;display:inline}.single-download .edd_download_purchase_form{margin-bottom:1.387em}.edd_download_purchase_form .edd_download_quantity_wrapper{margin:0 0 .5em}.edd_download_purchase_form .edd_download_quantity_wrapper .edd-item-quantity{width:75px}.edd_download_purchase_form .edd_price_options{margin:0 0 15px}.edd_download_purchase_form .edd_price_options ul{margin:0;padding:0;list-style:none}.edd_download_purchase_form .edd_price_options li{display:block;padding:0;margin:0}.edd_download_purchase_form .edd_price_options span{display:inline;padding:0;margin:0}.edd_download_purchase_form .edd_price_options .edd_download_quantity_wrapper{padding-left:18px}.edd_download_purchase_form .edd_price_options .edd_download_quantity_wrapper *{font-size:80%}.edd_download_purchase_form .edd_price_options input.edd-item-quantity{display:inline;width:50px;max-width:90%}#edd-purchase-button,.edd-submit,input[type=submit].edd-submit{display:inline-block;padding:6px 12px;margin:0;font-size:14px;font-weight:400;line-height:1.428571429;text-align:center;white-space:nowrap;vertical-align:middle;cursor:pointer;border:1px solid #ccc;border-radius:4px;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-o-user-select:none;user-select:none}.edd-submit.button:focus,input[type=submit].edd-submit:focus{outline:#333 dotted thin;outline:-webkit-focus-ring-color auto 5px;outline-offset:-2px}.edd-submit.button:active{background-image:none;outline:0;-webkit-box-shadow:inset 0 3px 5px rgba(0,0,0,.125);box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.edd-submit.plain{padding:0;border:none;-webkit-border-radius:0;-moz-border-radius:0;border-radius:0}.edd-submit.button,.edd-submit.button.gray,.edd-submit.button:visited{color:#333;background:#f0f0f0;border-color:#ccc}.edd-submit.button.gray:active,.edd-submit.button.gray:focus,.edd-submit.button.gray:hover,.edd-submit.button:active,.edd-submit.button:focus,.edd-submit.button:hover{color:#333;background:#ebebeb;border-color:#adadad}.edd-submit.button.gray:active{background-image:none}.edd-submit.button.white{color:#333;background:#fff;border-color:#ccc}.edd-submit.button.white:active,.edd-submit.button.white:focus,.edd-submit.button.white:hover{color:#333;background:#ebebeb;border-color:#adadad}.edd-submit.button.white:active{background-image:none}.edd-submit.button.blue{color:#fff;background:#428bca;border-color:#357ebd}.edd-submit.button.blue.active,.edd-submit.button.blue:focus,.edd-submit.button.blue:hover{color:#fff;background:#3276b1;border-color:#285e8e}.edd-submit.button.blue.active{background-image:none}.edd-submit.button.red{color:#fff;background:#d9534f;border-color:#d43f3a}.edd-submit.button.red:active,.edd-submit.button.red:focus,.edd-submit.button.red:hover{color:#fff;background:#d2322d;border-color:#ac2925}.edd-submit.button.red:active{background-image:none}.edd-submit.button.green{color:#fff;background:#5cb85c;border-color:#4cae4c}.edd-submit.button.green:active,.edd-submit.button.green:focus,.edd-submit.button.green:hover{color:#fff;background:#47a447;border-color:#398439}.edd-submit.button.green:active{background-image:none}.edd-submit.button.yellow{color:#fff;background:#f0ad4e;border-color:#eea236}.edd-submit.button.yellow:active,.edd-submit.button.yellow:focus,.edd-submit.button.yellow:hover{color:#fff;background:#ed9c28;border-color:#d58512}.edd-submit.button.yellow:active{background-image:none}.edd-submit.button.orange{color:#fff;background:#ed9c28;border-color:#e3921e}.edd-submit.button.orange:active,.edd-submit.button.orange:focus,.edd-submit.button.orange:hover{color:#fff;background:#e59016;border-color:#d58512}.edd-submit.button.orange:active{background-image:none}.edd-submit.button.dark-gray{color:#fff;background:#363636;border-color:#222}.edd-submit.button.dark-gray:active,.edd-submit.button.dark-gray:focus,.edd-submit.button.dark-gray:hover{color:#fff;background:#333;border-color:#adadad}.edd-submit.button.dark-gray:active{background-image:none}.edd_downloads_list:after{content:"";display:table;clear:both}.edd_download{float:left}.edd_download_columns_1 .edd_download{width:100%}.edd_download_columns_2 .edd_download{width:50%}.edd_download_columns_0 .edd_download,.edd_download_columns_3 .edd_download{width:33%}.edd_download_columns_4 .edd_download{width:25%}.edd_download_columns_5 .edd_download{width:20%}.edd_download_columns_6 .edd_download{width:16.6%}.edd_download_inner{padding:0 8px 8px;margin:0 0 10px}.edd_download_columns_2 .edd_download:nth-child(2n+1),.edd_download_columns_3 .edd_download:nth-child(3n+1),.edd_download_columns_4 .edd_download:nth-child(4n+1),.edd_download_columns_5 .edd_download:nth-child(5n+1),.edd_download_columns_6 .edd_download:nth-child(6n+1){clear:left}.edd_download_image{max-width:100%}.edd_download .edd_price{margin-bottom:10px}#edd_download_pagination{clear:both}.edd-hide-on-empty.cart-empty{display:none}edd-hide-on-empty.cart-not.empty{display:block}.edd-cart-ajax{margin:0 8px 0 4px;top:2px;background:0 0;border:none;padding:0}.edd-cart-number-of-items{font-style:italic;color:grey}.edd-cart-meta.edd_subtotal{font-weight:700;font-style:italic}.edd-cart-meta.edd_cart_tax{font-size:1em;font-style:italic}.edd-cart-meta.edd_cart_tax::before{font-style:normal}.edd-cart-meta.edd_total{font-weight:700}.edd-cart-meta{padding:2px 5px}.edd-cart-meta.edd_subtotal,.edd-cart-meta.edd_total{background-color:#f9f9f9}.edd_errors:not(.edd-alert){-webkit-border-radius:2px;-moz-border-radius:2px;border-radius:2px;border:1px solid #E6DB55;margin:0 0 21px;background:#FFFFE0;color:#333}.edd_error{padding:10px}p.edd_error{margin:0!important}.edd_success:not(.edd-alert){-webkit-border-radius:2px;-moz-border-radius:2px;border-radius:2px;border:1px solid #b3ce89;margin:20px 0;background:#d5eab3;color:#567622;padding:6px 8px;box-shadow:inset 0 1px 0 rgba(255,255,255,.7)}.edd-alert{-webkit-border-radius:2px;-moz-border-radius:2px;border-radius:2px;margin-bottom:20px;padding:10px;border:1px solid transparent;vertical-align:middle}.edd-alert p{padding:0}.edd-alert p:not(:last-child){margin-bottom:5px}.edd-alert p:last-child{margin-bottom:0}.edd-alert-error{color:#a94442;background-color:#f2dede;border-color:#ebccd1}.edd-alert-success{background-color:#dff0d8;border-color:#d6e9c6;color:#3c763d}.edd-alert-info{color:#31708f;background-color:#d9edf7;border-color:#bce8f1}.edd-alert-warn{color:#8a6d3b;background-color:#fcf8e3;border-color:#faebcc}</style><style type="text/css" media="all">@keyframes fade-in{0%{opacity:0}to{opacity:1}}@keyframes editor_region_focus{0%{box-shadow:inset 0 0 0 0 #33b3db}to{box-shadow:inset 0 0 0 4px #33b3db}}@keyframes rotation{0%{transform:rotate(0deg)}to{transform:rotate(1turn)}}.wp-block-audio figcaption{margin-top:.5em;color:#6c7781;text-align:center;font-size:13px}.editor-block-list__layout .reusable-block-edit-panel{align-items:center;background:#f8f9f9;color:#555d66;display:flex;flex-wrap:wrap;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:13px;position:relative;top:14px;margin:0 -28px;padding:10px 28px}.editor-block-list__layout .editor-block-list__layout .reusable-block-edit-panel{margin:0 -14px;padding:10px 14px}.editor-block-list__layout .reusable-block-edit-panel .reusable-block-edit-panel__spinner{margin:0 5px}.editor-block-list__layout .reusable-block-edit-panel .reusable-block-edit-panel__info{margin-right:auto}.editor-block-list__layout .reusable-block-edit-panel .reusable-block-edit-panel__label{margin-right:10px;white-space:nowrap;font-weight:600}.editor-block-list__layout .reusable-block-edit-panel .reusable-block-edit-panel__title{flex:1 1 100%;font-size:14px;height:30px;margin:5px 0 10px}.editor-block-list__layout .reusable-block-edit-panel .components-button.reusable-block-edit-panel__button{margin:0 5px 0 0}@media (min-width:960px){.editor-block-list__layout .reusable-block-edit-panel{flex-wrap:nowrap}.editor-block-list__layout .reusable-block-edit-panel .reusable-block-edit-panel__title{margin:0}.editor-block-list__layout .reusable-block-edit-panel .components-button.reusable-block-edit-panel__button{margin:0 0 0 5px}}.editor-block-list__layout .reusable-block-indicator{background:#fff;border-left:1px dashed #e2e4e7;color:#555d66;border-top:1px dashed #e2e4e7;bottom:-14px;height:30px;padding:5px;position:absolute;width:30px;right:-28px}.editor-block-list__layout .editor-block-list__layout .reusable-block-indicator{right:-14px}.wp-block-button{margin-bottom:1.5em}.wp-block-button .wp-block-button__link{border:none;border-radius:23px;box-shadow:none;cursor:pointer;display:inline-block;font-size:18px;line-height:24px;margin:0;padding:11px 24px;text-align:center;text-decoration:none;white-space:normal;word-break:break-all}.wp-block-button.is-style-squared .wp-block-button__link{border-radius:0}.wp-block-button.aligncenter{text-align:center}.wp-block-button.alignright{text-align:right}.wp-block-button__link:not(.has-background),.wp-block-button__link:not(.has-background):active,.wp-block-button__link:not(.has-background):focus,.wp-block-button__link:not(.has-background):hover{background-color:#32373c}.wp-block-button.is-style-outline .wp-block-button__link{background:transparent;border:2px solid transparent}.wp-block-button.is-style-outline .wp-block-button__link.has-pale-pink-background-color{border-color:#f78da7}.wp-block-button.is-style-outline .wp-block-button__link.has-vivid-red-background-color{border-color:#cf2e2e}.wp-block-button.is-style-outline .wp-block-button__link.has-luminous-vivid-orange-background-color{border-color:#ff6900}.wp-block-button.is-style-outline .wp-block-button__link.has-luminous-vivid-amber-background-color{border-color:#fcb900}.wp-block-button.is-style-outline .wp-block-button__link.has-light-green-cyan-background-color{border-color:#7bdcb5}.wp-block-button.is-style-outline .wp-block-button__link.has-vivid-green-cyan-background-color{border-color:#00d084}.wp-block-button.is-style-outline .wp-block-button__link.has-pale-cyan-blue-background-color{border-color:#8ed1fc}.wp-block-button.is-style-outline .wp-block-button__link.has-vivid-cyan-blue-background-color{border-color:#0693e3}.wp-block-button.is-style-outline .wp-block-button__link.has-very-light-gray-background-color{border-color:#eee}.wp-block-button.is-style-outline .wp-block-button__link.has-cyan-bluish-gray-background-color{border-color:#abb8c3}.wp-block-button.is-style-outline .wp-block-button__link.has-very-dark-gray-background-color{border-color:#313131}.wp-block-button.is-style-outline .wp-block-button__link:not(.has-background),.wp-block-button.is-style-outline .wp-block-button__link:not(.has-background):active,.wp-block-button.is-style-outline .wp-block-button__link:not(.has-background):focus,.wp-block-button.is-style-outline .wp-block-button__link:not(.has-background):hover{border-color:#32373c}.wp-block-button.is-style-outline .wp-block-button__link:not(.has-text-color),.wp-block-button.is-style-outline .wp-block-button__link:not(.has-text-color):active,.wp-block-button.is-style-outline .wp-block-button__link:not(.has-text-color):focus,.wp-block-button.is-style-outline .wp-block-button__link:not(.has-text-color):hover{color:#32373c}.wp-block-button__link:not(.has-text-color),.wp-block-button__link:not(.has-text-color):active,.wp-block-button__link:not(.has-text-color):focus,.wp-block-button__link:not(.has-text-color):hover{color:#fff}.wp-block-categories.alignleft{margin-right:2em}.wp-block-categories.alignright{margin-left:2em}.wp-block-columns{display:flex}.wp-block-column{flex:1}.wp-block-cover-image{position:relative;background-size:cover;background-position:50%;min-height:430px;width:100%;margin:0 0 1.5em;display:flex;justify-content:center;align-items:center}.wp-block-cover-image.has-left-content{justify-content:flex-start}.wp-block-cover-image.has-left-content .wp-block-cover-image-text,.wp-block-cover-image.has-left-content h2{margin-left:0;text-align:left}.wp-block-cover-image.has-right-content{justify-content:flex-end}.wp-block-cover-image.has-right-content .wp-block-cover-image-text,.wp-block-cover-image.has-right-content h2{margin-right:0;text-align:right}.wp-block-cover-image .wp-block-cover-image-text,.wp-block-cover-image h2{color:#fff;font-size:2em;line-height:1.25;z-index:1;margin-bottom:0;max-width:610px;padding:14px;text-align:center}.wp-block-cover-image .wp-block-cover-image-text a,.wp-block-cover-image .wp-block-cover-image-text a:active,.wp-block-cover-image .wp-block-cover-image-text a:focus,.wp-block-cover-image .wp-block-cover-image-text a:hover,.wp-block-cover-image h2 a,.wp-block-cover-image h2 a:active,.wp-block-cover-image h2 a:focus,.wp-block-cover-image h2 a:hover{color:#fff}.wp-block-cover-image.has-parallax{background-attachment:fixed}.wp-block-cover-image.has-background-dim:before{content:"";position:absolute;top:0;left:0;bottom:0;right:0;background-color:rgba(0,0,0,.5)}.wp-block-cover-image.has-background-dim.has-background-dim-10:before{background-color:rgba(0,0,0,.1)}.wp-block-cover-image.has-background-dim.has-background-dim-20:before{background-color:rgba(0,0,0,.2)}.wp-block-cover-image.has-background-dim.has-background-dim-30:before{background-color:rgba(0,0,0,.3)}.wp-block-cover-image.has-background-dim.has-background-dim-40:before{background-color:rgba(0,0,0,.4)}.wp-block-cover-image.has-background-dim.has-background-dim-50:before{background-color:rgba(0,0,0,.5)}.wp-block-cover-image.has-background-dim.has-background-dim-60:before{background-color:rgba(0,0,0,.6)}.wp-block-cover-image.has-background-dim.has-background-dim-70:before{background-color:rgba(0,0,0,.7)}.wp-block-cover-image.has-background-dim.has-background-dim-80:before{background-color:rgba(0,0,0,.8)}.wp-block-cover-image.has-background-dim.has-background-dim-90:before{background-color:rgba(0,0,0,.9)}.wp-block-cover-image.has-background-dim.has-background-dim-100:before{background-color:#000}.wp-block-cover-image.components-placeholder{height:inherit}.editor-block-list__block[data-type="core/embed"][data-align=left] .editor-block-list__block-edit,.editor-block-list__block[data-type="core/embed"][data-align=right] .editor-block-list__block-edit,.wp-block-cover-image.alignleft,.wp-block-cover-image.alignright,.wp-block-embed.alignleft,.wp-block-embed.alignright,[data-align=left] .wp-block-cover-image,[data-align=right] .wp-block-cover-image{max-width:305px;width:100%}.wp-block-embed figcaption{margin-top:.5em;color:#6c7781;text-align:center;font-size:13px}.wp-block-file{margin-bottom:1.5em}.wp-block-file.aligncenter{text-align:center}.wp-block-file.alignright{text-align:right}.wp-block-file__button{background:#32373c;border-radius:2em;color:#fff;font-size:13px;padding:.5em 1em}.wp-block-file a.wp-block-file__button{text-decoration:none}.wp-block-file a.wp-block-file__button:active,.wp-block-file a.wp-block-file__button:focus,.wp-block-file a.wp-block-file__button:hover,.wp-block-file a.wp-block-file__button:visited{box-shadow:none;color:#fff;opacity:.85;text-decoration:none}*+.wp-block-file__button{margin-left:.75em}.wp-block-gallery{display:flex;flex-wrap:wrap;list-style-type:none;padding:0;margin:0 -8px}.wp-block-gallery .blocks-gallery-image,.wp-block-gallery .blocks-gallery-item{margin:8px;display:flex;flex-grow:1;flex-direction:column;justify-content:center;position:relative}.wp-block-gallery .blocks-gallery-image figure,.wp-block-gallery .blocks-gallery-item figure{margin:0;height:100%;display:flex;align-items:flex-end}.wp-block-gallery .blocks-gallery-image img,.wp-block-gallery .blocks-gallery-item img{display:block;max-width:100%;height:auto}.wp-block-gallery .blocks-gallery-image figcaption,.wp-block-gallery .blocks-gallery-item figcaption{position:absolute;width:100%;max-height:100%;overflow:auto;padding:40px 10px 5px;color:#fff;text-align:center;font-size:13px;background:linear-gradient(0deg,rgba(0,0,0,.7),rgba(0,0,0,.3) 60%,transparent)}.wp-block-gallery .blocks-gallery-image figcaption img,.wp-block-gallery .blocks-gallery-item figcaption img{display:inline}.wp-block-gallery.is-cropped .blocks-gallery-image a,.wp-block-gallery.is-cropped .blocks-gallery-image img,.wp-block-gallery.is-cropped .blocks-gallery-item a,.wp-block-gallery.is-cropped .blocks-gallery-item img{flex:1;width:100%;height:100%;-o-object-fit:cover;object-fit:cover}.wp-block-gallery .blocks-gallery-image,.wp-block-gallery .blocks-gallery-item{width:calc(100% / 2 - 16px)}.wp-block-gallery.columns-1 .blocks-gallery-image,.wp-block-gallery.columns-1 .blocks-gallery-item{width:calc(100% / 1 - 16px)}@media (min-width:600px){.wp-block-gallery.columns-3 .blocks-gallery-image,.wp-block-gallery.columns-3 .blocks-gallery-item{width:calc(100% / 3 - 16px)}.wp-block-gallery.columns-4 .blocks-gallery-image,.wp-block-gallery.columns-4 .blocks-gallery-item{width:calc(100% / 4 - 16px)}.wp-block-gallery.columns-5 .blocks-gallery-image,.wp-block-gallery.columns-5 .blocks-gallery-item{width:calc(100% / 5 - 16px)}.wp-block-gallery.columns-6 .blocks-gallery-image,.wp-block-gallery.columns-6 .blocks-gallery-item{width:calc(100% / 6 - 16px)}.wp-block-gallery.columns-7 .blocks-gallery-image,.wp-block-gallery.columns-7 .blocks-gallery-item{width:calc(100% / 7 - 16px)}.wp-block-gallery.columns-8 .blocks-gallery-image,.wp-block-gallery.columns-8 .blocks-gallery-item{width:calc(100% / 8 - 16px)}}.wp-block-gallery .blocks-gallery-item.has-add-item-button{width:100%}.wp-block-gallery.alignleft,.wp-block-gallery.alignright,[data-align=left] .wp-block-gallery,[data-align=right] .wp-block-gallery{max-width:305px;width:100%}.wp-block-image{width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;max-width:100%}.wp-block-image img{max-width:100%}.wp-block-image.aligncenter{display:block;margin-left:auto;margin-right:auto;text-align:center}.wp-block-image.is-resized{width:-webkit-min-content;width:-moz-min-content;width:min-content;display:-ms-inline-grid;-ms-grid-columns:min-content}.wp-block-image.is-resized figcaption{-ms-grid-row:2}.wp-block-image.is-resized img{max-width:none}.wp-block-image figcaption{margin-top:.5em;color:#6c7781;text-align:center;font-size:13px}.wp-block-latest-comments__comment{font-size:15px;line-height:1.1;list-style:none;margin-bottom:1em}.has-avatars .wp-block-latest-comments__comment{min-height:36px;list-style:none}.has-avatars .wp-block-latest-comments__comment .wp-block-latest-comments__comment-excerpt,.has-avatars .wp-block-latest-comments__comment .wp-block-latest-comments__comment-meta{margin-left:52px}.has-dates .wp-block-latest-comments__comment,.has-excerpts .wp-block-latest-comments__comment{line-height:1.5}.wp-block-latest-comments__comment-excerpt p{font-size:14px;line-height:1.8;margin:5px 0 20px}.wp-block-latest-comments__comment-date{color:#8f98a1;display:block;font-size:12px}.wp-block-latest-comments .avatar,.wp-block-latest-comments__comment-avatar{border-radius:24px;display:block;float:left;height:40px;margin-right:12px;width:40px}.wp-block-latest-posts.alignleft{margin-right:2em}.wp-block-latest-posts.alignright{margin-left:2em}.wp-block-latest-posts.is-grid{display:flex;flex-wrap:wrap;padding:0;list-style:none}.wp-block-latest-posts.is-grid li{margin:0 16px 16px 0;width:100%}@media (min-width:600px){.wp-block-latest-posts.columns-2 li{width:calc(50% - 16px)}.wp-block-latest-posts.columns-3 li{width:calc(33.33333% - 16px)}.wp-block-latest-posts.columns-4 li{width:calc(25% - 16px)}.wp-block-latest-posts.columns-5 li{width:calc(20% - 16px)}.wp-block-latest-posts.columns-6 li{width:calc(16.66667% - 16px)}}.wp-block-latest-posts__post-date{display:block;color:#6c7781;font-size:13px}p.is-small-text{font-size:14px}p.is-regular-text{font-size:16px}p.is-large-text{font-size:36px}p.is-larger-text{font-size:48px}p.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;line-height:.68;font-weight:100;margin:.05em .1em 0 0;text-transform:uppercase;font-style:normal}p.has-background{padding:20px 30px}.wp-block-pullquote{padding:3em 0;text-align:center}.wp-block-pullquote.alignleft,.wp-block-pullquote.alignright{max-width:400px}.wp-block-pullquote.alignleft>p,.wp-block-pullquote.alignright>p{font-size:20px}.wp-block-pullquote>p{font-size:24px;line-height:1.6}.wp-block-pullquote cite,.wp-block-pullquote footer{position:relative}.wp-block-quote.is-large,.wp-block-quote.is-style-large{margin:0 0 16px;padding:0 1em}.wp-block-quote.is-large p,.wp-block-quote.is-style-large p{font-size:24px;font-style:italic;line-height:1.6}.wp-block-quote.is-large cite,.wp-block-quote.is-large footer,.wp-block-quote.is-style-large cite,.wp-block-quote.is-style-large footer{font-size:18px;text-align:right}.wp-block-separator.is-style-wide{border-bottom-width:1px}.wp-block-separator.is-style-dots{border:none;text-align:center;max-width:none;line-height:1;height:auto}.wp-block-separator.is-style-dots:before{content:"\00b7 \00b7 \00b7";color:#32373c;font-size:20px;letter-spacing:2em;padding-left:2em;font-family:serif}p.wp-block-subhead{font-size:1.1em;font-style:italic;opacity:.75}.wp-block-table.has-fixed-layout tbody{table-layout:fixed}.wp-block-text-columns,.wp-block-text-columns.aligncenter{display:flex}.wp-block-text-columns .wp-block-column{margin:0 16px;padding:0}.wp-block-text-columns .wp-block-column:first-child{margin-left:0}.wp-block-text-columns .wp-block-column:last-child{margin-right:0}.wp-block-text-columns.columns-2 .wp-block-column{width:50%}.wp-block-text-columns.columns-3 .wp-block-column{width:33.33333%}.wp-block-text-columns.columns-4 .wp-block-column{width:25%}.wp-block-video video{max-width:100%}.wp-block-video.aligncenter{text-align:center}.has-pale-pink-background-color{background-color:#f78da7}.has-vivid-red-background-color{background-color:#cf2e2e}.has-luminous-vivid-orange-background-color{background-color:#ff6900}.has-luminous-vivid-amber-background-color{background-color:#fcb900}.has-light-green-cyan-background-color{background-color:#7bdcb5}.has-vivid-green-cyan-background-color{background-color:#00d084}.has-pale-cyan-blue-background-color{background-color:#8ed1fc}.has-vivid-cyan-blue-background-color{background-color:#0693e3}.has-very-light-gray-background-color{background-color:#eee}.has-cyan-bluish-gray-background-color{background-color:#abb8c3}.has-very-dark-gray-background-color{background-color:#313131}.has-pale-pink-color{color:#f78da7}.has-vivid-red-color{color:#cf2e2e}.has-luminous-vivid-orange-color{color:#ff6900}.has-luminous-vivid-amber-color{color:#fcb900}.has-light-green-cyan-color{color:#7bdcb5}.has-vivid-green-cyan-color{color:#00d084}.has-pale-cyan-blue-color{color:#8ed1fc}.has-vivid-cyan-blue-color{color:#0693e3}.has-very-light-gray-color{color:#eee}.has-cyan-bluish-gray-color{color:#abb8c3}.has-very-dark-gray-color{color:#313131}.has-small-font-size{font-size:14px}.has-regular-font-size{font-size:16px}.has-large-font-size{font-size:36px}.has-larger-font-size{font-size:48px}</style><style type="text/css" media="all">pre.prettyprint{padding:10px 15px !important;border:none !important}.prettyprint,pre.prettyprint{background-color:#272822 !important;border:1px solid #272822 !important;overflow:hidden !important !important;padding:8px !important}.prettyprint.linenums,pre.prettyprint.linenums{-webkit-box-shadow:inset 40px 0 0 #39382E,inset 41px 0 0 #464741;-moz-box-shadow:inset 40px 0 0 #39382E,inset 41px 0 0 #464741;box-shadow:inset 40px 0 0 #39382E,inset 41px 0 0 #464741}.prettyprint.linenums ol,pre.prettyprint.linenums ol{margin:0 0 0 33px}.prettyprint.linenums ol li,pre.prettyprint.linenums ol li{padding-left:12px;color:#bebec5;line-height:20px;margin-left:0;list-style:decimal}.prettyprint .com{color:#93a1a1}.prettyprint .lit{color:#AE81FF}.prettyprint .pun,.prettyprint .opn,.prettyprint .clo{color:#F8F8F2}.prettyprint .fun{color:#dc322f}.prettyprint .str,.prettyprint .atv{color:#E6DB74}.prettyprint .kwd,.prettyprint .tag{color:#F92659}.prettyprint .typ,.prettyprint .atn,.prettyprint .dec,.prettyprint .var{color:#A6E22E}.prettyprint .pln{color:#66D9EF}</style><style type="text/css" media="all"></style><style type="text/css" media="all"> html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}table{border-collapse:collapse;border-spacing:0}html{font-size:100%;-ms-text-size-adjust:none;-webkit-text-size-adjust:none}body{font-family:'Open Sans',Helvetica,Arial,sans-serif;font-size:14px;font-size:0.875rem;line-height:1.6;background:#f7f7f7;word-wrap:break-word}.mh-container,.mh-container-inner{width:100%;max-width:1080px;margin:0 auto;position:relative}.mh-container-outer{margin:25px auto;-webkit-box-shadow:0px 0px 10px rgba(50,50,50,0.17);-moz-box-shadow:0px 0px 10px rgba(50,50,50,0.17);box-shadow:0px 0px 10px rgba(50,50,50,0.17)}.mh-wrapper{padding:25px;background:#fff}.mh-content{width:65.83%;overflow:hidden}.mh-sidebar{width:31.66%;float:left}.mh-margin-left{margin-left:2.5%}.mh-margin-right{margin-right:2.5%}.mh-right-sb #main-content{float:left;margin-right:2.5%}.mh-left-sb #main-content{float:right;margin-left:2.5%}.mh-home-area-3,.mh-home-area-4{width:48.24%}.mh-home-area-4{margin-left:3.52%}.page-template-homepage .mh-wrapper{padding-bottom:0}.mh-main-nav li,.entry-tags li,.tagcloud a{-webkit-transition:0.25s ease-out;-moz-transition:0.25s ease-out;transition:0.25s ease-out}.mh-row [class*='mh-col-']:first-child{margin:0}[class*='mh-col-']{float:left;margin-left:2.5%;overflow:hidden}.mh-col-1-1{width:100%}.mh-col-1-2{width:48.75%}.mh-col-1-3{width:31.66%}.mh-col-2-3{width:65.83%}.mh-col-1-4{width:23.12%}.mh-col-3-4{width:74.37%}.mh-col-1-5{width:18.00%}.mh-col-1-6{width:14.58%}.mh-col-1-7{width:12.14%}.mh-col-1-8{width:10.31%}.clear{clear:both}.clearfix{display:block}.clearfix:after{content:".";display:block;clear:both;visibility:hidden;line-height:0;height:0;margin:0;padding:0}h1{font-size:32px;font-size:2rem}h2{font-size:24px;font-size:1.5rem}h3{font-size:20px;font-size:1.25rem}h4{font-size:18px;font-size:1.125rem}h5{font-size:16px;font-size:1rem}h6{font-size:14px;font-size:0.875rem}h1,h2,h3,h4,h5,h6{font-family:'Open Sans',Helvetica,Arial,sans-serif;color:#000;line-height:1.3;font-weight:700}b,strong{font-weight:bold}i,em{font-style:italic}small{font-size:6px;font-size:0.375rem}big{font-size:20px;font-size:1.25rem}pre,code{font-family:"Consolas",Courier New,Courier,monospace;color:#000;margin-bottom:20px;margin-bottom:1.25rem;background:#f5f5f5;border:1px solid #ebebeb}pre{padding:20px;padding:1.25rem;white-space:pre-wrap;white-space:-o-pre-wrap;white-space:-moz-pre-wrap;white-space:-webkit-pre-wrap}kbd{padding:0px 6px;padding:0rem 0.375rem;background:#f5f5f5;border:1px solid #aaa;border-radius:0.1875em;-moz-border-radius:0.1875em;-moz-box-shadow:0 0.0625em 0 rgba(0,0,0,0.2),0 0 0 0.125em white inset;-webkit-border-radius:0.1875em;-webkit-box-shadow:0 0.0625em 0 rgba(0,0,0,0.2),0 0 0 0.125em white inset;box-shadow:0 0.0625em 0 rgba(0,0,0,0.2),0 0 0 0.125em white inset;text-shadow:0 0.0625em 0 #fff}blockquote{display:block;font-size:15px;font-size:0.9375rem;line-height:1.6;font-style:italic;color:#666;padding:5px 0 5px 15px;border-left:5px solid #e64946}sup{vertical-align:super;font-size:10px;font-size:0.625rem}sub{vertical-align:sub;font-size:10px;font-size:0.625rem}abbr,acronym{border-bottom:1px dashed;cursor:help}cite{color:#9a9b97}q{font-style:italic}address{font-family:"Courier new";line-height:1.5;margin-bottom:20px;margin-bottom:1.25rem}a{color:#000;text-decoration:none}a:hover{color:#e64946}.screen-reader-text{position:absolute;top:-9999rem;left:-9999rem}.entry-content ul{list-style:square}.entry-content ol{list-style:decimal}.entry-content ul,.entry-content ol{margin:0 0 20px 40px}.entry-content ul ul,.entry-content ol ol{margin:0 0 0 40px}.entry-content li{margin-bottom:5px}dl{margin:0 0 10px 20px}dt,dd{display:list-item}dt{list-style-type:square;font-weight:bold}dd{list-style-type:circle;margin-left:20px}select{max-width:100%}.mh-header{background:#fff}.mh-site-logo{padding:20px;overflow:hidden}.mh-header-text{margin:5px 0;text-transform:uppercase}.mh-header-title{font-size:32px;font-size:2rem;line-height:1}.mh-header-tagline{display:inline-block;font-size:14px;font-size:0.875rem;line-height:1;color:#e64946;padding-top:10px;margin-top:10px;border-top:1px solid #e5e5e5}.mh-main-nav-wrap{background:#2a2a2a}.mh-main-nav{text-transform:uppercase;border-bottom:5px solid #e64946}.mh-main-nav li{float:left;position:relative;font-weight:700}.mh-main-nav li:hover{background:#e64946}.mh-main-nav li:hover a{color:#fff}.mh-main-nav li a{display:block;color:#fff;padding:10px 20px;border-left:1px solid rgba(255,255,255,0.1)}.mh-main-nav li:first-child a,.mh-main-nav .sub-menu li a{border:none}.mh-main-nav ul li:hover > ul{display:block;background:#2a2a2a;z-index:9999}.mh-main-nav ul ul{display:none;position:absolute}.mh-main-nav ul ul li{width:16em}.mh-main-nav ul ul ul{left:100%;top:0}.mh-main-nav .menu > .current-menu-item{background:#e64946}.mh-navigation .menu-item-has-children > a:after{font-family:'FontAwesome';font-weight:400;line-height:1;content:'\f107';margin-left:5px}.mh-navigation .sub-menu .menu-item-has-children > a:after{content:'\f105';margin-left:10px}.mh-navigation img{vertical-align:baseline}.slicknav_btn{position:relative;display:block;vertical-align:middle;float:right;padding:0.438em 0.625em;line-height:1.125em;cursor:pointer;margin:5px 5px 6px;text-decoration:none;text-shadow:0 1px 1px rgba(255,255,255,0.75);-webkit-border-radius:4px;-moz-border-radius:4px;border-radius:4px;background-color:#222}.slicknav_btn .slicknav_icon-bar + .slicknav_icon-bar{margin-top:0.188em}.slicknav_menu{*zoom:1;font-size:16px;font-size:1rem;box-sizing:border-box;background:#4c4c4c;padding:5px}.slicknav_menu *{box-sizing:border-box}.slicknav_menu .slicknav_menutxt{display:block;line-height:1.188em;float:left;color:#fff;font-weight:bold;text-shadow:0 1px 3px #000}.slicknav_menu .slicknav_icon{float:left;margin:0.188em 0 0 0.438em}.slicknav_menu .slicknav_no-text{margin:0}.slicknav_menu .slicknav_icon-bar{display:block;width:1.125em;height:0.125em;background-color:#fff;-webkit-border-radius:1px;-moz-border-radius:1px;border-radius:1px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,0.25);-moz-box-shadow:0 1px 0 rgba(0,0,0,0.25);box-shadow:0 1px 0 rgba(0,0,0,0.25)}.slicknav_menu:before,.slicknav_menu:after{content:" ";display:table}.slicknav_menu:after{clear:both}.slicknav_nav{clear:both;color:#fff;margin:0;padding:0;font-size:0.875em;list-style:none;overflow:hidden}.slicknav_nav ul{display:block;list-style:none;overflow:hidden;padding:0;margin:0 0 0 20px}.slicknav_nav li{display:block}.slicknav_nav a{display:block;padding:5px 10px;margin:2px 5px;text-decoration:none;color:#fff}.slicknav_nav a:hover{-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px;background:#ccc;color:#222}.slicknav_nav .slicknav_arrow{font-size:0.8em;margin:0 0 0 0.4em}.slicknav_nav .slicknav_item{cursor:pointer}.slicknav_nav .slicknav_item a{display:inline;padding:0;margin:0}.slicknav_nav .slicknav_row{display:block;padding:5px 10px;margin:2px 5px}.slicknav_nav .slicknav_row:hover{-webkit-border-radius:6px;-moz-border-radius:6px;border-radius:6px;background:#ccc;color:#fff}.slicknav_nav .slicknav_txtnode{margin-left:15px}.slicknav_nav .slicknav_parent-link a{display:inline;padding:0;margin:0}.slicknav_brand{float:left;color:#fff;font-size:18px;line-height:30px;padding:7px 12px;height:44px}.slicknav_btn{background:#e64946}.slicknav_menu{padding:0;border-bottom:1px solid #2a2a2a;background:#e64946;display:none}.slicknav_menu .slicknav_menutxt{text-shadow:none;display:none}.slicknav_menu .slicknav_icon-bar{box-shadow:none;-moz-box-shadow:none;-webkit-box-shadow:none}.slicknav_nav{font-size:16px;font-size:1rem;font-weight:600;margin-bottom:20px}.slicknav_nav ul{padding:20px 0 0 15px;margin:0;border-top:1px solid #2a2a2a}.slicknav_nav ul ul{border:0;padding:0 0 0 15px}.slicknav_nav .slicknav_item:hover,.slicknav_nav a:hover{color:#fff;background:#e64946}.mh-main-nav-wrap .slicknav_btn,.mh-main-nav-wrap .slicknav_menu,.mh-main-nav-wrap .slicknav_nav .slicknav_item:hover,.mh-main-nav-wrap .slicknav_nav a:hover{background:transparent}.mh-main-nav-wrap .slicknav_nav ul{border-top:1px solid #e64946}.mh-main-nav-wrap .slicknav_nav ul ul{border:none}.flex-container a:active,.flexslider a:active,.flex-container a:focus,.flexslider a:focus{outline:none}.slides,.flex-control-nav,.flex-direction-nav{margin:0;padding:0;list-style:none}.flexslider{margin:0;padding:0}.flexslider .slides > li{display:none;-webkit-backface-visibility:hidden}.flexslider .slides img{display:block}.flex-pauseplay span{text-transform:capitalize}.slides:after{content:".";display:block;clear:both;visibility:hidden;line-height:0;height:0}html[xmlns] .slides{display:block}* html .slides{height:1%}.no-js .slides > li:first-child{display:block}.flexslider{position:relative;zoom:1}.flex-viewport{max-height:2000px;-webkit-transition:all 1s ease;-moz-transition:all 1s ease;transition:all 1s ease}.loading .flex-viewport{max-height:300px}.flexslider .slides{zoom:1}.flexslider .slides img{height:auto}.flex-direction-nav{*height:0}.flex-direction-nav a{width:30px;height:30px;margin:-12px 0 0;display:block;background:url(http://adventuresinmachinelearning.com/wp-content/themes/mh-magazine-lite/images/bg_direction_nav.png) no-repeat 0 0;position:absolute;top:50%;z-index:10;cursor:pointer;text-indent:-9999px;opacity:0;-webkit-transition:all .3s ease}.flex-direction-nav .flex-next{background-position:100% 0;right:-36px;display:none}.flex-direction-nav .flex-prev{left:-36px;display:none}.flex-direction-nav .flex-disabled{opacity:0.3 !important;filter:alpha(opacity=30);cursor:default}.flexslider:hover .flex-next{opacity:0.8;right:5px;display:block}.flexslider:hover .flex-prev{opacity:0.8;left:5px;display:block}.flexslider:hover .flex-next:hover,.flexslider:hover .flex-prev:hover{opacity:1}.flex-control-nav{width:100%;position:absolute;bottom:6px;text-align:center}.flex-control-nav li{margin:0 6px;display:inline-block;zoom:1;*display:inline}.flex-control-paging li a{width:11px;height:11px;display:block;background:#666;background:rgba(0,0,0,0.5);cursor:pointer;text-indent:-9999px;-webkit-border-radius:20px;-moz-border-radius:20px;-o-border-radius:20px;border-radius:20px;box-shadow:inset 0 0 3px rgba(0,0,0,0.3)}.flex-control-paging li a:hover{background:#333;background:rgba(0,0,0,0.7)}.flex-control-paging li a.flex-active{background:#000;background:rgba(0,0,0,0.9);cursor:default}.entry-header,.page-header{margin-bottom:20px;margin-bottom:1.25rem}.page-title{font-size:28px;font-size:1.75rem}.mh-meta{font-size:13px;font-size:0.8125rem}.mh-meta,.mh-meta a{color:#979797}.mh-meta a:hover{color:#e64946}.mh-footer .mh-meta,.mh-footer .mh-meta a,.mh-footer .mh-meta a:hover{color:#fff}.mh-meta span{margin-right:10px}.mh-meta .fa{margin-right:5px}.entry-meta{margin-top:10px;margin-top:0.625rem;padding:5px 10px;border-top:1px dotted #ebebeb;border-bottom:1px dotted #ebebeb}.entry-thumbnail{max-width:1030px;margin-bottom:20px;margin-bottom:1.25rem}.entry-thumbnail img{width:100%}.entry-content h1,.entry-content h2,.entry-content h3,.entry-content h4,.entry-content h5,.entry-content h6,.entry-content p,.entry-content blockquote,.entry-content .flex-vid,.entry-content .mh-row,.entry-content .mh-video-container{margin-bottom:20px;margin-bottom:1.25rem}.entry-content blockquote p:last-child{margin:0}.entry-content a{font-weight:600;color:#e64946}.entry-content a:hover{color:#e64946;text-decoration:underline}.entry-tags{font-size:12px;font-size:0.75rem;color:#fff;line-height:1;margin-bottom:20px;margin-bottom:1.25rem}.entry-tags li{float:left;font-weight:700;margin:0 6px 6px 0;margin:0 0.375rem 0.375rem 0;background:#2a2a2a;text-transform:uppercase}.entry-tags li:hover{background:#e64946}.entry-tags a,.entry-tags a:hover{display:block;color:#fff;padding:10px 15px}.entry-tags .fa{float:left;padding:10px;margin-right:6px;background:#e64946}.sticky{}.mh-author-box{line-height:1.5;padding:25px 0;margin-bottom:25px;border-top:1px solid #ebebeb;border-bottom:1px solid #ebebeb}.mh-author-box-avatar{float:left;padding:3px;margin-right:25px;border:1px solid #ebebeb}.mh-author-box-header{margin-bottom:10px}.mh-author-box-name{font-size:16px;font-size:1rem;font-weight:700}.mh-author-box-postcount:before{font-family:'FontAwesome';content:'\f105';padding:0 5px}.author .mh-author-box{margin-top:25px}.mh-loop-description{padding:20px 0;margin-top:20px;border-top:1px solid #ebebeb;border-bottom:1px solid #ebebeb}.mh-loop-description p:last-child{margin:0}.mh-loop-item{padding-bottom:20px;padding-bottom:1.25rem;margin-bottom:20px;margin-bottom:1.25rem;border-bottom:1px solid #ebebeb}.mh-loop-header{margin-bottom:10px;margin-bottom:0.625rem}.mh-loop-meta{margin-top:5px}.mh-loop-excerpt{overflow:hidden}.mh-loop-thumb{float:left;margin-right:20px}.mh-loop-thumb img{width:100%;max-width:235px}.mh-loop-pagination{margin-top:20px}.post .pagination{padding-bottom:20px;padding-bottom:1.25rem}.page-numbers{display:inline-block;font-weight:700;color:#000;padding:10px 15px;background:#f5f5f5}.page-numbers:hover,.mh-loop-pagination .current,.mh-comments-pagination .current,a:hover .pagelink{color:#fff;background:#e64946}.page-numbers a{display:block}.pagelink{display:inline-block;font-weight:700;color:#fff;padding:10px 15px;background:#e64946}a .pagelink{color:#000;background:#f5f5f5}.mh-post-nav{padding-bottom:25px;margin-bottom:25px;border-bottom:1px solid #ebebeb}.mh-post-nav span{display:block;font-weight:700;line-height:1;margin-bottom:5px;text-transform:uppercase;overflow:hidden}.mh-post-nav p{font-size:13px;font-size:0.8125rem;overflow:hidden}.mh-post-nav-next{float:right;text-align:right}.mh-post-nav-prev img{float:left;margin-right:15px}.mh-post-nav-next img{float:right;margin-left:15px}.mh-post-nav-prev span:before,.mh-post-nav-next span:after{font-family:'FontAwesome';font-weight:400}.mh-post-nav-prev span:before{content:'\f100';margin-right:5px}.mh-post-nav-next span:after{content:'\f101';margin-left:5px}.mh-footer{font-size:13px;font-size:0.8125rem;color:#fff;padding:25px 25px 0;background:#2a2a2a}.mh-footer a,.mh-footer a:hover{color:#f7f7f7}.mh-copyright-wrap{padding:10px 25px;border-top:3px solid #999;background:#2a2a2a}.mh-copyright{font-size:12px;font-size:0.75rem;color:#999}.mh-copyright a{color:#999}img{max-width:100%;width:auto\9;height:auto;vertical-align:bottom}iframe,embed,object,video{max-width:100%}.entry-content .alignnone{display:block;margin:20px 0;margin:1.25rem 0}.entry-content .aligncenter{display:block;margin:20px auto;margin:1.25rem auto}.entry-content .alignleft{display:inline;float:left;margin:5px 20px 20px 0;margin:0.3125rem 1.25rem 1.25rem 0}.entry-content .alignright{display:inline;float:right;margin:5px 0 20px 20px;margin:0.3125rem 0 1.25rem 1.25rem}.wp-caption{max-width:100%}.wp-caption-text{display:block;font-size:12px;font-size:0.75rem;font-weight:700;line-height:1.4;color:#000;margin-top:5px}.alignnone .wp-caption-text,.aligncenter .wp-caption-text,.alignleft .wp-caption-text,.alignright .wp-caption-text{margin-bottom:0}.gallery{margin-bottom:20px}.gallery:after{content:".";display:block;clear:both;visibility:hidden;line-height:0;height:0;margin:0;padding:0}.gallery-item{float:left;margin:0 4px 4px 0;overflow:hidden;position:relative}.gallery-columns-1 .gallery-item{max-width:100%}.gallery-columns-2 .gallery-item{max-width:48%;max-width:-webkit-calc(50% - 4px);max-width:calc(50% - 4px)}.gallery-columns-3 .gallery-item{max-width:32%;max-width:-webkit-calc(33.3% - 4px);max-width:calc(33.3% - 4px)}.gallery-columns-4 .gallery-item{max-width:23%;max-width:-webkit-calc(25% - 4px);max-width:calc(25% - 4px)}.gallery-columns-5 .gallery-item{max-width:19%;max-width:-webkit-calc(20% - 4px);max-width:calc(20% - 4px)}.gallery-columns-6 .gallery-item{max-width:15%;max-width:-webkit-calc(16.7% - 4px);max-width:calc(16.7% - 4px)}.gallery-columns-7 .gallery-item{max-width:13%;max-width:-webkit-calc(14.28% - 4px);max-width:calc(14.28% - 4px)}.gallery-columns-8 .gallery-item{max-width:11%;max-width:-webkit-calc(12.5% - 4px);max-width:calc(12.5% - 4px)}.gallery-columns-9 .gallery-item{max-width:9%;max-width:-webkit-calc(11.1% - 4px);max-width:calc(11.1% - 4px)}.gallery-columns-1 .gallery-item:nth-of-type(1n),.gallery-columns-2 .gallery-item:nth-of-type(2n),.gallery-columns-3 .gallery-item:nth-of-type(3n),.gallery-columns-4 .gallery-item:nth-of-type(4n),.gallery-columns-5 .gallery-item:nth-of-type(5n),.gallery-columns-6 .gallery-item:nth-of-type(6n),.gallery-columns-7 .gallery-item:nth-of-type(7n),.gallery-columns-8 .gallery-item:nth-of-type(8n),.gallery-columns-9 .gallery-item:nth-of-type(9n){margin-right:0}.gallery-columns-1.gallery-size-medium figure.gallery-item:nth-of-type(1n+1),.gallery-columns-1.gallery-size-thumbnail figure.gallery-item:nth-of-type(1n+1),.gallery-columns-2.gallery-size-thumbnail figure.gallery-item:nth-of-type(2n+1),.gallery-columns-3.gallery-size-thumbnail figure.gallery-item:nth-of-type(3n+1){clear:left}.gallery-caption{background-color:rgba(0,0,0,0.7);-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;color:#fff;font-size:12px;line-height:1.5;margin:0;max-height:50%;opacity:0;padding:6px 8px;position:absolute;bottom:0;left:0;text-align:left;width:100%}.gallery-caption:before{content:"";height:100%;min-height:49px;position:absolute;top:0;left:0;width:100%}.gallery-item:hover .gallery-caption{opacity:1}.gallery-columns-7 .gallery-caption,.gallery-columns-8 .gallery-caption,.gallery-columns-9 .gallery-caption{display:none}table{width:100%;margin-bottom:20px;margin-bottom:1.25rem;border-left:1px solid #ebebeb;border-top:1px solid #ebebeb}td,th{padding:5px 10px;border-right:1px solid #ebebeb;border-bottom:1px solid #ebebeb}th{font-weight:600}.mh-footer-widget table,.mh-footer-widget td,.mh-footer-widget th{border-color:rgba(255,255,255,0.3)}.mh-comment-list{margin-bottom:25px;list-style:none}.mh-comment-list .children{margin-left:25px;list-style:none}.mh-comment-body{margin-bottom:25px;border-bottom:1px solid #ebebeb}.mh-comment-footer{margin-bottom:20px}.mh-comment-gravatar{float:left}.mh-comment-gravatar .avatar{width:50px;height:50px;padding:2px;margin:0 15px 0 0;border:1px solid #f5f5f5;vertical-align:middle}.mh-comment-meta{overflow:hidden}.mh-comment-meta,.mh-comment-meta a{color:#2a2a2a}.mh-comment-meta-author{font-size:16px;font-size:1rem;font-weight:700}.mh-comment-meta-author .fn{margin-right:5px}.bypostauthor .mh-comment-meta-author:after{font-family:'FontAwesome';font-weight:400;color:#e64946;content:'\f006'}.mh-comment-meta-date{font-size:10px;font-size:0.625rem;text-transform:uppercase}.mh-comment-meta-links{font-weight:700;line-height:1;text-transform:uppercase;margin-bottom:25px}.mh-comment-meta-links .comment-edit-link{margin-right:15px}.mh-comment-meta-links .comment-reply-link:before{font-family:'FontAwesome';font-weight:400;color:#e64946;content:'\f112';margin-right:5px}.mh-comment-info{color:#e64946;margin:20px 0}.mh-comments-pagination{margin-bottom:20px}.mh-ping-list{margin-bottom:20px}.mh-ping-list .mh-ping-item{padding:10px 0;border-bottom:1px solid #ebebeb}.mh-ping-list .mh-ping-item:first-child{border-top:1px solid #ebebeb}.mh-ping-list .mh-ping-item .fa{margin-right:15px}#respond .comment-reply-title{font-size:24px;font-size:1.5rem;line-height:1;margin-bottom:20px;margin-bottom:1.25rem}#respond #cancel-comment-reply-link{font-size:12px;font-size:0.75rem;margin-left:10px}#respond #cancel-comment-reply-link:before{font-family:'FontAwesome';font-weight:400;color:#e64946;content:'\f05e';margin-right:5px}#commentform p{margin-bottom:10px}#commentform .form-submit{margin:25px 0}.required{color:#e64946}input{font-size:12px;padding:5px;border:1px solid rgba(0,0,0,0.1);vertical-align:middle;background:#f5f5f5;-webkit-transition:all 0.25s ease-in-out;-moz-transition:all 0.25s ease-in-out;transition:all 0.25s ease-in-out}input[type=text],input[type=email],input[type=tel],input[type=url]{width:60%}input[type=text]:hover,input[type=email]:hover,input[type=tel]:hover,input[type=url]:hover,textarea:hover{border:1px solid #e64946}input[type=submit]{display:inline-block;min-width:150px;font-weight:700;color:#fff;padding:10px 15px;background:#e64946;cursor:pointer;text-transform:uppercase;-webkit-transition:all 0.1s linear;-moz-transition:all 0.1s linear;transition:all 0.1s linear;border:0;-webkit-appearance:none}input[type=submit]:hover{background:#2a2a2a}textarea{width:96%;line-height:1.5;padding:5px;border:1px solid rgba(0,0,0,0.1);background:#f5f5f5}.wpcf7-form{font-size:14px;font-size:0.875rem;margin-bottom:20px}.wpcf7-form p{margin-bottom:15px}.wpcf7-text{padding:8px}.wpcf7-textarea{width:86%;padding:10px}div.wpcf7-response-output{margin:20px 0 15px}.search-form input{font-size:11px;line-height:1;color:#1f1e1e;text-transform:uppercase}.search-form .search-submit{display:none}.search-form .search-field{position:relative;padding:10px;margin:0;border:1px solid #ebebeb;background:#fff;cursor:pointer;-webkit-appearance:none;-webkit-border-radius:0;border-radius:0}.search-form .search-field:active,.search-form .search-field:focus{cursor:text}.mh-widget .search-form{display:block;margin:0 auto;padding:5%;background:#f5f5f5}.mh-widget .search-form .search-field{display:block;margin:0 auto;width:90%}.mh-footer .search-form input{width:100%}.ad-label{font-size:10px;font-size:0.625rem}.mh-box{position:relative;overflow:hidden;padding:20px 20px 0;padding:1.25rem 1.25rem 0;margin-bottom:20px;margin-bottom:1.25rem;border:1px solid #ebebeb}.mh-widget,.mh-footer-widget{margin-bottom:25px;overflow:hidden}.mh-widget-title{position:relative;font-size:16px;font-size:1rem;padding-bottom:5px;margin-bottom:20px;margin-bottom:1.25rem;text-transform:uppercase;border-bottom:3px solid #e64946}.mh-footer-widget-title{font-size:14px;font-size:0.875rem;color:#fff}.mh-footer-widget-title a{color:#fff}.mh-ad-spot{display:block;padding:10px;text-align:center;background:#f5f5f5}.mh-footer-widget .mh-ad-spot{background:rgba(255,255,255,0.1)}.mh-slider-widget .flex-control-nav{bottom:10px}.mh-slider-widget .flex-control-nav li{margin:0 10px 0 0}.mh-slider-widget .flex-control-paging li a{width:15px;height:15px;border:1px solid #2a2a2a;background:#fff;-webkit-box-shadow:none;-moz-box-shadow:none;-o-box-shadow:none;box-shadow:none}.mh-slider-widget .flex-control-paging li a:hover{background:#f5f5f5}.mh-slider-widget .flex-control-paging li a.flex-active{background:#e64946;border:1px solid #fff}.mh-widget-col-1 .mh-slider-widget .flex-control-nav{width:auto;top:15px;right:10px;bottom:auto}.mh-slider-item{position:relative;overflow:hidden}.mh-slider-normal{width:678px;max-width:100%;overflow:hidden}.mh-slider-caption{position:absolute;width:350px;max-width:100%;top:0;right:0;color:#fff;border-bottom:3px solid #e64946;background:#2a2a2a;background:rgba(42,42,42,0.8)}.mh-slider-caption .mh-excerpt-more,.mh-slider-title{color:#fff}.mh-slider-content{padding:25px;overflow:hidden}.mh-slider-content .mh-excerpt{margin-top:10px;margin-top:0.625rem}.mh-widget-col-1 .mh-slider-caption{position:relative;width:100%;margin:0;background:#2a2a2a}.mh-custom-posts-item{padding:20px 0;border-bottom:1px dotted #ebebeb}.mh-custom-posts-item:first-child{padding-top:0}.mh-custom-posts-thumb{float:left;margin-right:15px}.mh-custom-posts-small-title{font-weight:700;line-height:1.3;margin-bottom:5px;margin-bottom:0.3125rem}.mh-custom-posts-small .mh-meta{font-size:11px;font-size:0.6875rem}.mh-custom-posts-header{overflow:hidden}.mh-footer-widget .mh-custom-posts-item{border-color:rgba(255,255,255,0.3)}.mh-footer-widget .mh-custom-posts-small-title{font-weight:400}.mh-posts-large-item{margin-top:20px;padding-bottom:20px;border-bottom:1px dotted #ebebeb}.mh-posts-large-item:first-child{margin-top:0}.mh-posts-large-thumb{position:relative;margin-bottom:10px}.mh-posts-large-thumb img{width:100%}.mh-posts-large-caption{position:absolute;top:0;font-size:13px;font-size:0.8125rem;font-weight:700;color:#fff;padding:10px 15px;background:#e64946;text-transform:uppercase}.mh-posts-large-title{font-size:24px;font-size:1.5rem}.mh-posts-large-excerpt{margin-top:10px}.mh-widget-col-1 .mh-posts-large-title{font-size:20px;font-size:1.25rem}.mh-footer-widget .mh-posts-large-item{border-color:rgba(255,255,255,0.3)}.mh-posts-stacked-wrap{float:left;width:50%;overflow:hidden}.mh-posts-stacked-large,.mh-posts-stacked-small{position:relative}.mh-posts-stacked-large:after,.mh-posts-stacked-small:after{display:block;padding-top:75%;content:' '}.mh-posts-stacked-content{position:absolute;width:100%;height:100%;overflow:hidden}.mh-posts-stacked-thumb img{width:100%}.mh-posts-stacked-thumb:hover .mh-posts-stacked-overlay{background:none}.mh-posts-stacked-overlay{position:absolute;top:0;right:0;bottom:0;left:0;background:rgba(0,0,0,0.2)}.mh-posts-stacked-overlay-small{border-left:1px solid #fff}.mh-posts-stacked-overlay-last{border-top:1px solid #fff}.mh-posts-stacked-item{position:absolute;bottom:25px;left:25px;padding-right:25px}.mh-posts-stacked-title{background:#2a2a2a;background:rgba(42,42,42,0.8)}.mh-posts-stacked-title-large{font-size:24px;padding:5px 10px}.mh-posts-stacked-title-small{font-size:14px;padding:2px 5px}.mh-posts-stacked-title a,.mh-posts-stacked-title a:hover,.mh-posts-stacked-meta a,.mh-posts-stacked-meta a:hover{color:#fff}.mh-posts-stacked-meta{display:inline-block;font-size:10px;font-weight:700;color:#fff;padding:1px 5px;margin-top:5px;background:#e64946;text-transform:uppercase}.mh-posts-stacked-meta .fa,.mh-posts-stacked-meta .mh-meta-date{margin-right:5px}.mh-widget-col-1 .mh-posts-stacked-wrap{float:none;width:100%}.mh-widget-col-1 .mh-posts-stacked-title{font-size:16px;padding:2px 5px}.mh-widget-col-1 .mh-posts-stacked-overlay-small{border-top:1px solid #fff;border-left:none}.mh-widget-col-2 .mh-posts-stacked-large,.mh-widget-col-2 .mh-posts-stacked-columns{float:none;width:100%}.mh-widget-col-2 .mh-posts-stacked-overlay-small{border-top:1px solid #fff;border-left:none}.mh-widget-col-2 .mh-posts-stacked-small:nth-child(odd) .mh-posts-stacked-overlay-small{border-right:1px solid #fff}.mh-widget-col-2 .mh-posts-stacked-title-small{font-size:16px}#mh-mobile .mh-footer-widget .mh-posts-stacked-overlay{border-color:#2a2a2a}.mh-posts-focus-full{width:100%}.mh-posts-focus-full .mh-posts-focus-large{width:65.83%}.mh-posts-focus-full .mh-posts-focus-small-inner{width:31.66%;margin-left:2.5%}.mh-posts-focus-inner{float:right}.mh-posts-focus-large{width:65.67%}.mh-posts-focus-small-inner{width:31.07%;margin-left:3.26%}.mh-posts-focus-outer{margin-right:2.5%;margin-left:0}.mh-posts-focus-item{position:relative}.mh-posts-focus-thumb{margin-bottom:10px}.mh-posts-focus-thumb img{width:100%}.mh-posts-focus-title{font-size:20px;font-size:1.25rem;margin-bottom:5px}.mh-posts-focus-excerpt{margin-top:10px;overflow:hidden}.mh-posts-focus-item-small:first-child{margin-bottom:20px}.mh-home-wide .mh-posts-focus-title-large{font-size:24px;font-size:1.5rem}.mh-home-wide .mh-posts-focus-title-small{font-size:18px;font-size:1.125rem}.mh-home-wide .mh-posts-focus-excerpt-small{display:none}.mh-widget-col-1 .mh-posts-focus-wrap,.mh-widget-col-2 .mh-posts-focus-wrap{float:none;width:100%;margin:0}.mh-widget-col-1 .mh-posts-focus-item,.mh-widget-col-2 .mh-posts-focus-item{padding-bottom:20px;margin-top:20px;border-bottom:1px dotted #ebebeb}.mh-widget-col-1 .mh-posts-focus-item-large,.mh-widget-col-2 .mh-posts-focus-item-large{margin:0}.mh-widget-col-2 .mh-posts-focus-thumb-small{float:left;max-width:235px;margin:0 20px 0 0}.mh-widget-col-2 .mh-posts-focus-title-large{font-size:24px;font-size:1.5rem}#mh-mobile .mh-footer-widget .mh-posts-focus-item{border-color:rgba(255,255,255,0.3)}.mh-tabbed-widget,.mh-tabbed-widget a{color:#000}.mh-tabbed-widget a:hover{color:#e64946}.mh-tabbed-widget .tagcloud a{color:#fff}.mh-tab-buttons{border-bottom:3px solid #e64946}.mh-tab-button{display:block;float:left;width:33.33%;line-height:2;padding:5px 0;background:#f5f5f5;cursor:pointer;text-align:center;-webkit-transition:0.25s ease-out;-moz-transition:0.25s ease-out;transition:0.25s ease-out}.mh-tab-button.active,.mh-tab-button.active:hover{color:#fff;background:#e64946}.mh-tab-button span{padding:0 15px;font-size:24px}.mh-tab-content{display:none;padding:25px;background:#f5f5f5}.mh-tab-posts{display:block}.mh-tab-post-item{padding-bottom:5px;margin-top:5px;border-bottom:1px dotted #e5e5e5}.mh-tab-post-item:first-child,.mh-tab-comment-item:first-child{margin-top:0}.mh-tab-comment-item{margin-top:25px}.mh-tab-comment-avatar{float:left;display:block;margin:0 10px 0 0}.mh-tab-comment-author{font-weight:700;text-transform:uppercase}.mh-tab-comment-excerpt{display:block;padding:25px;margin-top:10px;background:#fff;border-radius:25px;font-style:italic}.mh-footer-widget .mh-tabbed-widget,.mh-footer-widget .mh-tabbed-widget a{color:#fff}.mh-footer-widget .mh-tab-button,.mh-footer-widget .mh-tab-content{background:rgba(255,255,255,0.1)}.mh-footer-widget .mh-tab-post-item{border-color:rgba(255,255,255,0.3)}.mh-footer-widget .mh-tab-button.active{background:#e64946}.mh-footer-widget .mh-tab-comment-excerpt{background:#2a2a2a}.widget_archive li,.widget_categories li,.widget_pages li a,.widget_meta li,.widget_nav_menu .menu > li,.widget_rss li{border-bottom:1px dotted #ebebeb}.widget_archive li a,.widget_categories li a,.widget_pages li a,.widget_meta li a,.widget_nav_menu li a,.widget_rss li{display:block;padding:5px 0}.widget_archive li:first-child a,.widget_categories li:first-child a,.widget_pages li:first-child a,.widget_meta li:first-child a,.widget_nav_menu li:first-child a,.widget_rss li:first-child{padding-top:0}.widget_pages .children li a{padding:5px 0}.widget_nav_menu .sub-menu li{border-top:1px dotted #ebebeb}.widget_nav_menu .sub-menu li:first-child a{padding-top:5px}.widget_rss a{font-weight:700}.widget_rss .rss-date{margin-right:5px}.widget_rss .rss-date,.widget_rss cite{font-size:13px;font-size:0.8125rem;color:#979797}.widget_rss .rssSummary{margin-top:5px}.mh-footer-widget.widget_rss a{font-weight:400}.mh-footer-widget.widget_archive li,.mh-footer-widget.widget_categories li,.mh-footer-widget.widget_pages li a,.mh-footer-widget.widget_meta li,.mh-footer-widget.widget_nav_menu .menu > li,.mh-footer-widget.widget_nav_menu .sub-menu li,.mh-footer-widget.widget_rss li{border-color:rgba(255,255,255,0.3)}#wp-calendar caption{text-align:left;padding:10px;margin-bottom:1px;background:#f5f5f5}#wp-calendar th,#wp-calendar td{padding:5px 10px;text-align:center}#wp-calendar th{font-weight:300}#wp-calendar #prev{text-align:left}.mh-footer-widget #wp-calendar caption{background:rgba(255,255,255,0.1)}.tagcloud a{display:inline-block;font-weight:700;color:#fff;padding:5px 10px;margin:0 4px 8px 0;white-space:nowrap;text-transform:uppercase}.mh-widget .tagcloud a{background:#2a2a2a}.mh-footer-widget .tagcloud a{background:#000}.tagcloud a:hover,.mh-widget .tagcloud a:hover,.mh-footer-widget .tagcloud a:hover{color:#fff;background:#e64946}.widget_recent_entries li,.recentcomments{display:block;padding:5px 0;border-bottom:1px dotted #e5e5e5}.widget_recent_entries li:first-child,.recentcomments:first-child{padding-top:0}.widget_recent_entries .post-date{display:block;font-size:11px;font-size:0.6875rem;color:#979797;margin-bottom:5px}.mh-footer-widget.widget_recent_entries li,.mh-footer-widget .recentcomments{border-color:rgba(255,255,255,0.3)}@media screen and (max-width:1475px){.mh-container{width:95%}.mh-container-inner{width:100%}}@media screen and (max-width:1120px){.mh-main-nav li{font-size:12px}.mh-main-nav li a{padding:10px 15px}}@media screen and (max-width:900px){.mh-container-outer{margin:0 auto}#mh-mobile .mh-container{width:100%}.mh-wrapper,.mh-site-logo{padding:20px}.mh-footer{padding:20px 20px 0}.mh-copyright{text-align:center}.mh-comment-list .children{margin-left:15px}.mh-widget,.mh-footer-widget{margin-bottom:20px}#mh-mobile .mh-footer-area{width:31.66%}.mh-footer-4{display:none}.mh-slider-normal{width:100%}.mh-slider-widget .flex-control-nav{width:auto;top:15px;right:10px;bottom:auto}#mh-mobile .mh-slider-caption{position:relative;width:100%;top:auto;bottom:0;background:rgba(42,42,42,1)}.mh-widget-col-1 .mh-custom-posts-small-title{font-size:13px;font-size:0.8125rem}.mh-meta-comments{display:none}.mh-posts-stacked-item{bottom:20px;left:20px;padding-right:20px}#mh-mobile .mh-posts-stacked-title-small,.mh-widget-col-1 .mh-posts-stacked-title-large{font-size:13px;padding:2px 5px}}@media screen and (max-width:767px){.js .slicknav_menu{display:block}.js .mh-main-nav{display:none}.mh-custom-header,.mh-content,.mh-sidebar,.mh-right-sb #main-content,.mh-left-sb #main-content,#mh-mobile .mh-footer-area{float:none;width:100%;margin:0}.mh-site-logo{text-align:center}.mh-header-title,.entry-title{font-size:24px;font-size:1.5rem}.mh-sidebar{margin-top:20px}.mh-home-sidebar{margin-top:0}.entry-meta{padding:5px 0}#commentform .form-submit{margin:25px 0 0}.mh-footer-4{display:block}.mh-slider-item img{width:100%}#mh-mobile .mh-custom-posts-small-title{font-size:14px;font-size:0.875rem}#mh-mobile .mh-posts-stacked-large,#mh-mobile .mh-posts-stacked-columns{float:none;width:100%}#mh-mobile .mh-posts-stacked-small{float:left;width:50%}#mh-mobile .mh-posts-stacked-overlay-small{border-top:1px solid #fff;border-left:none}.mh-posts-stacked-small:nth-child(odd) .mh-posts-stacked-overlay-small{border-right:1px solid #fff}#mh-mobile .mh-posts-stacked-title-large{font-size:24px;padding:5px 10px}#mh-mobile .mh-posts-stacked-title-small{font-size:16px}#mh-mobile .mh-posts-focus-wrap{float:none;width:100%;margin:0}#mh-mobile .mh-posts-focus-title-small{font-size:20px;font-size:1.25rem}#mh-mobile .mh-posts-focus-item{padding-bottom:20px;margin-top:20px;border-bottom:1px dotted #ebebeb}#mh-mobile .mh-posts-focus-item-large{margin:0}.mh-posts-focus-thumb-small{float:left;max-width:235px;margin:0 20px 0 0}#mh-mobile .mh-posts-focus-excerpt-small{display:block}#mh-mobile .mh-posts-focus-title-large{font-size:24px;font-size:1.5rem}}@media screen and (max-width:620px){input[type=text],input[type=email],input[type=tel],input[type=url]{width:88%}[class*='mh-col-']{float:none;width:100%;margin:0}.entry-meta-categories,.entry-meta-comments{display:none}.mh-author-box{text-align:center}.mh-author-box-avatar{float:none;display:inline-block;margin:0 0 20px}.entry-content ul,.entry-content ol{margin:0 0 20px 20px}.entry-content ul ul,.entry-content ol ol{margin:0 0 0 20px}.single-post .mh-post-nav-next{margin-top:25px}#mh-mobile .mh-slider-title,.mh-posts-large-title,#mh-mobile .mh-posts-focus-title-large{font-size:20px;font-size:1.25rem}.mh-slider-widget .flex-control-nav{display:none}.mh-loop-thumb,#mh-mobile .mh-posts-focus-thumb-small{max-width:80px}.mh-loop-title,#mh-mobile .mh-posts-focus-title-small{font-size:14px;font-size:0.875rem}.mh-loop-meta,.mh-custom-posts-content .mh-meta,.mh-posts-focus-meta-small{display:block;font-size:11px;font-size:0.6875rem}.mh-loop-excerpt,.mh-custom-posts-content .mh-excerpt,.mh-posts-list-excerpt,#mh-mobile .mh-posts-focus-excerpt-small,.mh-posts-focus-caption-small{display:none}.mh-posts-stacked-item{bottom:10px;left:10px;padding-right:10px}#mh-mobile .mh-posts-stacked-title-small{font-size:14px}#mh-mobile .mh-posts-focus-thumb-small{margin:0 15px 0 0}}@media only screen and (max-width:420px){.mh-comment-list .children{margin:0}.mh-comment-body{text-align:center}.mh-comment-gravatar{float:none}.mh-comment-gravatar .avatar{width:80px;height:80px;margin:0 auto 10px}.mh-comment-meta-author .fn{margin:0}}@media only screen and (max-width:360px){#mh-mobile .mh-posts-stacked-wrap{float:none;width:100%}#mh-mobile .mh-posts-stacked-overlay{border-right:none}#mh-mobile .mh-posts-stacked-title{font-size:16px;padding:2px 5px}}</style><style type="text/css" media="all"></style><style type="text/css" media="screen"> .codecolorer{padding:5px;text-align:left}code.codecolorer{padding:2px}.codecolorer-container{margin-bottom:10px;text-align:left}.codecolorer,.codecolorer *,.codecolorer-container,.codecolorer-container *{font:0.9rem/1.3rem Monaco,Lucida Console,monospace;-webkit-text-size-adjust:100%}.codecolorer-container table{border:0px;margin:0px;width:100%;direction:ltr}.codecolorer-container table td{margin:0px;padding:0px;border:0px;width:auto}.codecolorer-container table td.line-numbers{padding:5px;text-align:right;width:1%;direction:ltr}.codecolorer-container table td.line-numbers div{min-width:23px}.codecolorer-container{border:1px solid #9F9F9F}.codecolorer-container table td.line-numbers{color:#888888;background-color:#EEEEEE;border-right:1px solid #9F9F9F}.codecolorer-noborder,.codecolorer-noborder table td.line-numbers{border:0px}.codecolorer-container,.codecolorer{color:#000000;background-color:#F1F1F1}.codecolorer .co0,.codecolorer .co1,.codecolorer .co2,.codecolorer .co3,.codecolorer .co4,.codecolorer .coMULTI{color:#406040;font-style:italic}.codecolorer .nu0,.codecolorer .re3{color:#0080A0}.codecolorer .st0,.codecolorer .st_h,.codecolorer .es0,.codecolorer .es1{color:#C03030}.codecolorer .me1,.codecolorer .me2{color:#0080FF}.codecolorer .kw1,.codecolorer .kw2,.codecolorer .sy1{color:#2060A0}.codecolorer .kw3,.codecolorer .kw4,.codecolorer .kw5,.codecolorer .re2{color:#008080}.codecolorer .re0,.codecolorer .re1{color:#A08000}.codecolorer .br0,.codecolorer .sy0{color:#000000}.codecolorer.xml .re1,.codecolorer.xsl .re1,.codecolorer.xml .re2,.codecolorer.xsl .re2{color:#008080}.codecolorer.xml .re0{color:#2060A0}.codecolorer .ln-xtra{background-color:#ffff66;display:block}.twitlight,.twitlight .codecolorer{color:#F8F8F8;background-color:#141414}.twitlight .codecolorer .co0,.twitlight .codecolorer .co1,.twitlight .codecolorer .co2,.twitlight .codecolorer .co3,.twitlight .codecolorer .co4,.twitlight .codecolorer .coMULTI,code.twitlight .co0,code.twitlight .co1,code.twitlight .co2,code.twitlight .co3,code.twitlight .co4,code.twitlight .coMULTI{color:#5F5A60;font-style:italic}.twitlight .codecolorer .nu0,.twitlight .codecolorer .re3,code.twitlight .nu0,code.twitlight .re3{color:#CF6A4C}.twitlight .codecolorer .st0,.twitlight .codecolorer .st_h,.twitlight .codecolorer .es0,.twitlight .codecolorer .es1{color:#8F9D6A}.twitlight .codecolorer .me1,.twitlight .codecolorer .me2,code.twitlight .me1,code.twitlight .me2{color:#9B703F}.twitlight .codecolorer .kw1,.twitlight .codecolorer .kw2,.twitlight .codecolorer .sy1,code.twitlight .kw1,code.twitlight .kw2,code.twitlight .sy1{color:#CDA869}.twitlight .codecolorer .kw3,.twitlight .codecolorer .kw4,.twitlight .codecolorer .kw5,.twitlight .codecolorer .re2,code.twitlight .kw3,code.twitlight .kw4,code.twitlight .kw5,code.twitlight .re2{color:#F9EE98}.twitlight .codecolorer .re0,.twitlight .codecolorer .re1,code.twitlight .re0,code.twitlight .re1{color:#7587A6}.twitlight .codecolorer .br0,.twitlight .codecolorer .sy0,code.twitlight .br0,code.twitlight .sy0{color:#F8F8F8}.twitlight .codecolorer.xml .re1,.twitlight .codecolorer.xsl .re1,.twitlight .codecolorer.xml .re2,.twitlight .codecolorer.xsl .re2,code.twitlight.xml .re1,code.twitlight.xsl .re1,code.twitlight.xml .re2,code.twitlight.xsl .re2{color:#F9EE98}.twitlight .codecolorer.xml .re0,code.twitlight.xml .re0{color:#CDA869}.twitlight .codecolorer .ln-xtra{background-color:#636338}.dawn,.dawn .codecolorer{color:#080808;background-color:#F9F9F9}.dawn .codecolorer .co0,.dawn .codecolorer .co1,.dawn .codecolorer .co2,.dawn .codecolorer .co3,.dawn .codecolorer .co4,.dawn .codecolorer .coMULTI,code.dawn .co0,code.dawn .co1,code.dawn .co2,code.dawn .co3,code.dawn .co4,code.dawn .coMULTI{color:#5A525F;font-style:italic}.dawn .codecolorer .nu0,.dawn .codecolorer .re3,code.dawn .nu0,code.dawn .re3{color:#811F24}.dawn .codecolorer .st0,.dawn .codecolorer .st_h,.dawn .codecolorer .es0,.dawn .codecolorer .es1,code.dawn .st0,code.dawn .st_h,code.dawn .es0,code.dawn .es1{color:#0B6125}.dawn .codecolorer .me1,.dawn .codecolorer .me2,code.dawn .me1,code.dawn .me2{color:#BF4F24}.dawn .codecolorer .kw1,.dawn .codecolorer .kw2,.dawn .codecolorer .sy1,code.dawn .kw1,code.dawn .kw2,code.dawn .sy1{color:#794938}.dawn .codecolorer .kw3,.dawn .codecolorer .kw4,.dawn .codecolorer .kw5,.dawn .codecolorer .re2,code.dawn .kw3,code.dawn .kw4,code.dawn .kw5,code.dawn .re2{color:#A71D5D}.dawn .codecolorer .re0,.dawn .codecolorer .re1,code.dawn .re0,code.dawn .re1{color:#234A97}.dawn .codecolorer .br0,.dawn .codecolorer .sy0,code.dawn .br0,code.dawn .sy0{color:#080808}.dawn .codecolorer.xml .re1,.dawn .codecolorer.xsl .re1,.dawn .codecolorer.xml .re2,.dawn .codecolorer.xsl .re2,code.dawn.xml .re1,code.dawn.xsl .re1,code.dawn.xml .re2,code.dawn.xsl .re2{color:#A71D5D}.dawn .codecolorer.xml .re0,code.dawn.xml .re0{color:#794938}.blackboard,.blackboard .codecolorer{color:#F8F8F8;background-color:#0C1021}.blackboard .codecolorer .co0,.blackboard .codecolorer .co1,.blackboard .codecolorer .co2,.blackboard .codecolorer .co3,.blackboard .codecolorer .co4,.blackboard .codecolorer .coMULTI,code.blackboard .co0,code.blackboard .co1,code.blackboard .co2,code.blackboard .co3,code.blackboard .co4,code.blackboard .coMULTI{color:#AEAEAE;font-style:italic}.blackboard .codecolorer .nu0,.blackboard .codecolorer .re3,code.blackboard .nu0,code.blackboard .re3{color:#D8FA3C}.blackboard .codecolorer .st0,.blackboard .codecolorer .st_h,.blackboard .codecolorer .es0,.blackboard .codecolorer .es1,code.blackboard .st0,code.blackboard .st_h,code.blackboard .es0,code.blackboard .es1{color:#61CE3C}.blackboard .codecolorer .me1,.blackboard .codecolorer .me2,code.blackboard .me1,code.blackboard .me2{color:#FF6400}.blackboard .codecolorer .kw1,.blackboard .codecolorer .kw2,.blackboard .codecolorer .sy1,code.blackboard .kw1,code.blackboard .kw2,code.blackboard .sy1{color:#FBDE2D}.blackboard .codecolorer .kw3,.blackboard .codecolorer .kw4,.blackboard .codecolorer .kw5,.blackboard .codecolorer .re2,code.blackboard .kw3,code.blackboard .kw4,code.blackboard .kw5,code.blackboard .re2{color:#FBDE2D}.blackboard .codecolorer .re0,.blackboard .codecolorer .re1,code.blackboard .re0,code.blackboard .re1{color:#F8F8F8}.blackboard .codecolorer .br0,.blackboard .codecolorer .sy0,code.blackboard .br0,code.blackboard .sy0{color:#F8F8F8}.blackboard .codecolorer.xml .re1,.blackboard .codecolorer.xsl .re1,.blackboard .codecolorer.xml .re2,.blackboard .codecolorer.xsl .re2,code.blackboard.xml .re1,code.blackboard.xsl .re1,code.blackboard.xml .re2,code.blackboard.xsl .re2{color:#FBDE2D}.blackboard .codecolorer.xml .re0,code.blackboard.xml .re0{color:#D8FA3C}.blackboard .codecolorer .ln-xtra{background-color:#636338}.mac-classic,.mac-classic .codecolorer{color:#000000;background-color:#FFFFFF}.mac-classic .codecolorer .co0,.mac-classic .codecolorer .co1,.mac-classic .codecolorer .co2,.mac-classic .codecolorer .co3,.mac-classic .codecolorer .co4,.mac-classic .codecolorer .coMULTI,code.mac-classic .co0,code.mac-classic .co1,code.mac-classic .co2,code.mac-classic .co3,code.mac-classic .co4,code.mac-classic .coMULTI{color:#0066FF;font-style:italic}.mac-classic .codecolorer .nu0,.mac-classic .codecolorer .re3,code.mac-classic .nu0,code.mac-classic .re3{color:#0000CD}.mac-classic .codecolorer .st0,.mac-classic .codecolorer .st_h,.mac-classic .codecolorer .es0,.mac-classic .codecolorer .es1,code.mac-classic .st0,code.mac-classic .st_h,code.mac-classic .es0,code.mac-classic .es1{color:#036A07}.mac-classic .codecolorer .me1,.mac-classic .codecolorer .me2,code.mac-classic .me1,code.mac-classic .me2{color:#0000A2}.mac-classic .codecolorer .kw1,.mac-classic .codecolorer .kw2,.mac-classic .codecolorer .sy1,code.mac-classic .kw1,code.mac-classic .kw2,code.mac-classic .sy1{color:#0000FF}.mac-classic .codecolorer .kw3,.mac-classic .codecolorer .kw4,.mac-classic .codecolorer .kw5,.mac-classic .codecolorer .re2,code.mac-classic .kw3,code.mac-classic .kw4,code.mac-classic .kw5,code.mac-classic .re2{color:#0000FF}.mac-classic .codecolorer .re0,.mac-classic .codecolorer .re1,code.mac-classic .re0,code.mac-classic .re1{color:#318495}.mac-classic .codecolorer .br0,.mac-classic .codecolorer .sy0,code.mac-classic .br0,code.mac-classic .sy0{color:#000000}.mac-classic .codecolorer.xml .re1,.mac-classic .codecolorer.xsl .re1,.mac-classic .codecolorer.xml .re2,.mac-classic .codecolorer.xsl .re2,code.mac-classic.xml .re1,code.mac-classic.xsl .re1,code.mac-classic.xml .re2,code.mac-classic.xsl .re2{color:#0000FF}.mac-classic .codecolorer.xml .re0,code.mac-classic.xml .re0{color:#0000CD}.vibrant,.vibrant .codecolorer{color:#FFFFFF;background-color:#000000}.vibrant .codecolorer .co0,.vibrant .codecolorer .co1,.vibrant .codecolorer .co2,.vibrant .codecolorer .co3,.vibrant .codecolorer .co4,.vibrant .codecolorer .coMULTI,code.vibrant .co0,code.vibrant .co1,code.vibrant .co2,code.vibrant .co3,code.vibrant .co4,code.vibrant .coMULTI{color:#9933CC;font-style:italic}.vibrant .codecolorer .nu0,.vibrant .codecolorer .re3,code.vibrant .nu0,code.vibrant .re3{color:#339999}.vibrant .codecolorer .st0,.vibrant .codecolorer .st_h,.vibrant .codecolorer .es0,.vibrant .codecolorer .es1,code.vibrant .st0,code.vibrant .st_h,code.vibrant .es0,code.vibrant .es1{color:#66FF00}.vibrant .codecolorer .me1,.vibrant .codecolorer .me2,code.vibrant .me1,code.vibrant .me2{color:#FFCC00}.vibrant .codecolorer .kw1,.vibrant .codecolorer .kw2,.vibrant .codecolorer .sy1,code.vibrant .kw1,code.vibrant .kw2,code.vibrant .sy1{color:#FF6600}.vibrant .codecolorer .kw3,.vibrant .codecolorer .kw4,.vibrant .codecolorer .kw5,.vibrant .codecolorer .re2,code.vibrant .kw3,code.vibrant .kw4,code.vibrant .kw5,code.vibrant .re2{color:#FFCC00}.vibrant .codecolorer .re0,.vibrant .codecolorer .re1,code.vibrant .re0,code.vibrant .re1{color:#FFFFFF}.vibrant .codecolorer .br0,.vibrant .codecolorer .sy0,code.vibrant .br0,code.vibrant .sy0{color:#FFFFFF}.vibrant .codecolorer.xml .re1,.vibrant .codecolorer.xsl .re1,.vibrant .codecolorer.xml .re2,.vibrant .codecolorer.xsl .re2,code.vibrant.xml .re1,code.vibrant.xsl .re1,code.vibrant.xml .re2,code.vibrant.xsl .re2{color:#FFCC00}.vibrant .codecolorer.xml .re0,code.vibrant.xml .re0{color:#339999}.vibrant .codecolorer .ln-xtra{background-color:#636338}.railscasts,.railscasts .codecolorer{color:#E6E1DC;background-color:#2B2B2B}.railscasts ::selection,.railscasts .codecolorer ::selection{background:#adb9d2}.railscasts ::-moz-selection,.railscasts .codecolorer ::-moz-selection{background:#566381}.railscasts .codecolorer .co0,.railscasts .codecolorer .co1,.railscasts .codecolorer .co2,.railscasts .codecolorer .co3,.railscasts .codecolorer .co4,.railscasts .codecolorer .coMULTI{color:#BC9458;font-style:italic}.railscasts .codecolorer .nu0,.railscasts .codecolorer .re3{color:#6D9CBE}.railscasts .codecolorer .st0,.railscasts .codecolorer .st_h,.railscasts .codecolorer .es0,.railscasts .codecolorer .es1{color:#A5C261}.railscasts .codecolorer .me1,.railscasts .codecolorer .me2{color:#FFC66D}.railscasts .codecolorer .kw1,.railscasts .codecolorer .kw2,.railscasts .codecolorer .sy1{color:#CC7833}.railscasts .codecolorer .kw3,.railscasts .codecolorer .kw4,.railscasts .codecolorer .kw5,.railscasts .codecolorer .re2{color:#6E9CBE}.railscasts .codecolorer .railscasts .re0,.railscasts .codecolorer .re1{color:#D0D0FF}.railscasts .codecolorer .br0,.railscasts .codecolorer .sy0{color:#E6E1DC}.railscasts .codecolorer.xml .re1,.railscasts .codecolorer.xsl .re1,.railscasts .codecolorer.xml .re2,.railscasts .codecolorer.xsl .re2,code.railscasts.xml .re1,code.railscasts.xsl .re1,code.railscasts.xml .re2,code.railscasts.xsl .re2{color:#FFCC00}.railscasts .codecolorer.xml .re0,code.railscasts.xml .re0{color:#6E9CBE}.railscasts .codecolorer .ln-xtra{background-color:#636338}.solarized-dark,.solarized-dark .codecolorer{color:#839496;background-color:#002b36}.solarized-dark ::selection,.solarized-dark .codecolorer ::selection{background:#073642}.solarized-dark ::-moz-selection,.solarized-dark .codecolorer ::-moz-selection{background:#073642}.solarized-dark .codecolorer .co0,.solarized-dark .codecolorer .co1,.solarized-dark .codecolorer .co2,.solarized-dark .codecolorer .co3,.solarized-dark .codecolorer .co4,.solarized-dark .codecolorer .coMULTI{color:#586E75;font-style:normal}.solarized-dark .codecolorer .nu0,.solarized-dark .codecolorer .re3{color:#269186}.solarized-dark .codecolorer .st0,.solarized-dark .codecolorer .st_h,.solarized-dark .codecolorer .es0,.solarized-dark .codecolorer .es1{color:#269186}.solarized-dark .codecolorer .me1,.solarized-dark .codecolorer .me2{color:#748B00}.solarized-dark .codecolorer .kw1,.solarized-dark .codecolorer .kw2,.solarized-dark .codecolorer .sy1{color:#859900}.solarized-dark .codecolorer .kw3,.solarized-dark .codecolorer .kw4,.solarized-dark .codecolorer .kw5,.solarized-dark .codecolorer .re2{color:#A57800}.solarized-dark .codecolorer .solarized-dark .re0,.solarized-dark .codecolorer .re1{color:#268BD2}.solarized-dark .codecolorer .br0,.solarized-dark .codecolorer .sy0{color:#D01F1E}.solarized-dark .codecolorer.xml .re1,.solarized-dark .codecolorer.xsl .re1,.solarized-dark .codecolorer.xml .re2,.solarized-dark .codecolorer.xsl .re2,code.solarized-dark.xml .re1,code.solarized-dark.xsl .re1,code.solarized-dark.xml .re2,code.solarized-dark.xsl .re2{color:#A57800}.solarized-dark .codecolorer.xml .re0,code.solarized-dark.xml .re0{color:#859900}.solarized-dark .codecolorer .ln-xtra{background-color:#FDF6E3}.codecolorer-container.solarized-dark table td.line-numbers{color:#839496;background-color:#073642;border-right-color:#001B26}.solarized-light,.solarized-light .codecolorer{color:#586E75;background-color:#FDF6E3}.solarized-light ::selection,.solarized-light .codecolorer ::selection{background:#EEE8D5}.solarized-light ::-moz-selection,.solarized-light .codecolorer ::-moz-selection{background:#EEE8D5}.solarized-light .codecolorer .co0,.solarized-light .codecolorer .co1,.solarized-light .codecolorer .co2,.solarized-light .codecolorer .co3,.solarized-light .codecolorer .co4,.solarized-light .codecolorer .coMULTI{color:#93A1A1;font-style:normal}.solarized-light .codecolorer .nu0,.solarized-light .codecolorer .re3{color:#269186}.solarized-light .codecolorer .st0,.solarized-light.codecolorer .st_h,.solarized-light .codecolorer .es0,.solarized-light .codecolorer .es1{color:#269186}.solarized-light .codecolorer .me1,.solarized-light .codecolorer .me2{color:#748B00}.solarized-light .codecolorer .kw1,.solarized-light .codecolorer .kw2,.solarized-light .codecolorer .sy1{color:#748B00}.solarized-light .codecolorer .kw3,.solarized-light .codecolorer .kw4,.solarized-light .codecolorer .kw5,.solarized-light .codecolorer .re2{color:#A57800}.solarized-light .codecolorer .solarized-light .re0,.solarized-light .codecolorer .re1{color:#4EB1F6}.solarized-light .codecolorer .br0,.solarized-light .codecolorer .sy0{color:#D01F1E}.solarized-light .codecolorer.xml .re1,.solarized-light .codecolorer.xsl .re1,.solarized-light .codecolorer.xml .re2,.solarized-light .codecolorer.xsl .re2,code.solarized-light.xml .re1,code.solarized-light.xsl .re1,code.solarized-light.xml .re2,code.solarized-light.xsl .re2{color:#A57800}.solarized-light .codecolorer.xml .re0,code.solarized-light.xml .re0{color:#748B00}.codecolorer-container.solarized-light table td.line-numbers{color:#839496;background-color:#EEE8D5;border-right-color:#CEC8B5}.codecolorer-container::-webkit-scrollbar{-webkit-appearance:none;width:7px;height:7px}.codecolorer-container::-webkit-scrollbar-thumb{border-radius:4px;background-color:rgba(0,0,0,.5);-webkit-box-shadow:0 0 1px rgba(255,255,255,.5)}.codecolorer-container.blackboard::-webkit-scrollbar-thumb,.codecolorer-container.twitlight::-webkit-scrollbar-thumb,.codecolorer-container.vibrant::-webkit-scrollbar-thumb,.codecolorer-container.railscasts::-webkit-scrollbar-thumb,.codecolorer-container.solarized-dark::-webkit-scrollbar-thumb{background-color:rgba(255,255,255,.5);-webkit-box-shadow:0 0 1px rgba(0,0,0,.5)}</style>

<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/cache/abtf/proxy/e6/c8/03/e6c803e2ce659c900824cc26bdd7bec8.js' defer='defer'></script>
<script type='text/javascript'>try{jQuery.noConflict();}catch(e){};</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-includes/js/jquery/jquery-migrate.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/above-the-fold-optimization/public/js/jquery.lazyloadxt.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/above-the-fold-optimization/public/js/jquery.lazyloadxt.widget.min.js' defer='defer'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var ctNocache = {"ajaxurl":"http:\/\/adventuresinmachinelearning.com\/wp-admin\/admin-ajax.php","info_flag":"","set_cookies_flag":"1","blog_home":"http:\/\/adventuresinmachinelearning.com\/"};
/* ]]> */
</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/cleantalk-spam-protect/inc/cleantalk_nocache.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/code-syntax-block/assets/prism.js' defer='defer'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var edd_scripts = {"ajaxurl":"http:\/\/adventuresinmachinelearning.com\/wp-admin\/admin-ajax.php","position_in_cart":"","has_purchase_links":"","already_in_cart_message":"You have already added this item to your cart","empty_cart_message":"Your cart is empty","loading":"Loading","select_option":"Please select an option","is_checkout":"0","default_gateway":"paypal","redirect_to_checkout":"0","checkout_page":"","permalinks":"1","quantities_enabled":"","taxes_enabled":"0"};
/* ]]> */
</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/easy-digital-downloads/assets/js/edd-ajax.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/cache/abtf/proxy/cf/d0/ce/cfd0cee7c6d1154f7dec014fa83848eb.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-includes/js/jquery/jquery.ui.touch-punch.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-admin/js/iris.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/simple-code-highlighter/js/pretty.js' defer='defer'></script>
<script type='text/javascript' src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML%2CSafe.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/wp-hide-post/public/js/wp-hide-post-public.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/themes/mh-magazine-lite/js/scripts.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-includes/js/comment-reply.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/js/jquery.sonar.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/lazy-load/js/lazy-load.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/dynamic-to-top/js/libs/jquery.easing.js' defer='defer'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mv_dynamic_to_top = {"text":"0","version":"0","min":"200","speed":"1000","easing":"easeInOutExpo","margin":"20"};
/* ]]> */
</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/dynamic-to-top/js/dynamic.to.top.min.js' defer='defer'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var boxzilla_options = {"testMode":"","boxes":[{"id":163,"icon":"&times;","content":"<h4>Hi!<\/h4>\n<p>Get your free <em>40 page<\/em> eBook &#8211; <strong>An Introduction to Neural Networks for Beginners<\/strong> by signing up (plus news and other exciting stuff!)<\/p>\n<p>&nbsp;<\/p>\n<script type=\"text\/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n<\/script><!-- MailChimp for WordPress v4.1.0 - https:\/\/wordpress.org\/plugins\/mailchimp-for-wp\/ --><form id=\"mc4wp-form-1\" class=\"mc4wp-form mc4wp-form-165\" method=\"post\" data-id=\"165\" data-name=\"Main sign-up\" ><div class=\"mc4wp-form-fields\"><p>\r\n\t<label>Email address: <\/label>\r\n\t<input type=\"email\" name=\"EMAIL\" placeholder=\"Your email address\" required \/>\r\n<\/p>\r\n<p>\r\n  \r\n<\/p>\r\n<p>\r\n\t<input type=\"submit\" value=\"Sign up\" \/>\r\n<\/p><div style=\"display: none;\"><input type=\"text\" name=\"_mc4wp_honeypot\" value=\"\" tabindex=\"-1\" autocomplete=\"off\" \/><\/div><input type=\"hidden\" name=\"_mc4wp_timestamp\" value=\"1536643234\" \/><input type=\"hidden\" name=\"_mc4wp_form_id\" value=\"165\" \/><input type=\"hidden\" name=\"_mc4wp_form_element_id\" value=\"mc4wp-form-1\" \/><\/div><div class=\"mc4wp-response\"><\/div><\/form><!-- \/ MailChimp for WordPress Plugin -->\n","css":{"background_color":"#edf9ff","width":340,"border_color":"#dd7575","border_width":4,"border_style":"dashed","position":"center"},"trigger":{"method":"time_on_page","value":"6"},"animation":"fade","cookie":{"triggered":0,"dismissed":24},"rehide":false,"position":"center","screenWidthCondition":null,"closable":true,"post":{"id":163,"title":"Main pop-up","slug":"main-pop-up"}}]};
/* ]]> */
</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/boxzilla/assets/js/script.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-includes/js/wp-embed.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/js/prism.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/plugins/normalize-whitespace/prism-normalize-whitespace.min.js' defer='defer'></script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/pastacode/plugins/line-numbers/prism-line-numbers.min.js' defer='defer'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var mc4wp_forms_config = [];
/* ]]> */
</script>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js' defer='defer'></script>
<!--[if lte IE 9]>
<script type='text/javascript' src='http://adventuresinmachinelearning.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js' defer='defer'></script>
<![endif]-->
<!-- We need this for debugging themes using Speed Booster Pack Plugin v3.0 -->
<!-- Scripts to footer: enabled -->
<!-- CSS to footer: enabled -->
<!-- Defer parsing of js: enabled -->
<!-- CSS Async: enabled -->
</body>
</html>