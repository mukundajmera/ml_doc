<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Neural networks &#8211; Adventures in Machine Learning</title>
	<atom:link href="http://adventuresinmachinelearning.com/category/deep-learning/neural-networks/feed/" rel="self" type="application/rss+xml" />
	<link>http://adventuresinmachinelearning.com</link>
	<description>Learn and explore machine learning</description>
	<lastBuildDate>Sun, 09 Sep 2018 07:53:16 +0000</lastBuildDate>
	<language>en-AU</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.8</generator>
	<item>
		<title>A PyTorch tutorial &#8211; deep learning in Python</title>
		<link>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/</link>
		<comments>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#respond</comments>
		<pubDate>Thu, 26 Oct 2017 08:46:48 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>
		<category><![CDATA[PyTorch]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=618</guid>
		<description><![CDATA[<p>So &#8211; if you&#8217;re a follower of this blog and you&#8217;ve been trying out your own deep learning networks in TensorFlow and Keras, you&#8217;ve probably <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" title="A PyTorch tutorial &#8211; deep learning in Python">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/">A PyTorch tutorial &#8211; deep learning in Python</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>So &#8211; if you&#8217;re a follower of this blog and you&#8217;ve been trying out your own deep learning networks in <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow</a> and <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras</a>, you&#8217;ve probably come across the somewhat frustrating business of debugging these deep learning libraries. Sure, they have Python APIs, but it&#8217;s kinda hard to figure out what exactly is happening when something goes wrong. They also don&#8217;t seem to play well with Python libraries such as numpy, scipy, scikit-learn, Cython and so on. Enter the <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> deep learning library &#8211; one of it&#8217;s <a href="http://pytorch.org/about/" target="_blank" rel="noopener">purported benefits</a> is that is a deep learning library that is more at home in Python, which, for a Python aficionado like myself, sounds great. It also has nifty features such as dynamic computational graph construction as opposed to the static computational graphs present in TensorFlow and Keras (for more on computational graphs, see below). It&#8217;s also on the up and up, with its development supported by companies such as Facebook, Twitter, NVIDIA and so on. So let&#8217;s dive into it in this PyTorch tutorial.</p>
<p>The first question to consider &#8211; is it better than TensorFlow? That&#8217;s a fairly subjective judgement &#8211; performance-wise there doesn&#8217;t appear to be a great deal of difference. Check out <a href="https://www.forbes.com/sites/quora/2017/07/10/is-pytorch-better-than-tensorflow/" target="_blank" rel="noopener">this article</a> for a quick comparison. In any case, its clear the PyTorch is here to stay and is likely to be a real contender in the &#8220;contest&#8221; between deep learning libraries, so let&#8217;s kick start our learning of it. I&#8217;ll leave it to you to decide which is &#8220;better&#8221;.</p>
<p>In this PyTorch tutorial we will introduce some of the core features of PyTorch, and build a fairly simple densely connected neural network to classify hand-written digits. To learn how to build more complex models in PyTorch, check out my post <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/" target="_blank" rel="noopener">Convolutional Neural Networks Tutorial in PyTorch</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video course learner, check out this inexpensive, highly rated, Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F" target="new">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>A PyTorch tutorial &#8211; the basics</h1>
<p>In this section, we&#8217;ll go through the basic ideas of PyTorch starting at tensors and computational graphs and finishing at the Variable class and the PyTorch autograd functionality.</p>
<h2>Installing on Windows</h2>
<p>For starters, if you are a Windows user like myself, you&#8217;ll find that there is no straight-forward installation options for that operating system on the <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch website</a>. However, there is a successful way to do it, check out <a href="https://www.superdatascience.com/pytorch/">this website</a> for instructions. It&#8217;s well worth the effort to get this library installed if you are a Windows user like myself.</p>
<h2>Computational graphs</h2>
<p>The first thing to understand about any deep learning library is the idea of a computational graph. A computational graph is a set of calculations, which are called <em>nodes</em>, and these nodes are connected in a directional ordering of computation. In other words, some nodes are dependent on other nodes for their input, and these nodes in turn output the results of their calculations to other nodes. A simple example of a computational graph for the calculation $a = (b + c) * (c + 2)$ can be seen below &#8211; we can break this calculation up into the following steps/nodes:</p>
<p>\begin{align}<br />
d &amp;= b + c \\<br />
e &amp;= c + 2 \\<br />
a &amp;= d * e<br />
\end{align}</p>
<figure id="attachment_158" style="width: 220px" class="wp-caption aligncenter"><img class=" wp-image-158" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png" alt="PyTorch tutorial - simple computational graph" width="220" height="253" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png 296w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example-260x300.png 260w" sizes="(max-width: 220px) 100vw, 220px" /><figcaption class="wp-caption-text">Simple computational graph</figcaption></figure>
<p>&nbsp;</p>
<p>The benefits of using a computational graph is that each node is like its own independently functioning piece of code (once it receives all its required inputs). This allows various performance optimizations to be performed in running the calculations such as threading and multiple processing / parallelism. All the major deep learning frameworks (TensorFlow, Theano, PyTorch etc.) involve constructing such computational graphs, through which neural network operations can be built and through which gradients can be back-propagated (if you&#8217;re unfamiliar with back-propagation, see my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">neural networks tutorial</a>).</p>
<h2>Tensors</h2>
<p>Tensors are matrix-like data structures which are essential components in deep learning libraries and efficient computation. Graphical Processing Units (GPUs) are especially effective at calculating operations between tensors, and this has spurred the surge in deep learning capability in recent times. In PyTorch, tensors can be declared simply in a number of ways:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import torch
x = torch.Tensor(2, 3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code creates a tensor of size (2, 3) &#8211; i.e. 2 rows and 3 columns, filled with zero float values i.e:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 0  0  0
 0  0  0
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can also create tensors filled random float values:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = torch.rand(2, 3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Multiplying tensors, adding them and so forth is straight-forward:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = torch.ones(2,3)
y = torch.ones(2,3) * 2
x + y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This returns:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 3  3  3
 3  3  3
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Another great thing is the numpy slice functionality that is available &#8211; for instance y[:, 1]
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">y[:,1] = y[:,1] + 1</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This returns:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 2  3  2
 2  3  2
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now you know how to create tensors and manipulate them in PyTorch, in the next step of this PyTorch tutorial let&#8217;s look at something a bit more complicated.</p>
<h2>Autograd in PyTorch</h2>
<p>In any deep learning library, there needs to be a mechanism where error gradients are calculated and back-propagated through the computational graph. This mechanism, called autograd in PyTorch, is easily accessible and intuitive. The Variable class is the main component of this autograd system in PyTorch. This Variable class wraps a tensor, and allows automatic gradient computation on the tensor when the .backward() function is called (more on this later). The object contains the data of the tensor, the gradient of the tensor (once computed with respect to some other value i.e. the loss) and also contains a reference to whatever function created the variable (if it is a user created function, this reference will be null).</p>
<p>Let&#8217;s create a Variable from a simple tensor:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = Variable(torch.ones(2, 2) * 2, requires_grad=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the Variable declaration above, we pass in a tensor of (2, 2) 2-values and we specify that this variable requires a gradient. If we were using this in a neural network, this would mean that this Variable would be trainable. If we set this flag to False, the Variable would not be trained. For this simple example we aren&#8217;t training anything, but we do want to interrogate the gradient for this Variable as will be shown below.</p>
<p>Next, let&#8217;s create another Variable, constructed based on operations on our original Variable <em>x</em>.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">z = 2 * (x * x) + 5 * x</code></pre> <div class="code-embed-infos"> </div> </div>
<p>To get the gradient of this operation with respect to <em>x</em> i.e. <em>dz/dx</em> we can analytically calculate this to by 4x +5. If all elements of <em>x</em> are 2, then we should expect the gradient <em>dz/dx</em> to be a (2, 2) shaped tensor with 13-values. However, first we have to run the .backwards() operation to compute these gradients. Of course, to compute gradients, we need to compute them with respect to something. In this case, we can supply a (2,2) tensor of 1-values to be what we compute the gradients against &#8211; so the calculation simply becomes <em>d/dx</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">z.backward(torch.ones(2, 2))
print(x.grad)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This produces the following output:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code">Variable containing:
 13  13
 13  13
[torch.FloatTensor of size 2x2]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As you can observe, the gradient is equal to a (2, 2), 13-valued tensor as we predicted. Note that the gradient is stored in the <em>x </em>Variable, in the property .grad.</p>
<p>Now that we&#8217;ve covered the basics of tensors, Variables and the autograd functionality within PyTorch, we can move onto creating a simple neural network in PyTorch which will showcase this functionality further.</p>
<h1>Creating a neural network in PyTorch</h1>
<p>This section is the main show of this PyTorch tutorial. To access the code for this tutorial, check out this website&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>. Here we will create a simple 4-layer  fully connected neural network (including an &#8220;input layer&#8221; and two hidden layers) to classify the hand-written digits of the MNIST dataset. The architecture we&#8217;ll use can be seen in the figure below:</p>
<figure id="attachment_439" style="width: 413px" class="wp-caption aligncenter"><img class="size-full wp-image-439" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture.jpg" alt="PyTorch tutorial - fully connected neural network example architecture" width="413" height="334" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture.jpg 413w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-300x243.jpg 300w" sizes="(max-width: 413px) 100vw, 413px" /><figcaption class="wp-caption-text">Fully connected neural network example architecture</figcaption></figure>
<p>The input layer consists of 28 x 28 (=784) greyscale pixels which constitute the input data of the MNIST data set. This input is then passed through two fully connected hidden layers, each with 200 nodes, with the nodes utilizing a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener">ReLU</a> activation function. Finally, we have an output layer with ten nodes corresponding to the 10 possible classes of hand-written digits (i.e. 0 to 9). We will use a <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax</a> output layer to perform this classification.</p>
<p>Let&#8217;s create the neural network.</p>
<h2>The neural network class</h2>
<p>In order to create a neural network in PyTorch, you need to use the included class nn.Module. To use this base class, we also need to use Python <a href="https://docs.python.org/2/tutorial/classes.html" target="_blank" rel="noopener">class inheritance</a> &#8211; this basically allows us to use all of the functionality of the nn.Module base class, but still have overwriting capabilities of the base class for the model construction / forward pass through the network. Some actual code will help explain:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the class definition, you can see the inheritance of the base class nn.Module. Then, in the first line of the class initialization (def __init__(self):) we have the required Python super() function, which creates an instance of the base nn.Module class. The following three lines is where we create our fully connected layers as per the architecture diagram. A fully connected neural network layer is represented by the nn.Linear object, with the first argument in the definition being the number of nodes in layer <em>l</em> and the next argument being the number of nodes in layer <em>l+1</em>. As you can observer, the first layer takes the 28 x 28 input pixels and connects to the first 200 node hidden layer. Then we have another 200 to 200 hidden layer, and finally a connection between the last hidden layer and the output layer (with 10 nodes).</p>
<p>Now we&#8217;ve setup the &#8220;skeleton&#8221; of our network architecture, we have to define how data flows through out network. We do this by defining a <em>forward</em>() method in our class &#8211; this method overwrites a dummy method in the base class, and needs to be defined for each network:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def forward(self, x):
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return F.log_softmax(x)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>For the <em>forward()</em> method, we supply the input data <em>x</em> as the primary argument. We feed this into our first fully connected layer <em>(self.fc1(x))</em> and then apply a ReLU activation to the nodes in this layer using <em>F.relu()</em>. Because of the hierarchical nature of this network, we replace <em>x</em> at each stage, feeding it into the next layer. We do this through our three fully connected layers, except for the last one &#8211; instead of a ReLU activation we return a log softmax &#8220;activation&#8221;. This, combined with the negative log likelihood loss function which will be defined later, gives us a multi-class cross entropy based loss function which we will use to train the network.</p>
<p>So that&#8217;s it &#8211; we&#8217;ve defined our neural network. Pretty easy right?</p>
<p>The next step is to create an instance of this network architecture:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">net = Net()
print(net)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>When we print the instance of the class Net, we get the following output:</p>
<blockquote><p>Net (<br />
(fc1): Linear (784 -&gt; 200)<br />
(fc2): Linear (200 -&gt; 200)<br />
(fc3): Linear (200 -&gt; 10)<br />
)</p></blockquote>
<p>This is pretty handy as it confirms the structure of our network for us.</p>
<h2>Training the network</h2>
<p>Next we have to setup an optimizer and a loss criterion:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create a stochastic gradient descent optimizer
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)
# create a loss function
criterion = nn.NLLLoss()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first line, we create a stochastic gradient descent optimizer, and we specify the learning rate (which I&#8217;ve passed to this function as 0.01) and a momentum of 0.9. The other ingredient we need to supply to our optimizer is all the parameters of our network &#8211; thankfully PyTorch make supplying these parameters easy by the .parameters() method of the base nn.Module class that we inherit from in the Net class.</p>
<p>Next, we set our loss criterion to be the negative log likelihood loss &#8211; this combined with our log softmax output from the neural network gives us an equivalent cross entropy loss for our 10 classification classes.</p>
<p>Now it&#8217;s time to train the network. During training, I will be extracting data from a data loader object which is included in the PyTorch utilities module. I won&#8217;t go into the details here (I&#8217;ll leave that for a future post), but you can find the code on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>. This data loader will supply batches of input and target data which we&#8217;ll supply to our network and loss function respectively. Here&#8217;s the full training code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># run the main training loop
for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)
        data = data.view(-1, 28*28)
        optimizer.zero_grad()
        net_out = net(data)
        loss = criterion(net_out, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print(&#039;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#039;.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The outer training loop is the number of epochs, whereas the inner training loop runs through the entire training set in batch sizes which are specified in the code as batch_size. On the next line, we convert <em>data</em> and <em>target</em> into PyTorch variables. The MNIST input data-set which is supplied in the <em>torchvision</em> package (which you&#8217;ll need to install using pip if you run the code for this tutorial) has the size (batch_size, 1, 28, 28) when extracted from the data loader &#8211; this 4D tensor is more suited to <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">convolutional neural network</a> architecture, and not so much our fully connected network. Therefore we need to flatten out the (1, 28, 28) data to a single dimension of 28 x 28 =  784 input nodes.</p>
<p>The .view() function operates on PyTorch variables to reshape them. If we want to be agnostic about the size of a given dimension, we can use the &#8220;-1&#8221; notation in the size definition. So by using <em>data.view(-1, 28*28)</em> we say that the second dimension must be equal to 28 x 28, but the first dimension should be calculated from the size of the original data variable. In practice, this means that <em>data</em> will now be of size (batch_size, 784). We can pass a batch of input data like this into our network and the magic of PyTorch will do all the hard work by efficiently performing the required operations on the tensors.</p>
<p>On the next line, we run <em>optimizer.zero_grad()</em> &#8211; this zeroes / resets all the gradients in the model, so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, but in PyTorch you have to remember to do it explicitly. Let&#8217;s single out the next two lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">net_out = net(data)
loss = criterion(net_out, target)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line is where we pass the input data batch into the model &#8211; this will actually call the <em>forward()</em> method in our Net class. After this line is run, the variable <em>net_out</em> will now hold the log softmax output of our neural network for the given data batch. That&#8217;s one of the great things about PyTorch, you can activate whatever normal Python debugger you usually use and instantly get a gauge of what is happening in your network. This is opposed to other deep learning libraries such as TensorFlow and Keras which require elaborate debugging sessions to be setup before you can check out what your network is actually producing. I hope you&#8217;ll play around with how useful this debugging is, by utilizing the code for this PyTorch tutorial <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">here</a>.</p>
<p>The second line is where we get the negative log likelihood loss between the output of our network and our target batch data.</p>
<p>Let&#8217;s look at the next two lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">loss.backward()
optimizer.step()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line here runs a back-propagation operation from the loss Variable backwards through the network. If you compare this with our review of the .backward() operation that we undertook earlier in this PyTorch tutorial, you&#8217;ll notice that we aren&#8217;t supplying the .backward() operation with an argument. Scalar variables, when we call .backward() on them, don&#8217;t require arguments &#8211; only tensors require a matching sized tensor argument to be passed to the .backward() operation.</p>
<p>The next line is where we tell PyTorch to execute a gradient descent step based on the gradients calculated during the .backward() operation.</p>
<p>Finally, we print out some results every time we reach a certain number of iterations:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if batch_idx % log_interval == 0:
    print(&#039;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#039;.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This print function shows our progress through the epochs and also gives the network loss at that point in the training. Note how you access the loss &#8211; you access the Variable .data property, which in this case will be a single valued array. We access the scalar loss by executing loss.data[0].</p>
<p>Running this training loop you&#8217;ll get an output that looks something like this:</p>
<blockquote><p>Train Epoch: 9 [52000/60000 (87%)] Loss: 0.015086</p>
<p>Train Epoch: 9 [52000/60000 (87%)] Loss: 0.015086</p>
<p>Train Epoch: 9 [54000/60000 (90%)] Loss: 0.030631</p>
<p>Train Epoch: 9 [56000/60000 (93%)] Loss: 0.052631</p>
<p>Train Epoch: 9 [58000/60000 (97%)] Loss: 0.052678</p></blockquote>
<p>After 10 epochs, you should get a loss value down around the &lt;0.05 magnitude.</p>
<h2>Testing the network</h2>
<p>To test the trained network on our test MNIST data set, we can run the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># run a test loop
test_loss = 0
correct = 0
for data, target in test_loader:
    data, target = Variable(data, volatile=True), Variable(target)
    data = data.view(-1, 28 * 28)
    net_out = net(data)
    # sum up batch loss
    test_loss += criterion(net_out, target).data[0]
    pred = net_out.data.max(1)[1]  # get the index of the max log-probability
    correct += pred.eq(target.data).sum()

test_loss /= len(test_loader.dataset)
print(&#039;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#039;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This loop is the same as the previous training loop up until the <em>test_loss</em> line &#8211; here we extract the network loss using the .data[0] property as before, but all in the same line. Next, we have the <em>pred</em> line, where the data.max(1) method is used &#8211; this .max() method can return the index of the maximum value in a certain dimension of a tensor. Now, the output of our neural network will be of size (batch_size, 10), where each value of the 10-length second dimension is a log probability which the network assigns to each output class (i.e. it is the log probability of whether the given image is a digit between 0 and 9). So for each input sample/row in the batch, net_out.data will look something like this:</p>
[-1.3106e+01, -1.6731e+01, -1.1728e+01, -1.1995e+01, -1.5886e+01, -1.7700e+01, -2.4950e+01, -5.9817e-04, -1.3334e+01, -7.4527e+00]
<p>&nbsp;</p>
<p>The value with the highest log probability is the digit that the network considers to be the most probable given the input image &#8211; this is the best prediction of the class from the network. In the example of net_out.data above, it is the value -5.9817e-04 which is maximum, which corresponds to the digit &#8220;7&#8221;. So for this sample, the predicted digit is &#8220;7&#8221;. The .max(1) function will determine this maximum value in the second dimension (if we wanted the maximum in the first dimension, we&#8217;d supply an argument of 0) and returns both the maximum value that it has found, and the index that this maximum value was found at. It therefore has a size of (batch_size, 2) &#8211; in this case we are interested in the index where the maximum value is found at, therefore we access these values by calling .max(1)[1].</p>
<p>Now we have the prediction of the neural network for each sample in the batch determined, we can compare this with the actual target class from our training data, and count how many times in the batch the neural network got it right. We can use the PyTorch .eq() function to do this, which compares the values in two tensors and if they match, returns a 1. If they don&#8217;t match, it returns a 0:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">correct += pred.eq(target.data).sum()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>By summing the output of the .eq() function, we get a count of the number of times the neural network has produced a correct output, and we take an accumulating sum of these correct predictions so that we can determine the overall accuracy of the network on our test data set. Finally, after running through the test data in batches, we print out the averaged loss and accuracy:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">test_loss /= len(test_loader.dataset)
print(&#039;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#039;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>After training the network for 10 epochs, we get the following output from the above code on the test data:</p>
<blockquote><p>Test set: Average loss: 0.0003, Accuracy: 9783/10000 (98%)</p></blockquote>
<p>A 98% accuracy &#8211; not bad!</p>
<p>So there you have it &#8211; this PyTorch tutorial has shown you the basic ideas in PyTorch, from tensors to the autograd functionality, and finished with how to build a fully connected neural network using the nn.Module. I hope it was helpful. If you&#8217;d like to learn more about PyTorch, check out my post on <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/" target="_blank" rel="noopener">Convolutional Neural Networks in PyTorch</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video course learner, check out this inexpensive, highly rated, Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F" target="new">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/">A PyTorch tutorial &#8211; deep learning in Python</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Python TensorFlow Tutorial &#8211; Build a Neural Network</title>
		<link>http://adventuresinmachinelearning.com/python-tensorflow-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments</comments>
		<pubDate>Sat, 08 Apr 2017 04:53:48 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=148</guid>
		<description><![CDATA[<p>Google&#8217;s TensorFlow has been a hot topic in deep learning recently.  The open source software, designed to allow efficient computation of data flow graphs, is especially suited to <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" title="Python TensorFlow Tutorial &#8211; Build a Neural Network">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/">Python TensorFlow Tutorial &#8211; Build a Neural Network</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p style="text-align: left;">Google&#8217;s TensorFlow has been a hot topic in deep learning recently.  The open source software, designed to allow efficient computation of data flow graphs, is especially suited to deep learning tasks.  It is designed to be executed on single or multiple CPUs and GPUs, making it a good option for complex deep learning tasks.  In it&#8217;s most recent incarnation &#8211; version 1.0 &#8211; it can even be run on certain mobile operating systems.  This introductory tutorial to TensorFlow will give an overview of some of the basic concepts of TensorFlow in Python.  These will be a good stepping stone to building more complex deep learning networks, such as <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">Convolution Neural Networks</a>, <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">natural language models</a> and <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">Recurrent Neural Networks</a> in the package.  We&#8217;ll be creating a simple three-layer neural network to classify the MNIST dataset.  This tutorial assumes that you are familiar with the basics of neural networks, which you can get up to scratch with in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener noreferrer">neural networks tutorial</a> if required.  To install TensorFlow, follow the instructions <a href="https://www.tensorflow.org/install/" target="_blank" rel="noopener noreferrer">here</a>. The code for this tutorial can be found in <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener noreferrer">this site&#8217;s GitHub repository</a>.  Once you&#8217;re done, you also might want to check out a higher level deep learning library that sits on top of TensorFlow called Keras &#8211; see <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">my Keras tutorial</a>.</p>
<p>(<strong>Note</strong>: this is a rather lengthy post &#8211; to get the 14 page eBook version, click below)</p>

        <link rel="stylesheet" type="text/css" href="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/css/bookblock.css" />
        <link rel="stylesheet" type="text/css" href="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/css/component.css" />
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/modernizr.custom.js"></script>
<div id="bookshelf" class="bookshelf">
                    <div id="ebook_formData" class="ebook_formData"></div>
            <figure>
                            <div class="perspective"><div class="book" data-book="book-653"><div onClick="document.getElementById('42014f5b2ebb0b678a56b4ca3c27d083').submit(); return false;" class="cover"><div data-dd="dd" class="front" style="cursor: pointer; cursor: hand; background: url(http://adventuresinmachinelearning.com/wp-content/uploads/ebooks_misc/2017/10/Python-TensorFlow-Tutorial-cover.png);"></div><div class="inner inner-left"></div></div><div class="inner inner-right"></div></div></div><div class="buttons">
                            		<a href="#" style="display:none;">Look inside</a>
                            		<a href="#" class="details_link">Details</a>
                            		
                            		
                            		<a target="_blank" href="" style="display:none;" class="">Preview</a>
									<a style="" class="ebook_buy_link" data-md5_nonce="5afe730983d13f04441fce87653e0d3b" href="" onClick="document.getElementById('42014f5b2ebb0b678a56b4ca3c27d083').submit(); return false;">Buy (&#36;1.99)</a>
<form method="post" id="42014f5b2ebb0b678a56b4ca3c27d083" name="dmp_order_form" action="https://www.paypal.com/cgi-bin/webscr">
		<input type="hidden" name="rm" value="0">
		<input type="hidden" name="discount_rate" value="0">
		<input type="hidden" name="cmd" value="_xclick">
		<input type="hidden" name="charset" value="utf-8">
		<input type="hidden" name="md5_nonce" value="5afe730983d13f04441fce87653e0d3b">
		<input type="hidden" name="lc" value="">
		<input type="hidden" name="no_shipping" value="0">
		<input type="hidden" name="button_subtype" value="products">
		<input type="hidden" name="return" value="http://adventuresinmachinelearning.com/thanks-for-the-download/?ebook_key=1100ad6a0c26921dcb2dd659c3af0c08&action=thank_you">
		<input type="hidden" name="cancel_return" value="http://adventuresinmachinelearning.com/ebook/python-tensorflow-tutorial-ebook/?ebook_key=1100ad6a0c26921dcb2dd659c3af0c08&action=cancel">
		<input type="hidden" name="notify_url" value="http://adventuresinmachinelearning.com/?task=ipn&ebook_key=1100ad6a0c26921dcb2dd659c3af0c08&md5_nonce=5afe730983d13f04441fce87653e0d3b">
		<input type="hidden" name="item_name" value="Python TensorFlow Tutorial – Build a Neural Network eBook">
		<input type="hidden" name="item_number" value="1">
		<input type="hidden" name="tax_rate" value="">
		<input type="hidden" name="amount" value="1.99">
		<input type="hidden" name="upload" value="1">
		<input type="hidden" name="custom" value="653|9b7d06616c8feb062c70f97248d808ac">
		<input type="hidden" name="business" id="af69ae50c1be74757508c8f7fae10abd" value="andy@adventuresinmachinelearning.com">
		<input type="hidden" name="receiver_email" id="af69ae50c1be74757508c8f7fae10abd0xff" value="andy@adventuresinmachinelearning.com">
		<input type="hidden" name="currency_code" value="USD">
		<input type="hidden" name="cbt" value="Click here to go to download page">
		<input type="hidden" name="no_note" value="1">
		</form>
</div>
                            <figcaption><h2>Python TensorFlow Tutorial – Build a Neural Network eBook <span>Dr Andrew Thomas</span></h2></figcaption>
                            <div class="details">
                                <ul>
                                    <li><div class="ebookStorEbookContent">In this eBook, you'll learn how to build a neural network from scratch in TensorFlow - this is a great place to start investigating this very popular deep learning library. Get the 14 page PDF for only $1.99USD

&nbsp;</div></li>
                                    <li></li>
                                    <li></li>
                                    <li></li>
                                </ul>
                            <span class="close-details"></span></div>
            </figure>
            <style>
.book[data-book="book-653"] .cover::before {
background: url(http://adventuresinmachinelearning.com/wp-content/uploads/ebooks_misc/2017/10/Python-TensorFlow-Tutorial-cover.png);
}
.bookshelf figure .buttons a {
	font-size:0.65em !important;
}
.bookshelf figure h2 {
	font-size:1.8em !important;
}
div#bookshelf {
    -ms-transform: scale(1, 1); /* IE 9 */
    -webkit-transform: scale(1, 1); /* Safari */
    transform: scale(1, 1);
}            </style>
            
                </div>

        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/bookblock.min.js"></script>
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/classie.js"></script>
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/bookshelf.js"></script>
    
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>Once you&#8217;ve read this post, and you&#8217;d like to learn more in a video course, I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>First, let&#8217;s have a look at the main ideas of TensorFlow.</p>
<h1>1.0 TensorFlow graphs</h1>
<p>TensorFlow is based on graph based computation &#8211; &#8220;what on earth is that?&#8221;, you might say.  It&#8217;s an alternative way of conceptualising mathematical calculations.  Consider the following expression $a = (b + c) * (c + 2)$.  We can break this function down into the following components:</p>
<p>\begin{align}<br />
d &amp;= b + c \\<br />
e &amp;= c + 2 \\<br />
a &amp;= d * e<br />
\end{align}</p>
<p>Now we can represent these operations graphically as:</p>
<figure id="attachment_158" style="width: 260px" class="wp-caption aligncenter"><img class="wp-image-158 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example-260x300.png" alt="TensorFlow tutorial - simple computational graph" width="260" height="300" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example-260x300.png 260w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png 296w" sizes="(max-width: 260px) 100vw, 260px" /><figcaption class="wp-caption-text">Simple computational graph</figcaption></figure>
<p>This may seem like a silly example &#8211; but notice a powerful idea in expressing the equation this way: two of the computations ($d=b+c$ and $e=c+2$) can be performed in parallel.  By splitting up these calculations across CPUs or GPUs, this can give us significant gains in computational times.  These gains are a <em>must</em> for big data applications and deep learning &#8211; especially for complicated neural network architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).  The idea behind TensorFlow is to ability to create these computational graphs in code and allow significant performance improvements via parallel operations and other efficiency gains.</p>
<p>We can look at a similar graph in TensorFlow below, which shows the computational graph of a three-layer neural network.</p>
<figure id="attachment_205" style="width: 252px" class="wp-caption aligncenter"><img class="wp-image-205 size-full" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/TensorFlow-data-flow-graph.gif" alt="TensorFlow tutorial - data flow graph" width="252" height="448" /><figcaption class="wp-caption-text">TensorFlow data flow graph</figcaption></figure>
<p>The animated data flows between different nodes in the graph are <em>tensors</em> which are multi-dimensional data arrays.  For instance, the input data tensor may be 5000 x 64 x 1, which represents a 64 node input layer with 5000 training samples.  After the input layer there is a hidden layer with <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener noreferrer">rectified linear units </a>as the activation function.  There is a final output layer (called a &#8220;logit layer&#8221; in the above graph) which uses cross entropy as a cost/loss function.  At each point we see the relevant tensors flowing to the &#8220;Gradients&#8221; block which finally flow to the <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener noreferrer">Stochastic Gradient Descent</a> optimiser which performs the back-propagation and gradient descent.</p>
<p>Here we can see how computational graphs can be used to represent the calculations in neural networks, and this, of course, is what TensorFlow excels at.  Let&#8217;s see how to perform some basic mathematical operations in TensorFlow to get a feel for how it all works.</p>
<h1>2.0 A Simple TensorFlow example</h1>
<p>Let&#8217;s first make TensorFlow perform our little example calculation above &#8211; $a = (b + c) * (c + 2)$.  First we need to introduce ourselves to TensorFlow <em>variables</em> and <em>constants</em>.  Let&#8217;s declare some then I&#8217;ll explain the syntax:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import tensorflow as tf

# first, create a TensorFlow constant
const = tf.constant(2.0, name=&quot;const&quot;)
    
# create TensorFlow variables
b = tf.Variable(2.0, name=&#039;b&#039;)
c = tf.Variable(1.0, name=&#039;c&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As can be observed above, TensorFlow constants can be declared using the <em>tf.constant</em> function, and variables with the <em>tf.Variable</em> function.  The first element in both is the value to be assigned the constant / variable when it is initialised.  The second is an optional name string which can be used to label the constant / variable &#8211; this is handy for when you want to do visualisations (as will be discussed briefly later).  TensorFlow will infer the type of the constant / variable from the initialised value, but it can also be set explicitly using the optional <em>dtype </em>argument.  TensorFlow has many of its own types like<em> tf.float32</em>, <em>tf.int32</em> etc. &#8211; see them all <a href="https://www.tensorflow.org/api_docs/python/tf/DType" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>It&#8217;s important to note that, as the Python code runs through these commands, the variables haven&#8217;t actually been declared as they would have been if you just had a standard Python declaration (i.e. b = 2.0).  Instead, all the constants, variables, operations and the computational graph are only created when the initialisation commands are run.</p>
<p>Next, we create the TensorFlow <em>operations</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># now create some operations
d = tf.add(b, c, name=&#039;d&#039;)
e = tf.add(c, const, name=&#039;e&#039;)
a = tf.multiply(d, e, name=&#039;a&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>TensorFlow has a wealth of operations available to perform all sorts of interactions between variables, some of which we&#8217;ll get to later in the tutorial.  The operations above are pretty obvious, and they instantiate the operations $b+c$, $c+2.0$ and $d*e$.</p>
<p>The next step is to setup an object to initialise the variables and the graph structure:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># setup the variable initialisation
init_op = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Ok, so now we are all set to go.  To run the operations between the variables, we need to start a TensorFlow session &#8211; <em>tf.Session.  </em>The TensorFlow session is an object where all operations are run. TensorFlow was initially created in a <em>static </em>graph paradigm &#8211; in other words, first all the operations and variables are defined (the graph structure) and then these are compiled within the <em>tf.Session</em> object. There is now the option to build graphs on the fly using the TensorFlow Eager framework, to check this out see my <a href="http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/">TensorFlow Eager tutorial</a>.</p>
<p>However, there are still advantages in building static graphs using the <em>tf.Session</em> object. You can do this by using the <em>with</em> Python syntax, to run the graph like so:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># start the session
with tf.Session() as sess:
    # initialise the variables
    sess.run(init_op)
    # compute the output of the graph
    a_out = sess.run(a)
    print(&quot;Variable a is {}&quot;.format(a_out))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first command within the <em>with</em> block is the initialisation, which is run with the, well, <em>run</em> command.  Next we want to figure out what the variable <em>a</em> should be.  All we have to do is run the operation which calculates <em>a</em> i.e. <em>a = tf.multiply(d, e, name=&#8217;a&#8217;).  </em>Note that <em>a</em> is an <em>operation</em>, not a variable and therefore it can be <em>run</em>.  We do just that with the <em>sess.run(a)</em> command and assign the output to <em>a_out, </em>the value of which we then print out.</p>
<p>Note something cool &#8211; we defined operations <em>d</em> and <em>e</em> which need to be calculated before we can figure out what <em>a</em> is.  However, we don&#8217;t have to explicitly run <em>those</em> operations, as TensorFlow knows what other operations and variables the operation <em>a</em> depends on, and therefore runs the necessary operations on its own.  It does this through its data flow graph which shows it all the required dependencies. Using the TensorBoard functionality, we can see the graph that TensorFlow created in this little program:</p>
<figure id="attachment_214" style="width: 1035px" class="wp-caption aligncenter"><img class="wp-image-214 size-full" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Simple-TensorFlow-graph.png" alt="TensorFlow tutorial - simple graph" width="1035" height="379" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Simple-TensorFlow-graph.png 1035w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Simple-TensorFlow-graph-300x110.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Simple-TensorFlow-graph-768x281.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Simple-TensorFlow-graph-1024x375.png 1024w" sizes="(max-width: 1035px) 100vw, 1035px" /><figcaption class="wp-caption-text">Simple TensorFlow graph</figcaption></figure>
<p>Now that&#8217;s obviously a trivial example &#8211; what if we had an array of <em>b</em> values that we wanted to calculate the value of <em>a</em> over?</p>
<h2>2.1 The TensorFlow placeholder</h2>
<p>Let&#8217;s also say that we didn&#8217;t know what the value of the array <em>b</em> would be during the declaration phase of the TensorFlow problem (i.e. before the <em>with tf.Session() as sess</em>) stage.  In this case, TensorFlow requires us to declare the basic structure of the data by using the <em>tf.placeholder </em>variable declaration.  Let&#8217;s use it for <em>b</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create TensorFlow variables
b = tf.placeholder(tf.float32, [None, 1], name=&#039;b&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Because we aren&#8217;t providing an initialisation in this declaration, we need to tell TensorFlow what data type each element within the <em>tensor </em>is going to be.  In this case, we want to use <em>tf.float32</em>.  The second argument is the shape of the data that will be &#8220;injected&#8221; into this variable.  In this case, we want to use a (? x 1) sized array &#8211; because we are being cagey about how much data we are supplying to this variable (hence the &#8220;?&#8221;), the placeholder is willing to accept a <em>None</em> argument in the size declaration.  Now we can inject as much 1-dimensional data that we want into the <em>b </em>variable.</p>
<p>The only other change we need to make to our program is in the <em>sess.run(a,&#8230;)</em> command:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that we have added the <em>feed_dict</em> argument to the <em>sess.run(a,&#8230;) </em>command.  Here we remove the mystery and specify exactly what the variable <em>b</em> is to be &#8211; a one-dimensional range from 0 to 10.  As suggested by the argument name, <em>feed_dict, </em>the input to be supplied is a Python dictionary, with each key being the name of the <em>placeholder</em> that we are filling.</p>
<p>When we run the program again this time we get:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">Variable a is [[  3.]
 [  6.]
 [  9.]
 [ 12.]
 [ 15.]
 [ 18.]
 [ 21.]
 [ 24.]
 [ 27.]
 [ 30.]]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Notice how TensorFlow adapts naturally from a scalar output (i.e. a singular output when <em>a=9.0</em>) to a <em>tensor</em> (i.e. an array/matrix)?  This is based on its understanding of how the data will flow through the graph.</p>
<p>Now we are ready to build a basic MNIST predicting neural network.</p>
<h1>3.0 A Neural Network Example</h1>
<p>Now we&#8217;ll go through an example in TensorFlow of creating a simple three layer neural network.  In future articles, we&#8217;ll show how to build more complicated neural network structures such as convolution neural networks and recurrent neural networks.  For this example though, we&#8217;ll keep it simple.  If you need to scrub up on your neural network basics, check out my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">popular tutorial on the subject</a>.  In this example, we&#8217;ll be using the MNIST dataset (and its associated loader) that the TensorFlow package provides.  This MNIST dataset is a set of 28&#215;28 pixel grayscale images which represent hand-written digits.  It has 55,000 training rows, 10,000 testing rows and 5,000 validation rows.</p>
<p>We can load the data by running:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The <em>one_hot=True</em> argument specifies that instead of the labels associated with each image being the digit itself i.e. &#8220;4&#8221;, it is a vector with &#8220;one hot&#8221; node and all the other nodes being zero i.e. [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].  This lets us easily feed it into the output layer of our neural network.</p>
<h2>3.1 Setting things up</h2>
<p>Next, we can set-up the placeholder variables for the training data (and some training parameters):</p>
<p><!--?prettify linenums=true?--></p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Python optimisation variables
learning_rate = 0.5
epochs = 10
batch_size = 100

# declare the training data placeholders
# input x - for 28 x 28 pixels = 784
x = tf.placeholder(tf.float32, [None, 784])
# now declare the output data placeholder - 10 digits
y = tf.placeholder(tf.float32, [None, 10])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Notice the <em>x </em>input layer is 784 nodes corresponding to the 28 x 28 (=784) pixels, and the <em>y</em> output layer is 10 nodes corresponding to the 10 possible digits.  Again, the size of <em>x </em>is (? x 784), where the ? stands for an as yet unspecified number of samples to be input &#8211; this is the function of the <em>placeholder</em> variable.</p>
<p>Now we need to setup the weight and bias variables for the three layer neural network.  There are always <em>L-1</em> number of weights/bias tensors, where <em>L</em> is the number of layers.  So in this case, we need to setup two tensors for each:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># now declare the weights connecting the input to the hidden layer
W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name=&#039;W1&#039;)
b1 = tf.Variable(tf.random_normal([300]), name=&#039;b1&#039;)
# and the weights connecting the hidden layer to the output layer
W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name=&#039;W2&#039;)
b2 = tf.Variable(tf.random_normal([10]), name=&#039;b2&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Ok, so let&#8217;s unpack the above code a little.  First, we declare some variables for W1 and b1, the weights and bias for the connections between the input and hidden layer.  This neural network will have 300 nodes in the hidden layer, so the size of the weight tensor W1 is [784, 300].  We initialise the values of the weights using a random normal distribution with a mean of zero and a standard deviation of 0.03.  TensorFlow has a replicated version of the <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html" target="_blank" rel="noopener noreferrer">numpy random normal function</a>, which allows you to create a matrix of a given size populated with random samples drawn from a given distribution.  Likewise, we create W2 and b2 variables to connect the hidden layer to the output layer of the neural network.</p>
<p>Next, we have to setup node inputs and activation functions of the hidden layer nodes:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># calculate the output of the hidden layer
hidden_out = tf.add(tf.matmul(x, W1), b1)
hidden_out = tf.nn.relu(hidden_out)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first line, we execute the standard matrix multiplication of the weights (<em>W1</em>) by the input vector <em>x </em>and we add the bias <em>b1</em>.  The matrix multiplication is executed using the <em>tf.matmul</em> operation.  Next, we finalise the <em>hidden_out</em> operation by applying a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener noreferrer">rectified linear unit </a>activation function to the matrix multiplication plus bias.  Note that TensorFlow has a rectified linear unit activation already setup for us, <em>tf.nn.relu</em>.</p>
<p>This is to execute the following equations, as detailed in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener noreferrer">neural networks tutorial</a>:</p>
<p>\begin{align}<br />
z^{(l+1)} &amp;= W^{(l)} x + b^{(l)} \\<br />
h^{(l+1)} &amp;= f(z^{(l+1)})<br />
\end{align}</p>
<p>Now, let&#8217;s setup the output layer, <em>y_</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># now calculate the hidden layer output - in this case, let&#039;s use a softmax activated
# output layer
y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Again we perform the weight multiplication with the output from the hidden layer (<em>hidden_out</em>) and add the bias, <em>b2</em>.  In this case, we are going to use a <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener noreferrer">softmax activation</a> for the output layer &#8211; we can use the included TensorFlow softmax function <em>tf.nn.softmax</em>.</p>
<p>We also have to include a cost or loss function for the optimisation / backpropagation to work on. Here we&#8217;ll use the cross entropy cost function, represented by:</p>
<p>$$J = -\frac{1}{m} \sum_{i=1}^m \sum_{j=1}^n y_j^{(i)}log(y_j\_^{(i)}) + (1 &#8211; y_j^{(i)})log(1 &#8211; y_j\_^{(i)})$$</p>
<p>Where $y_j^{(i)}$ is the ith training label for output node <em>j</em>, $y_j\_^{(i)}$ is the ith predicted label for output node <em>j, </em><em>m </em>is the number of training / batch samples and <em>n </em>is the number .  There are two operations occurring in the above equation.  The first is the summation of the logarithmic products and additions <em>across all the output nodes</em>.  The second is taking a mean of this summation <em>across all the training samples</em>.  We can implement this cross entropy cost function in TensorFlow with the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)
                         + (1 - y) * tf.log(1 - y_clipped), axis=1))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Some explanation is required.  The first line is an operation converting the output <em>y_</em> to a clipped version, limited between 1e-10 to 0.999999.  This is to make sure that we never get a case were we have<em> </em>a <em>log(0) </em>operation occurring during training &#8211; this would return NaN and break the training process.  The second line is the cross entropy calculation.</p>
<p>To perform this calculation, first we use TensorFlow&#8217;s <em>tf.reduce_sum </em>function &#8211; this function basically takes the sum of a given axis of the tensor you supply.  In this case, the tensor that is supplied is the element-wise cross-entropy calculation for a single node and training sample i.e.: $y_j^{(i)}log(y_j\_^{(i)}) + (1 &#8211; y_j^{(i)})log(1 &#8211; y_j\_^{(i)})$.  Remember that <em>y</em> and <em>y_clipped</em> in the above calculation are (<em>m</em> x<em> 10</em>) tensors &#8211; therefore we need to perform the first sum over the second axis.  This is specified using the axis=1 argument, where &#8220;1&#8221; actually refers to the second axis when we have a zero-based indices system like Python.</p>
<p>After this operation, we have an (<em>m</em> x <em>1</em>) tensor.  To take the mean of this tensor and complete our cross entropy cost calculation (i.e. execute this part $\frac{1}{m} \sum_{i=1}^m$), we use TensorFlow&#8217;s <em>tf.reduce_mean</em> function.  This function simply takes the mean of whatever tensor you provide it.  So now we have a cost function that we can use in the training process.</p>
<p>Let&#8217;s setup the optimiser in TensorFlow:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># add an optimiser
optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here we are just using the gradient descent optimiser provided by TensorFlow.  We initialize it with a learning rate, then specify what we want it to do &#8211; i.e. minimise the cross entropy cost operation we created.  This function will then perform the gradient descent (for more details on gradient descent see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener noreferrer">here</a> and <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener noreferrer">here</a>) and the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener noreferrer">backpropagation</a> for you.  How easy is that?  TensorFlow has a library of popular neural network training optimisers, see <a href="https://www.tensorflow.org/api_guides/python/train" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>Finally, before we move on to the main show, were we actually run the operations, let&#8217;s setup the variable initialisation operation and an operation to measure the accuracy of our predictions:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># finally setup the initialisation operator
init_op = tf.global_variables_initializer()

# define an accuracy assessment operation
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The correct prediction operation <em>correct_prediction</em> makes use of the TensorFlow <em>tf.equal</em> function which returns <em>True</em> or <em>False </em>depending on whether to arguments supplied to it are equal.  The <em>tf.argmax</em> function is the same as the <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html" target="_blank" rel="noopener noreferrer">numpy argmax function</a>, which returns the index of the maximum value in a vector / tensor.  Therefore, the <em>correct_prediction</em> operation returns a tensor of size (<em>m</em> x <em>1</em>) of <em>True</em> and <em>False</em> values designating whether the neural network has correctly predicted the digit.  We then want to calculate the mean accuracy from this tensor &#8211; first we have to cast the type of the <em>correct_prediction </em>operation from a Boolean to a TensorFlow float in order to perform the <em>reduce_mean</em> operation.  Once we&#8217;ve done that, we now have an <em>accuracy</em> operation ready to assess the performance of our neural network.</p>
<h2>3.2 Setting up the training</h2>
<p>We now have everything we need to setup the training process of our neural network.  I&#8217;m going to show the full code below, then talk through it:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"> # start the session
 with tf.Session() as sess:
    # initialise the variables
    sess.run(init_op)
    total_batch = int(len(mnist.train.labels) / batch_size)
    for epoch in range(epochs):
         avg_cost = 0
         for i in range(total_batch):
             batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)
              _, c = sess.run([optimiser, cross_entropy], 
                          feed_dict={x: batch_x, y: batch_y})
             avg_cost += c / total_batch
         print(&quot;Epoch:&quot;, (epoch + 1), &quot;cost =&quot;, &quot;{:.3f}&quot;.format(avg_cost))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Stepping through the lines above, the first couple relate to setting up the <em>with</em> statement and running the initialisation operation.  The third line relates to our mini-batch training scheme that we are going to run for this neural network.  If you want to know about mini-batch gradient descent, check out this <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener noreferrer">post</a>.  In the third line, we are calculating the number of batches to run through in each training epoch.  After that, we loop through each training epoch and initialise an <em>avg_cost</em> variable to keep track of the average cross entropy cost for each epoch.  The next line is where we extract a randomised batch of samples, <em>batch_x </em>and <em>batch_y</em>, from the MNIST training dataset.  The TensorFlow provided MNIST dataset has a handy utility function, <em>next_batch, </em>that makes it easy to extract batches of data for training.</p>
<p>The following line is where we run two operations.  Notice that <em>sess.run</em> is capable of taking a list of operations to run as its first argument.  In this case, supplying <em>[optimiser, cross_entropy]</em> as the list means that both these operations will be performed.  As such, we get two outputs, which we have assigned to the variables <em>_</em> and <em>c</em>.  We don&#8217;t really care too much about the output from the <em>optimiser</em> operation but we want to know the output from the <em>cross_entropy</em> operation &#8211; which we have assigned to the variable <em>c</em>.  Note, we run the <em>optimiser</em> (and <em>cross_entropy</em>) operation on the batch samples.  In the following line, we use <em>c </em>to calculate the average cost for the epoch.</p>
<p>Finally, we print out our progress in the average cost, and after the training is complete, we run the <em>accuracy</em> operation to print out the accuracy of our trained network on the test set.  Running this program produces the following output:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code">Epoch: 1 cost = 0.586
Epoch: 2 cost = 0.213
Epoch: 3 cost = 0.150
Epoch: 4 cost = 0.113
Epoch: 5 cost = 0.094
Epoch: 6 cost = 0.073
Epoch: 7 cost = 0.058
Epoch: 8 cost = 0.045
Epoch: 9 cost = 0.036
Epoch: 10 cost = 0.027

Training complete!
0.9787</code></pre> <div class="code-embed-infos"> </div> </div>
<p>There we go &#8211; approximately 98% accuracy on the test set, not bad.  We could do a number of things to improve the model, such as regularisation (see this <a href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/">tips and tricks post</a>), but here we are just interested in exploring TensorFlow.  You can also use TensorBoard visualisation to look at things like the increase in accuracy over the epochs:</p>
<figure id="attachment_242" style="width: 1307px" class="wp-caption aligncenter"><img class="wp-image-242 size-full" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/TensorBoard-increase-in-accuracy-NN.png" alt="TensorFlow tutorial - TensorBoard accuracy plot" width="1307" height="615" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/TensorBoard-increase-in-accuracy-NN.png 1307w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/TensorBoard-increase-in-accuracy-NN-300x141.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/TensorBoard-increase-in-accuracy-NN-768x361.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/TensorBoard-increase-in-accuracy-NN-1024x482.png 1024w" sizes="(max-width: 1307px) 100vw, 1307px" /><figcaption class="wp-caption-text">TensorBoard plot of the increase in accuracy over 10 epochs</figcaption></figure>
<p>In a future article, I&#8217;ll introduce you to TensorBoard visualisation, which is a really nice feature of TensorFlow.  For now, I hope this tutorial was instructive and helps get you going on the TensorFlow journey.  Just a reminder, you can check out the code for this post <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener noreferrer">here</a>.  I&#8217;ve also written an article that shows you how to build more complex neural networks such as <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">convolution neural networks</a>, <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">recurrent neural networks</a> and <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec natural language models</a> in TensorFlow.  You also might want to check out a higher level deep learning library that sits on top of TensorFlow called Keras &#8211; see <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">my Keras tutorial</a>.</p>
<p>Have fun!</p>

        <link rel="stylesheet" type="text/css" href="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/css/bookblock.css" />
        <link rel="stylesheet" type="text/css" href="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/css/component.css" />
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/modernizr.custom.js"></script>
<div id="bookshelf" class="bookshelf">
                    <div id="ebook_formData" class="ebook_formData"></div>
            <figure>
                            <div class="perspective"><div class="book" data-book="book-653"><div onClick="document.getElementById('0fd727af857ac9b192d4dd824b3ef2b7').submit(); return false;" class="cover"><div data-dd="dd" class="front" style="cursor: pointer; cursor: hand; background: url(http://adventuresinmachinelearning.com/wp-content/uploads/ebooks_misc/2017/10/Python-TensorFlow-Tutorial-cover.png);"></div><div class="inner inner-left"></div></div><div class="inner inner-right"></div></div></div><div class="buttons">
                            		<a href="#" style="display:none;">Look inside</a>
                            		<a href="#" class="details_link">Details</a>
                            		
                            		
                            		<a target="_blank" href="" style="display:none;" class="">Preview</a>
									<a style="" class="ebook_buy_link" data-md5_nonce="403055b32c5805ca1ddde85f37587ac5" href="" onClick="document.getElementById('0fd727af857ac9b192d4dd824b3ef2b7').submit(); return false;">Buy (&#36;1.99)</a>
<form method="post" id="0fd727af857ac9b192d4dd824b3ef2b7" name="dmp_order_form" action="https://www.paypal.com/cgi-bin/webscr">
		<input type="hidden" name="rm" value="0">
		<input type="hidden" name="discount_rate" value="0">
		<input type="hidden" name="cmd" value="_xclick">
		<input type="hidden" name="charset" value="utf-8">
		<input type="hidden" name="md5_nonce" value="403055b32c5805ca1ddde85f37587ac5">
		<input type="hidden" name="lc" value="">
		<input type="hidden" name="no_shipping" value="0">
		<input type="hidden" name="button_subtype" value="products">
		<input type="hidden" name="return" value="http://adventuresinmachinelearning.com/thanks-for-the-download/?ebook_key=838c2a2c1e8554369c48d0e30f4d79d8&action=thank_you">
		<input type="hidden" name="cancel_return" value="http://adventuresinmachinelearning.com/ebook/python-tensorflow-tutorial-ebook/?ebook_key=838c2a2c1e8554369c48d0e30f4d79d8&action=cancel">
		<input type="hidden" name="notify_url" value="http://adventuresinmachinelearning.com/?task=ipn&ebook_key=838c2a2c1e8554369c48d0e30f4d79d8&md5_nonce=403055b32c5805ca1ddde85f37587ac5">
		<input type="hidden" name="item_name" value="Python TensorFlow Tutorial – Build a Neural Network eBook">
		<input type="hidden" name="item_number" value="1">
		<input type="hidden" name="tax_rate" value="">
		<input type="hidden" name="amount" value="1.99">
		<input type="hidden" name="upload" value="1">
		<input type="hidden" name="custom" value="653|9b7d06616c8feb062c70f97248d808ac">
		<input type="hidden" name="business" id="af69ae50c1be74757508c8f7fae10abd" value="andy@adventuresinmachinelearning.com">
		<input type="hidden" name="receiver_email" id="af69ae50c1be74757508c8f7fae10abd0xff" value="andy@adventuresinmachinelearning.com">
		<input type="hidden" name="currency_code" value="USD">
		<input type="hidden" name="cbt" value="Click here to go to download page">
		<input type="hidden" name="no_note" value="1">
		</form>
</div>
                            <figcaption><h2>Python TensorFlow Tutorial – Build a Neural Network eBook <span>Dr Andrew Thomas</span></h2></figcaption>
                            <div class="details">
                                <ul>
                                    <li><div class="ebookStorEbookContent">In this eBook, you'll learn how to build a neural network from scratch in TensorFlow - this is a great place to start investigating this very popular deep learning library. Get the 14 page PDF for only $1.99USD

&nbsp;</div></li>
                                    <li></li>
                                    <li></li>
                                    <li></li>
                                </ul>
                            <span class="close-details"></span></div>
            </figure>
            <style>
.book[data-book="book-653"] .cover::before {
background: url(http://adventuresinmachinelearning.com/wp-content/uploads/ebooks_misc/2017/10/Python-TensorFlow-Tutorial-cover.png);
}
.bookshelf figure .buttons a {
	font-size:0.65em !important;
}
.bookshelf figure h2 {
	font-size:1.8em !important;
}
div#bookshelf {
    -ms-transform: scale(1, 1); /* IE 9 */
    -webkit-transform: scale(1, 1); /* Safari */
    transform: scale(1, 1);
}            </style>
            
                </div>

        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/bookblock.min.js"></script>
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/classie.js"></script>
        <script src="http://adventuresinmachinelearning.com/wp-content/plugins/ebook-store/js/bookshelf.js"></script>
    
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to dive a little deeper I&#8217;d recommend the following inexpensive Udemy video course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/ebook/python-tensorflow-tutorial-ebook/">Python TensorFlow Tutorial – Build a Neural Network eBook</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Stochastic Gradient Descent &#8211; Mini-batch and more</title>
		<link>http://adventuresinmachinelearning.com/stochastic-gradient-descent/</link>
		<comments>http://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments</comments>
		<pubDate>Thu, 30 Mar 2017 05:29:30 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>
		<category><![CDATA[Optimisation]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=161</guid>
		<description><![CDATA[<p>In the neural network tutorial, I introduced the gradient descent algorithm which is used to train the weights in an artificial neural network.  In reality, <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" title="Stochastic Gradient Descent &#8211; Mini-batch and more">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/">Stochastic Gradient Descent &#8211; Mini-batch and more</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural network tutorial</a>, I introduced the gradient descent algorithm which is used to train the weights in an artificial neural network.  In reality, for deep learning and big data tasks standard gradient descent is not often used.  Rather, a variant of gradient descent called <em>stochastic gradient descent </em>and in particular its cousin <em>mini-batch gradient descent</em> is used.  That is the focus of this post.</p>
<h2>Gradient descent review</h2>
<p>The gradient descent optimisation algorithm aims to minimise some cost/loss function based on that function&#8217;s gradient.  Successive iterations are employed to progressively approach either a local or global minimum of the cost function.  The figure below shows an example of gradient descent operating in a single dimension:</p>
<figure id="attachment_74" style="width: 300px" class="wp-caption aligncenter"><img class="size-medium wp-image-74" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent-300x156.jpg" alt="Stochastic gradient descent - simple gradient descent example" width="300" height="156" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent-300x156.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent.jpg 553w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Simple, one-dimensional gradient descent</figcaption></figure>
<p>When training weights in a neural network, normal <em>batch</em> gradient descent usually takes the mean squared error of <em>all</em> the training samples when it is updating the weights of the network:</p>
<p>$$W = W &#8211; \alpha \nabla J(W,b)$$</p>
<p>where $W$ are the weights, $\alpha$ is the learning rate and $\nabla$ is the gradient of the cost function $J(W,b)$ with respect to changes in the weights.  More details can be found in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a>, but in that tutorial the cost function $J$ was defined as:</p>
<p>\begin{align}<br />
J(W,b) &amp;= \frac{1}{m} \sum_{z=0}^m J(W, b, x^{(z)}, y^{(z)})<br />
\end{align}</p>
<p>As can be observed, the overall cost function (and therefore the gradient) depends on the mean cost function calculated on <em>all</em> of the <i>m</i> training samples ($x^{(z)}$ and $y^{(z)}$ refer to each training sample pair).  Is this the best way of doing things?  Batch gradient descent is good because the training progress is nice and smooth &#8211; if you plot the average value of the cost function over the number of iterations / epochs it will look something like this:</p>
<figure id="attachment_140" style="width: 300px" class="wp-caption aligncenter"><img class="size-medium wp-image-140" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Optimised-J-vs-iterations-300x205.png" alt="Stochastic gradient descent - batch gradient example" width="300" height="205" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Optimised-J-vs-iterations-300x205.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Optimised-J-vs-iterations.png 389w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Example batch gradient descent progress</figcaption></figure>
<p>As you can see, the line is mostly smooth and predictable.  However, a problem with batch gradient descent in neural networks is that for every gradient descent update in the weights, you have to cycle through every training sample.  For big data sets i.e. &gt; 50,000 training samples, this can be time prohibitive.  Batch gradient descent also has the following disadvantages:</p>
<ul>
<li>It requires the loading of the whole dataset into memory, which can be problematic for big data sets</li>
<li>Batch gradient descent can&#8217;t be efficiently parallelised (compared to the techniques about to be presented) &#8211; this is because each update in the weight parameters requires a mean calculation of the cost function over <em>all</em> the training samples.</li>
<li>The smooth nature of the reducing cost function tends to ensure that the neural network training will get stuck in local minimums, which makes it less likely that a global minimum of the cost function will be found.</li>
</ul>
<p>Stochastic gradient descent is an algorithm that attempts to address some of these issues.</p>
<h2>Stochastic gradient descent</h2>
<p>Stochastic gradient descent updates the weight parameters after evaluation the cost function <em>after each sample</em>.  That is, rather than summing up the cost function results for all the sample then taking the mean, stochastic gradient descent (or SGD) updates the weights after every training sample is analysed.  Therefore, the updates look like this:</p>
<p>$$W = W &#8211; \alpha \nabla J(W,b, x^{(z)}, y^{(z)})$$</p>
<p>Notice that an update to the weights (and bias) is performed after every sample $z$ in $m$.  This is easily implemented by a minor variation of the batch gradient descent code in Python, by simply shifting the update component into the sample loop (the original train_nn function can be found in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a> and <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank">here</a>):</p>
<pre class="prettyprint">def train_nn_SGD(nn_structure, X, y, iter_num=3000, alpha=0.25, lamb=0.000):
    W, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print('Starting gradient descent for {} iterations'.format(iter_num))
    while cnt &lt; iter_num:
        if cnt%50 == 0:
            print('Iteration {} of {}'.format(cnt, iter_num))
        tri_W, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        for i in range(len(y)):
            delta = {}
            # perform the feed forward pass and return the stored h and z values, 
            # to be used in the gradient descent step
            h, z = feed_forward(X[i, :], W, b)
            # loop from nl-1 to 1 backpropagating the errors
            for l in range(len(nn_structure), 0, -1):
                if l == len(nn_structure):
                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])
                    avg_cost += np.linalg.norm((y[i,:]-h[l]))
                else:
                    if l &gt; 1:
                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])
                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))
                    tri_W[l] = np.dot(delta[l+1][:,np.newaxis],
                                       np.transpose(h[l][:,np.newaxis])) 
                    # trib^(l) = trib^(l) + delta^(l+1)
                    tri_b[l] = delta[l+1]
            # perform the gradient descent step for the weights in each layer
            for l in range(len(nn_structure) - 1, 0, -1):
                W[l] += -alpha * (tri_W[l] + lamb * W[l])
                b[l] += -alpha * (tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return W, b, avg_cost_func</pre>
<p>In the above function, to implement stochastic gradient descent, the following code was simply indented into the sample loop &#8220;for i in range(len(y)):&#8221; (and the averaging over <em>m</em> samples removed):</p>
<pre class="prettyprint">for l in range(len(nn_structure) - 1, 0, -1):
    W[l] += -alpha * (tri_W[l] + lamb * W[l])
    b[l] += -alpha * (tri_b[l])</pre>
<p>In other words, a very easy transition from batch to stochastic gradient descent.  Where does the &#8220;stochastic&#8221; part come in?  The stochastic component is in the selection of the random selection of training sample.  However, if we use the scikit-learn <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank">test_train_split function</a> the random selection has already occurred, so we can simply iterate through each training sample, which has a randomised order.</p>
<h2>Stochastic gradient descent performance</h2>
<p>So how does SGD perform?  Let&#8217;s take a look.  The plot below shows the average cost versus the number of training epochs / iterations for batch gradient descent and SGD on the scikit-learn MNIST dataset.  Note that both of these are operating off the same optimised learning parameters (i.e. learning rate, regularisation parameter) which were determined according to the methods described in <a href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/" target="_blank">this post</a>.</p>
<figure id="attachment_193" style="width: 300px" class="wp-caption aligncenter"><img class="size-medium wp-image-193" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/BGD-vs-SGD-2-300x205.png" alt="Stochastic gradient descent - BGD vs SGD" width="300" height="205" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/BGD-vs-SGD-2-300x205.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/BGD-vs-SGD-2.png 389w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Batch gradient descent versus SGD</figcaption></figure>
<p>Some interesting things can be noted from the above figure.  First, SGD converges much more rapidly than batch gradient descent.  In fact, SGD converges on a minimum <em>J </em>after &lt; 20 iterations.  Secondly, despite what the average cost function plot says, batch gradient descent after 1000 iterations <em>outperforms</em> SGD.  On the MNIST test set, the SGD run has an accuracy of 94% compared to a BGD accuracy of 96%.  Why is that?  Let&#8217;s zoom into the SGD run to have a closer look:</p>
<figure id="attachment_194" style="width: 300px" class="wp-caption aligncenter"><img class="size-medium wp-image-194" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Noisy-SGD-2-300x193.png" alt="Stochastic gradient descent - Noisy SGD" width="300" height="193" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Noisy-SGD-2-300x193.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Noisy-SGD-2.png 414w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Noisy SGD</figcaption></figure>
<p>As you can see in the figure above, SGD is <em>noisy</em>.  That is because it responds to the effects of each and every sample, and the samples themselves will no doubt contain an element of noisiness.  While this can be a benefit in that it can act to &#8220;kick&#8221; the gradient descent out of local minimum values of the cost function, it can also hinder it settling down into a good minimum.  This is why, eventually, batch gradient descent has outperformed SGD after 1000 iterations.  It might be argued that this is a worthwhile pay-off, as the running time of SGD versus BGD is greatly reduced.  However, you might ask &#8211; is there a middle road, a trade-off?</p>
<p>There is, and it is called mini-batch gradient descent.</p>
<h2>Mini-batch gradient descent</h2>
<p>Mini-batch gradient descent is a trade-off between stochastic gradient descent and batch gradient descent.  In mini-batch gradient descent, the cost function (and therefore gradient) is averaged over a small number of samples, from around 10-500.  This is opposed to the SGD batch size of <em>1</em> sample, and the BGD size of <em>all </em>the training samples.  It looks like this:</p>
<p>$$W = W &#8211; \alpha \nabla J(W,b, x^{(z:z+bs)}, y^{(z:z+bs)})$$</p>
<p>Where $bs$ is the mini-batch size and the cost function is:</p>
<p>$$J(W,b, x^{(z:z+bs)}, y^{(z:z+bs)}) = \frac{1}{bs} \sum_{z=0}^{bs} J(W, b, x^{(z)}, y^{(z)})$$</p>
<p>What&#8217;s the benefit of doing it this way?  First, it smooths out some of the noise in SGD, but not all of it, thereby still allowing the &#8220;kick&#8221; out of local minimums of the cost function.  Second, the mini-batch size is still small, thereby keeping the performance benefits of SGD.</p>
<p>To create the mini-batches, we can use the following function:</p>
<pre class="prettyprint">from numpy import random
def get_mini_batches(X, y, batch_size):
    random_idxs = random.choice(len(y), len(y), replace=False)
    X_shuffled = X[random_idxs,:]
    y_shuffled = y[random_idxs]
    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for
                   i in range(0, len(y), batch_size)]
    return mini_batches</pre>
<p>Then our new neural network training algorithm looks like this:</p>
<pre class="prettyprint">def train_nn_MBGD(nn_structure, X, y, bs=100, iter_num=3000, alpha=0.25, lamb=0.000):
    W, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print('Starting gradient descent for {} iterations'.format(iter_num))
    while cnt &lt; iter_num:
        if cnt%1000 == 0:
            print('Iteration {} of {}'.format(cnt, iter_num))
        tri_W, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        mini_batches = get_mini_batches(X, y, bs)
        for mb in mini_batches:
            X_mb = mb[0]
            y_mb = mb[1]
            # pdb.set_trace()
            for i in range(len(y_mb)):
                delta = {}
                # perform the feed forward pass and return the stored h and z values, 
                # to be used in the gradient descent step
                h, z = feed_forward(X_mb[i, :], W, b)
                # loop from nl-1 to 1 backpropagating the errors
                for l in range(len(nn_structure), 0, -1):
                    if l == len(nn_structure):
                        delta[l] = calculate_out_layer_delta(y_mb[i,:], h[l], z[l])
                        avg_cost += np.linalg.norm((y_mb[i,:]-h[l]))
                    else:
                        if l &gt; 1:
                            delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])
                        # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))
                        tri_W[l] += np.dot(delta[l+1][:,np.newaxis], 
                                          np.transpose(h[l][:,np.newaxis])) 
                        # trib^(l) = trib^(l) + delta^(l+1)
                        tri_b[l] += delta[l+1]
            # perform the gradient descent step for the weights in each layer
            for l in range(len(nn_structure) - 1, 0, -1):
                W[l] += -alpha * (1.0/bs * tri_W[l] + lamb * W[l])
                b[l] += -alpha * (1.0/bs * tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return W, b, avg_cost_func</pre>
<p>Let&#8217;s see how it performs with a min-batch size of 100 samples:</p>
<figure id="attachment_195" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-195 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/MBGD-vs-the-rest-300x205.png" alt="Stochastic gradient descent - Mini-batch vs the rest" width="300" height="205" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/MBGD-vs-the-rest-300x205.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/MBGD-vs-the-rest.png 389w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Mini-batch gradient descent versus the rest</figcaption></figure>
<p>As can be observed in the figure above, mini-batch gradient descent appears be the superior method of gradient descent to be used in neural networks training.  The jagged decline in the average cost function is evidence that mini-batch gradient descent is &#8220;kicking&#8221; the cost function out of local minimum values to reach better, perhaps even the best, minimum.  However, it is still able to find a good minimum and stick to it.  This is confirmed in the test data &#8211; the mini-batch method achieves an accuracy of 98% compared to the next best, batch gradient descent, which has an accuracy of 96%.  The great thing is &#8211; it gets to these levels of accuracy after only 150 iterations or so.</p>
<p>One final benefit of mini-batch gradient descent is that it can be performed in a distributed manner.  That is, each mini-batch can be computed in parallel by &#8220;workers&#8221; across multiple servers, CPUs and GPUs to achieve significant improvements in training speeds.  There are multiple algorithms and architectures to perform this parallel operation, but that is a topic for another day.  In the mean-time, enjoy trying out mini-batch gradient descent in your neural networks.</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/">Stochastic Gradient Descent &#8211; Mini-batch and more</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/stochastic-gradient-descent/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Improve your neural networks &#8211; Part 1 [TIPS AND TRICKS]</title>
		<link>http://adventuresinmachinelearning.com/improve-neural-networks-part-1/</link>
		<comments>http://adventuresinmachinelearning.com/improve-neural-networks-part-1/#comments</comments>
		<pubDate>Tue, 28 Mar 2017 08:06:45 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=114</guid>
		<description><![CDATA[<p>In the last post, I presented a comprehensive tutorial of how to build and understand neural networks.  At the end of that tutorial, we developed <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/" title="Improve your neural networks &#8211; Part 1 [TIPS AND TRICKS]">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/">Improve your neural networks &#8211; Part 1 [TIPS AND TRICKS]</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">last post</a>, I presented a comprehensive tutorial of how to build and understand neural networks.  At the end of that tutorial, we developed a network to classify digits in the MNIST dataset.  However, the accuracy was well below the state-of-the-art results on the dataset.  This post will show some techniques on how to improve the accuracy of your neural networks, again using the <a href="http://scikit-learn.org/" target="_blank">scikit learn</a> MNIST dataset.</p>
<p>When we are thinking about &#8220;improving&#8221; the performance of a neural network, we are generally referring to two things:</p>
<ol>
<li>Improve the accuracy</li>
<li>Speed up the training process (while still maintaining the accuracy)</li>
</ol>
<p>(1) and (2) can play off against each other.  This is because, in order to improve the accuracy, we often need to train our network with more data and more iterations.  This slows down the training however, and makes it more expensive.  The &#8220;tips and tricks&#8221; in this post will address both of these issues.  All code will be in Python.</p>
<h2>1 Regularisation and over-fitting in neural networks</h2>
<h3>1.1 Over-fitting</h3>
<p>As was presented in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a>, we always split our available data into at least a training and a test set.  We do this because we want the neural network to <em>generalise </em>well.  This means that we want our network to perform well on data that it hasn&#8217;t &#8220;seen&#8221; before during training.  If we just throw all the data we have at the network during training, we will have no idea if it has <em>over-fitted</em> on the training data.  If it has, then it will perform badly on new data that it hasn&#8217;t been trained on.  We need another data set, the test set, to check and make sure our network is <em>generalising</em> well.</p>
<p>What happens when a machine learning model over-fits during training?  A simple way of thinking about it is that it becomes <em>over-complicated</em> given the data it has to train on.  This is often best illustrated using a linear regression example, see the image below from Wikipedia:</p>
<figure style="width: 256px" class="wp-caption aligncenter"><a title="By Ghiles (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AOverfitted_Data.png"><img src="https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png" alt="Overfitted Data" width="256" height="256" /></a><figcaption class="wp-caption-text">By Ghiles (Own work) [<a href="http://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>], <a href="https://commons.wikimedia.org/wiki/File%3AOverfitted_Data.png">via Wikimedia Commons</a></figcaption></figure>The black dots are the training data.  As can be observed, the data-points pretty obviously follow a linear trend with increasing <em>x</em>, but there is a bit of noise in the relationship i.e. not all the data points sit on the black linear trend line.  The blue line is a high order polynomial model, which has <em>perfectly</em> fit the training data.  However, intuitively we would say that the black linear line is the better model.  Why is that?  It is because the blue model is <em>unjustifiably</em> complicated given the data &#8211; it is so complicated that it has &#8220;chased after&#8221; the random noise of the data.  We can see that small changes in the horizontal axis can lead to huge changes in the vertical axis.  This is not a good model and will therefore perform badly on any test set that it is tried on.</p>
<p>Over-fitting is something we also have to be wary of in neural networks.  A good way of avoiding this is to use something called <em>regularisation.  </em></p>
<h3>1.2 Regularisation</h3>
<p>Regularisation involves making sure that the weights in our neural network do not grow too large during the training process.  During training, our neural networks will converge on local minimum values of the cost function.  There will be many of these local minima, and many of them will have roughly the same cost function &#8211; in other words, there are many ways to skin the cat.  Some of these local minimum values will have large weights connecting the nodes and layers, others will have smaller values.  We want to force our neural network to pick weights which are smaller rather than larger.</p>
<p>This makes our network less complex &#8211; but why is that?  Consider the previous section, where we discussed that an over-fitted model has large changes in predictions compared to small changes in input.  In other words, if we have a little bit of noise in our data, an over-fitted model will react strongly to that noise.  The analogous situation in neural networks is when we have large weights &#8211; such a network is more likely to react strongly to noise.  This is because large weights will amplify small variations in the input which could be solely due to noise.  Therefore, we want to adjust the cost function to try to make the training drive the magnitude of the weights down, while still producing good predictions.</p>
<p>The old cost function was (see the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a> for an explanation of the notation used):</p>
<p>$$J(w,b) = \frac{1}{m} \sum_{z=0}^m \frac{1}{2} \parallel y^z &#8211; h^{(n_l)}(x^z) \parallel ^2$$</p>
<p>In this cost function, we are trying to minimize the mean squared error (MSE) of the prediction compared to the training data.  Now we want to vary the cost function to:</p>
<p>$$J(w,b) = \frac{1}{m} \sum_{z=0}^m \frac{1}{2} \parallel y^z &#8211; h^{(n_l)}(x^z) \parallel ^2 + \frac {\lambda}{2}\sum_{all} \left(W_{ij}^{(l)}\right)^2$$</p>
<p>Notice the addition of the last term, which is a summation of all the weight values in each layer, multiplied by the $\lambda$ constant divided by 2 (the division by 2 is a little trick to clean things up when we take the derivative).  This $\lambda$ value is usually quite small.  This shows that any increase in the weights must be balanced by an associated decrease in the mean squared error term in the cost function.  In other words, large weights will be penalised in this new cost function if they don&#8217;t do much to improve the MSE.</p>
<p>To incorporate this new component into the training of our neural network, we need to take the partial derivative.  Working this through gives a new gradient descent step equation.  The old equation:</p>
<p>$$W^{(l)} = W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} \right]$$</p>
<p>is now replaced by the new:</p>
<p>$$W^{(l)} = W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} + \lambda W^{(l)} \right]$$</p>
<h3>1.3 Regularisation in the training code</h3>
<p>The gradient descent weight updating line in the code of the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural network tutorial</a> can simply be updated to the following, to incorporate regularisation into the Python code:</p>
<pre class="prettyprint">W[l] += -alpha * (1.0/m * tri_W[l] + lamb * W[l])</pre>
<p>Where &#8220;lamb&#8221; is the regularisation parameter, which must be chosen.  If you recall from the tutorial, without regularisation the prediction accuracy on the scikit learn sample MNIST data set was only 86%.  This was with a learning rate ($\alpha$) of 0.25 and 3,000 training iterations.  Using the same parameters, and a regularisation parameter ($\lambda$) equal to 0.001, <strong>we now get a prediction accuracy of 95%!</strong>  That is a 9% increase in prediction accuracy by altering a single line of code and adding a new parameter.</p>
<p>Therefore, it is safe to say that in our previous example without regularisation we were over-fitting the data, despite the mean squared error of both versions being practically the same after 3,000 iterations.</p>
<h2>2. Selecting the right parameters</h2>
<p>In our training code for neural networks, we have a number of free parameters.  These parameters are the learning rate $\alpha$, the number and size of the hidden layers and now the regularisation parameter $\lambda$.  We also have to make a choice about what activation function to use.  All of these selections will affect the performance of the neural network, and therefore must be selected carefully.  How do we do this?  Usually by some sort of brute force search method, where we vary the parameters and try to land on those parameters which give us the best predictive performance.</p>
<p>Do we still use the test set to determine the predictive accuracy by which we tune our parameters?  Is it really a test set in that case?  Aren&#8217;t we then using all our data to make the network better, rather than leaving some aside to ensure we aren&#8217;t over-fitting?  Yes, we are.  We need to introduce a new set of the training data called the <em>validation</em> set.</p>
<p>To create a validation set, we can use the scikit learn function called <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank">train_test_split</a>.  Usually, we want to keep the majority of data for training, say 60%.  The remaining data we can split into a test set and a validation set.  The code below shows how to do this:</p>
<pre class="prettyprint">from sklearn.model_selection import train_test_split
X_train, X_holdover, y_train, y_holdover = train_test_split(X, y, test_size=0.4)
X_valid, X_test, y_valid, y_test = train_test_split(X_holdover, y_holdover, test_size=0.5)</pre>
<p>Now we have training, validation and test data sets, and we&#8217;re ready to perform parameter selections.</p>
<h3>2.1 Brute-force search example</h3>
<p>Often model parameter selection is performed using the brute-force search method.  This method involves cycling through likely values for the parameters in different combinations and assessing some measure of accuracy / fitness for each combination on the validation set.  We then select the best set of parameter values and see how they go on the test set.  The brute-force search method is easy to implement but can take a long time to run, given the combinatorial explosion of scenarios to test when there are many parameters.</p>
<p>In the example below, we will be using the brute-force search method to find the best parameters for a three-layer neural network to classify the scikit learn MNIST dataset.  Note: this data set isn&#8217;t the &#8220;real&#8221; MNIST dataset that is used often as a benchmark (it&#8217;s a cut down version), but it is good enough for our purposes.  This example will be using some of the same functions as in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural network tutorial</a>.  The parameters that we are going to test are:</p>
<ol>
<li>The number of hidden layers</li>
<li>The learning rate</li>
<li>The regularisation constant</li>
</ol>
<p>Let&#8217;s first setup some lists for the parameter cycling:</p>
<pre class="prettyprint">hidden_size = [10, 25, 50, 60]
alpha = [0.05, 0.1, 0.25, 0.5]
lamb = [0.0001, 0.0005, 0.001, 0.01]</pre>
<p>It is now a simple matter of cycling through each parameter combination, training the neural network, and assessing the accuracy.  The code below shows how this can be done, assessing the accuracy of the trained neural network after 3,000 iterations.  Careful, this will take a while to run:</p>
<pre class="prettyprint">from sklearn.metrics import accuracy_score
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
digits = load_digits()
X_scale = StandardScaler()
X = X_scale.fit_transform(digits.data)
y = digits.target

# setup the parameter selection function
def select_parameters(hidden_size, alpha, lamb, X, y):
    X_train, X_holdover, y_train, y_holdover = train_test_split(X, y, test_size=0.4)
    X_valid, X_test, y_valid, y_test = train_test_split(X_holdover, y_holdover, 
                                       test_size=0.5)
    # convert the targets (scalars) to vectors
    yv_train = convert_y_to_vect(y_train)
    yv_valid = convert_y_to_vect(y_valid)
    results = np.zeros((len(hidden_size)*len(alpha)*len(lamb), 4))
    cnt = 0
    for hs in hidden_size:
        for al in alpha:
            for l in lamb:
                nn_structure = [64, hs, 10]
                W, b, avg_cost = train_nn(nn_structure, X_train, yv_train, 
                                    iter_num=3000, alpha=al, lamb=l)
                y_pred = predict_y(W, b, X_valid, 3)
                accuracy = accuracy_score(y_valid, y_pred) * 100
                print("Accuracy is {}% for {}, {}, {}".format(accuracy, hs, al, l))
                # store the data
                results[cnt, 0] = accuracy
                results[cnt, 1] = hs
                results[cnt, 2] = al
                results[cnt, 3] = l
                cnt += 1
    # get the index of the best accuracy
    best_idx = np.argmax(results[:, 0])
    return results, results[best_idx, :]
select_parameters(hidden_size, alpha, lamb, X, y)</pre>
<p>Note that the above code uses functions developed in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a>.  These functions can be found on this site&#8217;s GitHub <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank">repository</a>.   After running this code, we find that the best accuracy (98.6%) is achieved on the validation set with 50 hidden layers, a learning rate of 0.5 and a regularisation parameter of 0.001.</p>
<p>Using these parameters on the test set now gives us an accuracy of 96%.  Therefore, using the brute-force search method and a validation set, along with regularisation, improved our original naïve results in the <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank">neural networks tutorial</a> from 86% to 96%!  A big improvement, clearly worth the extra time taken to improve our model.</p>
<p>In the next part of this series we&#8217;ll look at ways of speeding up the training.</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/">Improve your neural networks &#8211; Part 1 [TIPS AND TRICKS]</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/improve-neural-networks-part-1/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Neural Networks Tutorial &#8211; A Pathway to Deep Learning</title>
		<link>http://adventuresinmachinelearning.com/neural-networks-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/neural-networks-tutorial/#comments</comments>
		<pubDate>Sat, 18 Mar 2017 03:05:37 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>
		<category><![CDATA[Gradient descent]]></category>
		<category><![CDATA[Python]]></category>
		<category><![CDATA[Tutorials]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=20</guid>
		<description><![CDATA[<p>Chances are, if you are searching for a tutorial on artificial neural networks (ANN) you already have some idea of what they are, and what <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" title="Neural Networks Tutorial &#8211; A Pathway to Deep Learning">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">Neural Networks Tutorial &#8211; A Pathway to Deep Learning</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Chances are, if you are searching for a tutorial on artificial neural networks (ANN) you already have some idea of what they are, and what they are capable of doing.  But did you know that neural networks are the foundation of the new and exciting field of deep learning?  Deep learning is the field of machine learning that is making many state-of-the-art advancements, from beating players at <a href="http://www.sciencemag.org/news/2016/01/huge-leap-forward-computer-mimics-human-brain-beats-professional-game-go" target="_blank" rel="noopener noreferrer">Go</a> and <a href="http://www.sciencemag.org/news/2017/03/artificial-intelligence-goes-deep-beat-humans-poker" target="_blank" rel="noopener noreferrer">Poker</a> (<a href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/" target="_blank" rel="noopener">reinforcement learning</a>), to speeding up <a href="http://www.nanalyze.com/2016/01/4-companies-using-deep-learning-for-drug-discovery/" target="_blank" rel="noopener noreferrer">drug discovery</a> and <a href="http://spectrum.ieee.org/cars-that-think/transportation/self-driving/driveai-brings-deep-learning-to-selfdriving-cars" target="_blank" rel="noopener noreferrer">assisting self-driving cars</a>.  If these types of cutting edge applications excite you like they excite me, then you will be interesting in learning as much as you can about deep learning.  However, that requires you to know quite a bit about how neural networks work.  This tutorial article is designed to help you get up to speed in neural networks as quickly as possible.</p>
<p>In this tutorial I&#8217;ll be presenting some concepts, code and maths that will enable you to build <em>and understand</em> a simple neural network.  Some tutorials focus only on the code and skip the maths &#8211; but this impedes understanding. I&#8217;ll take things as slowly as possible, but it might help to brush up on your <a href="https://www.khanacademy.org/math/precalculus/precalc-matrices" target="_blank" rel="noopener noreferrer">matrices</a> and <a href="https://www.khanacademy.org/math/differential-calculus" target="_blank" rel="noopener noreferrer">differentiation</a> if you need to. The code will be in Python, so it will be beneficial if you have a basic understanding of how Python works.  You&#8217;ll pretty much get away with knowing about Python functions, loops and the basics of the <a href="http://www.numpy.org/">numpy</a> library.  By the end of this neural networks tutorial you&#8217;ll be able to build an ANN in Python that will correctly classify handwritten digits in images with a fair degree of accuracy.</p>
<p>Once you&#8217;re done with this tutorial, you can dive a little deeper with the following posts:</p>
<p><a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/">Python TensorFlow Tutorial – Build a Neural Network</a><br />
<a href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/">Improve your neural networks – Part 1 [TIPS AND TRICKS]</a><br />
<a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/">Stochastic Gradient Descent – Mini-batch and more</a></p>
<p>All of the relevant code in this tutorial can be found <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you like video courses, I&#8217;d recommend the following inexpensive Udemy course on neural networks: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1151632&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeeplearning%2F" target="new">Deep Learning A-Z: Hands-On Artificial Neural Networks</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1151632&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>Here&#8217;s an outline of the tutorial, with links, so you can easily navigate to the parts you want:</p>
<p><a href="#what-are-anns">1 What are artificial neural networks?</a><br />
<a href="#structure-ann">2 The structure of an ANN</a><br />
<a href="#the-artificial-neuron">2.1 The artificial neuron</a><br />
<a href="#nodes">2.2 Nodes</a><br />
<a href="#the-bias">2.3 The bias</a><br />
<a href="#putting-together-the-structure">2.4 Putting together the structure</a><br />
<a href="#the-notation">2.5 The notation</a><br />
<strong><a href="#the-feed-forward-pass">3 The feed-forward pass</a></strong><br />
<a href="#the-feed-forward-pass">3.1 A feed-forward example</a><br />
<a href="#first-attempt-feed-forward">3.2 Our first attempt at a feed-forward function</a><br />
<a href="#more-efficient-implementation">3.3 A more efficient implementation</a><br />
<a href="#vectorisation">3.4 Vectorisation in neural networks</a><br />
<a href="#matrix-mult">3.5 Matrix multiplication</a><br />
<strong><a href="#gradient-desc-opt">4 Gradient descent and optimisation</a></strong><br />
<a href="#simple-example">4.1 A simple example in code</a><br />
<a href="#the-cost-function">4.2 The cost function</a><br />
<a href="#gradient-descent-in-nn">4.3 Gradient descent in neural networks</a><br />
<a href="#two-dimensional">4.4 A two dimensional gradient descent example</a><br />
<a href="#backprop-in-depth">4.5 Backpropagation in depth</a><br />
<a href="#prop-in-hidden-layers">4.6 Propagating into the hidden layers</a><br />
<a href="#vector-backprop">4.7 Vectorisation of backpropagation</a><br />
<a href="#imp-gradient-desc">4.8 Implementing the gradient descent step</a><br />
<a href="#final-gradient-desc-algo">4.9 The final gradient descent algorithm</a><br />
<strong><a href="#implementing-nn">5 Implementing the neural network in Python</a></strong><br />
<a href="#scaling-data">5.1 Scaling data</a><br />
<a href="#test-and-train">5.2 Creating test and training datasets</a><br />
<a href="#setting-up-output">5.3 Setting up the output layer</a><br />
<a href="#creating-nn">5.4 Creating the neural network</a><br />
<a href="#creating-nn">5.5 Assessing the accuracy of the trained model</a></p>
<h2 id="what-are-anns">1 What are artificial neural networks?</h2>
<p>Artificial neural networks (ANNs) are software implementations of the neuronal structure of our brains.  We don&#8217;t need to talk about the complex biology of our brain structures, but suffice to say, the brain contains <em>neurons</em> which are kind of like organic switches.  These can change their output state depending on the strength of their electrical or chemical input.  The neural network in a person&#8217;s brain is a hugely interconnected network of neurons, where the output of any given neuron may be the input to thousands of other neurons.  Learning occurs by repeatedly activating certain neural connections over others, and this reinforces those connections.  This makes them more likely to produce a desired outcome given a specified input.  This learning involves <em>feedback</em> &#8211; when the desired outcome occurs, the neural connections causing that outcome become strengthened.</p>
<p><em>Artificial</em> neural networks attempt to simplify and mimic this brain behaviour.  They can be trained in a <em>supervised</em> or <em>unsupervised</em> manner.  In a <em>supervised</em> ANN, the network is trained by providing matched input and output data samples, with the intention of getting the ANN to provide a desired output for a given input.  An example is an e-mail spam filter &#8211; the input training data could be the count of various words in the body of the e-mail, and the output training data would be a classification of whether the e-mail was truly spam or not.  If many examples of e-mails are passed through the neural network this allows the network to <em>learn </em>what input data makes it likely that an e-mail is spam or not.  This learning takes place be adjusting the <em>weights</em> of the ANN connections, but this will be discussed further in the next section.</p>
<p><em>Unsupervised</em> learning in an ANN is an attempt to get the ANN to &#8220;understand&#8221; the structure of the provided input data &#8220;on its own&#8221;.  This type of ANN will not be discussed in this post.</p>
<h2 id="structure-ann">2 The structure of an ANN</h2>
<h3 id="the-artificial-neuron">2.1 The artificial neuron</h3>
<p>The biological neuron is simulated in an ANN by an <em>activation function</em>. In classification tasks (e.g. identifying spam e-mails) this activation function has to have a &#8220;switch on&#8221; characteristic &#8211; in other words, once the input is greater than a certain value, the output should change state i.e. from 0 to 1, from -1 to 1 or from 0 to &gt;0. This simulates the &#8220;turning on&#8221; of a biological neuron. A common activation function that is used is the sigmoid function:</p>
<p>\begin{equation*}<br />
f(z) = \frac{1}{1+exp(-z)}<br />
\end{equation*}</p>
<p>Which looks like this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import matplotlib.pylab as plt
import numpy as np
x = np.arange(-8, 8, 0.1)
f = 1 / (1 + np.exp(-x))
plt.plot(x, f)
plt.xlabel(&#039;x&#039;)
plt.ylabel(&#039;f(x)&#039;)
plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<p><img class="size-medium wp-image-40 aligncenter" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/sigmoid-300x210.png" alt="" width="300" height="210" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/sigmoid-300x210.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/sigmoid.png 538w" sizes="(max-width: 300px) 100vw, 300px" /></p>
<p>As can be seen in the figure above, the function is &#8220;activated&#8221; i.e. it moves from 0 to 1 when the input <em>x</em> is greater than a certain value. The sigmoid function isn&#8217;t a step function however, the edge is &#8220;soft&#8221;, and the output doesn&#8217;t change instantaneously. This means that there is a derivative of the function and this is important for the training algorithm which is discussed more in <a href="#gradient-desc-opt">Section 4</a>.</p>
<h3 id="nodes">2.2 Nodes</h3>
<p>As mentioned previously, biological neurons are connected hierarchical networks, with the outputs of some neurons being the inputs to others. We can represent these networks as connected layers of <em>nodes. </em>Each node takes multiple weighted inputs, applies the <em>activation function</em> to the summation of these inputs, and in doing so generates an output. I&#8217;ll break this down further, but to help things along, consider the diagram below:</p>
<figure id="attachment_49" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-49 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Node-diagram-300x125.jpg" alt="" width="300" height="125" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Node-diagram-300x125.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Node-diagram.jpg 481w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 2. Node with inputs</figcaption></figure>
<p>The circle in the image above represents the node. The node is the &#8220;seat&#8221; of the activation function, and takes the weighted inputs, sums them, then inputs them to the activation function. The output of the activation function is shown as <em>h</em> in the above diagram. Note: a <em>node</em> as I have shown above is also called a <em>perceptron</em> in some literature.</p>
<p>What about this &#8220;weight&#8221; idea that has been mentioned? The weights are real valued numbers (i.e. not binary 1s or 0s), which are multiplied by the inputs and then summed up in the node. So, in other words, the weighted input to the node above would be:</p>
<p>\begin{equation*}<br />
x_1w_1 + x_2w_2 + x_3w_3 + b<br />
\end{equation*}</p>
<p>Here the $w_i$ values are weights (ignore the $b$ for the moment).  What are these weights all about?  Well, they are the variables that are changed during the learning process, and, along with the input, determine the output of the node.  The $b$ is the weight of the +1 <em>bias</em> element &#8211; the inclusion of this bias enhances the flexibility of the node, which is best demonstrated in an example.</p>
<h3 id="the-bias">2.3 The bias</h3>
<p>Let&#8217;s take an extremely simple node, with only one input and one output:</p>
<p>&nbsp;</p>
<figure id="attachment_45" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-45 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node-300x51.jpg" alt="" width="300" height="51" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node-300x51.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node.jpg 481w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 2. Simple node</figcaption></figure>
<p>The input to the activation function of the node in this case is simply $x_1w_1$.  What does changing $w_1$ do in this simple network?</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">w1 = 0.5
w2 = 1.0
w3 = 2.0
l1 = &#039;w = 0.5&#039;
l2 = &#039;w = 1.0&#039;
l3 = &#039;w = 2.0&#039;
for w, l in [(w1, l1), (w2, l2), (w3, l3)]:
    f = 1 / (1 + np.exp(-x*w))
    plt.plot(x, f, label=l)
plt.xlabel(&#039;x&#039;)
plt.ylabel(&#039;h_w(x)&#039;)
plt.legend(loc=2)
plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<figure id="attachment_54" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-54 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Weight-adjustment-example-300x210.png" alt="" width="300" height="210" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Weight-adjustment-example-300x210.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Weight-adjustment-example.png 538w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 4. Effect of adjusting weights</figcaption></figure>
<p>Here we can see that changing the weight changes the slope of the output of the sigmoid activation function, which is obviously useful if we want to model different strengths of relationships between the input and output variables.  However, what if we only want the output to change when x is greater than 1?  This is where the bias comes in &#8211; let&#8217;s consider the same network with a bias input:</p>
<figure id="attachment_56" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-56 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node-with-bias-300x126.png" alt="" width="300" height="126" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node-with-bias-300x126.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-node-with-bias.png 481w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 5. Effect of bias</figcaption></figure>
<p>&nbsp;</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">w = 5.0
b1 = -8.0
b2 = 0.0
b3 = 8.0
l1 = &#039;b = -8.0&#039;
l2 = &#039;b = 0.0&#039;
l3 = &#039;b = 8.0&#039;
for b, l in [(b1, l1), (b2, l2), (b3, l3)]:
    f = 1 / (1 + np.exp(-(x*w+b)))
    plt.plot(x, f, label=l)
plt.xlabel(&#039;x&#039;)
plt.ylabel(&#039;h_wb(x)&#039;)
plt.legend(loc=2)
plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<figure id="attachment_57" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-57 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Bias-adjustment-example-300x210.png" alt="" width="300" height="210" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Bias-adjustment-example-300x210.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Bias-adjustment-example.png 538w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 6. Effect of bias adjusments</figcaption></figure>
<p>In this case, the $w_1$ has been increased to simulate a more defined &#8220;turn on&#8221; function.  As you can see, by varying the bias &#8220;weight&#8221; $b$, you can change when the node activates.  Therefore, by adding a bias term, you can make the node simulate a generic <strong>if</strong> function, i.e. <em>if (x &gt; z) then 1 else 0</em>.  Without a bias term, you are unable to vary the <em>z</em> in that if statement, it will be always stuck around 0.  This is obviously very useful if you are trying to simulate conditional relationships.</p>
<h3 id="putting-together-the-structure">2.4 Putting together the structure</h3>
<p>Hopefully the previous explanations have given you a good overview of how a given node/neuron/perceptron in a neural network operates.  However, as you are probably aware, there are many such interconnected nodes in a fully fledged neural network.  These structures can come in a myriad of different forms, but the most common simple neural network structure consists of an <em>input layer</em>, a <em>hidden layer</em> and an <em>output layer</em>.  An example of such a structure can be seen below:</p>
<figure id="attachment_60" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-60 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network-300x158.png" alt="" width="300" height="158" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network-300x158.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network.png 591w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 10. Three layer neural network</figcaption></figure>
<p>The three layers of the network can be seen in the above figure &#8211; Layer 1 represents the <strong><em>input layer</em></strong>, where the external input data enters the network. Layer 2 is called the <strong><em>hidden layer</em></strong> as this layer is not part of the input or output. Note: neural networks can have many hidden layers, but in this case for simplicity I have just included one. Finally, Layer 3 is the <strong><em>output layer</em></strong>. You can observe the many connections between the layers, in particular between Layer 1 (L1) and Layer 2 (L2). As can be seen, each node in L1 has a connection to all the nodes in L2. Likewise for the nodes in L2 to the single output node L3. Each of these connections will have an associated weight.</p>
<h3 id="the-notation">2.5 The notation</h3>
<p>The maths below requires some fairly precise notation so that we know what we are talking about.  The notation I am using here is similar to that used in the Stanford deep learning tutorial.  In the upcoming equations, each of these weights are identified with the following notation: ${w_{ij}}^{(l)}$. $i$ refers to the node number of the connection in layer $l+1$ and $j$ refers to the node number of the connection in layer $l$. Take special note of this order. So, for the connection between node 1 in layer 1 and node 2 in layer 2, the weight notation would be ${w_{21}}^{(1)}$. This notation may seem a bit odd, as you would expect the *i* and *j* to refer the node numbers in layers $l$ and $l+1$ respectively (i.e. in the direction of input to output), rather than the opposite. However, this notation makes more sense when you add the bias.</p>
<p>As you can observe in the figure above &#8211; the (+1) bias is connected to each of the nodes in the subsequent layer. So the bias in layer 1 is connected to the all the nodes in layer two. Because the bias is not a true node with an activation function, it has no inputs (it always outputs the value +1). The notation of the bias weight is ${b_i}^{(l)}$, where *i* is the node number in the layer $l+1$ &#8211; the same as used for the normal weight notation ${w_{21}}^{(1)}$. So, the weight on the connection between the bias in layer 1 and the second node in layer 2 is given by ${b_2}^{(1)}$.</p>
<p>Remember, these values &#8211; ${w_{ji}}^{(1)}$ and ${b_i}^{(l)}$ &#8211; all need to be calculated in the training phase of the ANN.</p>
<p>Finally, the node output notation is ${h_j}^{(l)}$, where $j$ denotes the node number in layer $l$ of the network. As can be observed in the three layer network above, the output of node 2 in layer 2 has the notation of ${h_2}^{(2)}$.</p>
<p>Now that we have the notation all sorted out, it is now time to look at how you calculate the output of the network when the input and the weights are known. The process of calculating the output of the neural network given these values is called the <em>feed-forward</em> pass or process.</p>
<h2 id="the-feed-forward-pass">3 The feed-forward pass</h2>
<p>To demonstrate how to calculate the output from the input in neural networks, let&#8217;s start with the specific case of the three layer neural network that was presented above. Below it is presented in equation form, then it will be demonstrated with a concrete example and some Python code:</p>
<p>\begin{align}<br />
h_1^{(2)} &amp;= f(w_{11}^{(1)}x_1 + w_{12}^{(1)} x_2 + w_{13}^{(1)} x_3 + b_1^{(1)}) \\<br />
h_2^{(2)} &amp;= f(w_{21}^{(1)}x_1 + w_{22}^{(1)} x_2 + w_{23}^{(1)} x_3 + b_2^{(1)}) \\<br />
h_3^{(2)} &amp;= f(w_{31}^{(1)}x_1 + w_{32}^{(1)} x_2 + w_{33}^{(1)} x_3 + b_3^{(1)}) \\<br />
h_{W,b}(x) &amp;= h_1^{(3)} = f(w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)} h_2^{(2)} + w_{13}^{(2)} h_3^{(2)} + b_1^{(2)})<br />
\end{align}</p>
<p>In the equation above $f(\bullet)$ refers to the node activation function, in this case the sigmoid function. The first line, ${h_1}^{(2)}$ is the output of the first node in the second layer, and its inputs are $w_{11}^{(1)}x_1$, $w_{12}^{(1)} x_2$, $w_{13}^{(1)}x_3$ and $b_1^{(1)}$. These inputs can be traced in the three-layer connection diagram above. They are simply summed and then passed through the activation function to calculate the output of the first node. Likewise, for the other two nodes in the second layer.</p>
<p>The final line is the output of the only node in the third and final layer, which is ultimate output of the neural network. As can be observed, rather than taking the weighted input variables ($x_1, x_2, x_3$), the final node takes as input the weighted output of the nodes of the second layer ($h_{1}^{(2)}$, $h_{2}^{(2)}$, $h_{3}^{(2)}$), plus the weighted bias. Therefore, you can see in equation form the hierarchical nature of artificial neural networks.</p>
<h3 id="a-feed-forward-example">3.1 A feed-forward example</h3>
<p>Now, let&#8217;s do a simple first example of the output of this neural network in Python. First things first, notice that the weights between layer 1 and 2 ($w_{11}^{(1)}, w_{12}^{(1)}, \dots$) are ideally suited to matrix representation? Observe:</p>
<p>\begin{equation}<br />
W^{(1)} =<br />
\begin{pmatrix}<br />
w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)} \\<br />
w_{21}^{(1)} &amp; w_{22}^{(1)} &amp; w_{23}^{(1)} \\<br />
w_{31}^{(1)} &amp; w_{32}^{(1)} &amp; w_{33}^{(1)} \\<br />
\end{pmatrix}<br />
\end{equation}</p>
<p>This matrix can be easily represented using numpy arrays:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import numpy as np
w1 = np.array([[0.2, 0.2, 0.2], [0.4, 0.4, 0.4], [0.6, 0.6, 0.6]])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here I have just filled up the layer 1 weight array with some example weights. We can do the same for the layer 2 weight array:</p>
<p>\begin{equation}<br />
W^{(2)} =<br />
\begin{pmatrix}<br />
w_{11}^{(2)} &amp; w_{12}^{(2)} &amp; w_{13}^{(2)}<br />
\end{pmatrix}<br />
\end{equation}</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">w2 = np.zeros((1, 3))
w2[0,:] = np.array([0.5, 0.5, 0.5])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can also setup some dummy values in the layer 1 bias weight array/vector, and the layer 2 bias weight (which is only a single value in this neural network structure &#8211; i.e. a scalar):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">b1 = np.array([0.8, 0.8, 0.8])
b2 = np.array([0.2])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Finally, before we write the main program to calculate the output from the neural network, it&#8217;s handy to setup a separate Python function for the activation function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def f(x):
    return 1 / (1 + np.exp(-x))</code></pre> <div class="code-embed-infos"> </div> </div>
<h3 id="first-attempt-feed-forward">3.2 Our first attempt at a feed-forward function</h3>
<p>Below is a simple way of calculating the output of the neural network, using nested loops in python.  We&#8217;ll look at more efficient ways of calculating the output shortly.<!--?prettify linenums=true?--></p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def simple_looped_nn_calc(n_layers, x, w, b):
    for l in range(n_layers-1):
        #Setup the input array which the weights will be multiplied by for each layer
        #If it&#039;s the first layer, the input array will be the x input vector
        #If it&#039;s not the first layer, the input to the next layer will be the 
        #output of the previous layer
        if l == 0:
            node_in = x
        else:
            node_in = h
        #Setup the output array for the nodes in layer l + 1
        h = np.zeros((w[l].shape[0],))
        #loop through the rows of the weight array
        for i in range(w[l].shape[0]):
            #setup the sum inside the activation function
            f_sum = 0
            #loop through the columns of the weight array
            for j in range(w[l].shape[1]):
                f_sum += w[l][i][j] * node_in[j]
            #add the bias
            f_sum += b[l][i]
            #finally use the activation function to calculate the
            #i-th output i.e. h1, h2, h3
            h[i] = f(f_sum)
    return h</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function takes as input the number of layers in the neural network, the x input array/vector, then Python tuples or lists of the weights and bias weights of the network, with each element in the tuple/list representing a layer $l$ in the network.  In other words, the inputs are setup in the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">w = [w1, w2]
b = [b1, b2]
#a dummy x input vector
x = [1.5, 2.0, 3.0]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The function first checks what the input is to the layer of nodes/weights being considered. If we are looking at the first layer, the input to the second layer nodes is the input vector $x$ multiplied by the relevant weights. After the first layer though, the inputs to subsequent layers are the output of the previous layers. Finally, there is a nested loop through the relevant $i$ and $j$ values of the weight vectors and the bias. The function uses the dimensions of the weights for each layer to figure out the number of nodes and therefore the structure of the network.</p>
<p>Calling the function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">simple_looped_nn_calc(3, x, w, b)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>gives the output of 0.8354.  We can confirm this results by manually performing the calculations in the original equations:</p>
<p>\begin{align}<br />
h_1^{(2)} &amp;= f(0.2*1.5 + 0.2*2.0 + 0.2*3.0 + 0.8) = 0.8909 \\<br />
h_2^{(2)} &amp;= f(0.4*1.5 + 0.4*2.0 + 0.4*3.0 + 0.8) = 0.9677 \\<br />
h_3^{(2)} &amp;= f(0.6*1.5 + 0.6*2.0 + 0.6*3.0 + 0.8) = 0.9909 \\<br />
h_{W,b}(x) &amp;= h_1^{(3)} = f(0.5*0.8909 + 0.5*0.9677 + 0.5*0.9909 + 0.2) = 0.8354<br />
\end{align}</p>
<h3 id="more-efficient-implementation">3.3 A more efficient implementation</h3>
<p>As was stated earlier &#8211; using loops isn&#8217;t the most efficient way of calculating the feed forward step in Python. This is because the loops in Python are notoriously slow. An alternative, more efficient mechanism of doing the feed forward step in Python and numpy will be discussed shortly. We can benchmark how efficient the algorithm is by using the %timeit function in IPython, which runs the function a number of times and returns the average time that the function takes to run:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">%timeit simple_looped_nn_calc(3, x, w, b)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Running this tells us that the looped feed forward takes $40\mu s$. A result in the tens of microseconds sounds very fast, but when applied to very large practical NNs with 100s of nodes per layer, this speed will become prohibitive, especially when training the network, as will become clear later in this tutorial.  If we try a four layer neural network using the same code, we get significantly worse performance &#8211; $70\mu s$ in fact.</p>
<h3 id="vectorisation">3.4 Vectorisation in neural networks</h3>
<p>There is a way to write the equations even more compactly, and to calculate the feed forward process in neural networks more efficiently, from a computational perspective.  Firstly, we can introduce a new variable $z_{i}^{(l)}$ which is the summated input into node $i$ of layer $l$, including the bias term.  So in the case of the first node in layer 2, $z$ is equal to:</p>
<p>$$z_{1}^{(2)} = w_{11}^{(1)}x_1 + w_{12}^{(1)} x_2 + w_{13}^{(1)} x_3 + b_1^{(1)} = \sum_{j=1}^{n} w_{ij}^{(1)}x_i + b_{i}^{(1)}$$</p>
<p>where n is the number of nodes in layer 1.  Using this notation, the unwieldy previous set of equations for the example three layer network can be reduced to:</p>
<p>\begin{align}<br />
z^{(2)} &amp;= W^{(1)} x + b^{(1)} \\<br />
h^{(2)} &amp;= f(z^{(2)}) \\<br />
z^{(3)} &amp;= W^{(2)} h^{(2)} + b^{(2)} \\<br />
h_{W,b}(x) &amp;= h^{(3)} = f(z^{(3)})<br />
\end{align}</p>
<p>Note the use of capital W to denote the matrix form of the weights.  It should be noted that all of the elements in the above equation are now matrices / vectors.  If you&#8217;re unfamiliar with these concepts, they will be explained more fully in the next section. Can the above equation be simplified even further?  Yes, it can.  We can forward propagate the calculations through any number of layers in the neural network by generalising:</p>
<p>\begin{align}<br />
z^{(l+1)} &amp;= W^{(l)} h^{(l)} + b^{(l)}   \\<br />
h^{(l+1)} &amp;= f(z^{(l+1)})<br />
\end{align}</p>
<p>Here we can see the general feed forward process, where the output of layer $l$ becomes the input to layer $l+1$. We know that $h^{(1)}$ is simply the input layer $x$ and $h^{(n_l)}$ (where $n_l$ is the number of layers in the network) is the output of the output layer. Notice in the above equations that we have dropped references to the node numbers $i$ and $j$ &#8211; how can we do this? Don&#8217;t we still have to loop through and calculate all the various node inputs and outputs?</p>
<p>The answer is that we can use matrix multiplications to do this more simply. This process is called &#8220;vectorisation&#8221; and it has two benefits &#8211; first, it makes the code less complicated, as you will see shortly. Second, we can use fast linear algebra routines in Python (and other languages) rather than using loops, which will speed up our programs. Numpy can handle these calculations easily. First, for those who aren&#8217;t familiar with matrix operations, the next section is a brief recap.</p>
<h3 id="matrix-mult">3.5 Matrix multiplication</h3>
<p>Let&#8217;s expand out $z^{(l+1)} = W^{(l)} h^{(l)} + b^{(l)}$ in explicit matrix/vector form for the input layer (i.e. $h^{(l)} = x$):</p>
<p>\begin{align}<br />
z^{(2)} &amp;=<br />
\begin{pmatrix}<br />
w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)} \\<br />
w_{21}^{(1)} &amp; w_{22}^{(1)} &amp; w_{23}^{(1)} \\<br />
w_{31}^{(1)} &amp; w_{32}^{(1)} &amp; w_{33}^{(1)} \\<br />
\end{pmatrix}<br />
\begin{pmatrix}<br />
x_{1} \\<br />
x_{2} \\<br />
x_{3} \\<br />
\end{pmatrix} +<br />
\begin{pmatrix}<br />
b_{1}^{(1)} \\<br />
b_{2}^{(1)} \\<br />
b_{3}^{(1)} \\<br />
\end{pmatrix} \\<br />
&amp;=<br />
\begin{pmatrix}<br />
w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + w_{13}^{(1)}x_{3} \\<br />
w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} + w_{23}^{(1)}x_{3} \\<br />
w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2} + w_{33}^{(1)}x_{3} \\<br />
\end{pmatrix} +<br />
\begin{pmatrix}<br />
b_{1}^{(1)} \\<br />
b_{2}^{(1)} \\<br />
b_{3}^{(1)} \\<br />
\end{pmatrix} \\<br />
&amp;=<br />
\begin{pmatrix}<br />
w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + w_{13}^{(1)}x_{3} + b_{1}^{(1)} \\<br />
w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} + w_{23}^{(1)}x_{3} + b_{2}^{(1)} \\<br />
w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2} + w_{33}^{(1)}x_{3} + b_{3}^{(1)} \\<br />
\end{pmatrix} \\<br />
\end{align}</p>
<p>For those who aren&#8217;t aware of how matrix multiplication works, it is a good idea to scrub up on matrix operations. There are many <a href="http://stattrek.com/tutorials/matrix-algebra-tutorial.aspx" target="_blank" rel="noopener noreferrer">sites</a> which cover this well. However, just quickly, when the weight matrix is multiplied by the input layer vector, each element in the $row$ of the weight matrix is multiplied by each element in the single $column$ of the input vector, then summed to create a new (3 x 1) vector. Then you can simply add the bias weights vector to achieve the final result.</p>
<p>You can observe how each row of the final result above corresponds to the argument of the activation function in the original non-matrix set of equations above. If the activation function is capable of being applied element-wise (i.e. to each row separately in the $z^{(1)}$ vector), then we can do all our calculations using matrices and vectors rather than slow Python loops. Thankfully, numpy allows us to do just that, with reasonably fast matrix operations and element-wise functions. Let&#8217;s have a look at a much more simplified (and faster) version of the simple_looped_nn_calc:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def matrix_feed_forward_calc(n_layers, x, w, b):
    for l in range(n_layers-1):
        if l == 0:
            node_in = x
        else:
            node_in = h
        z = w[l].dot(node_in) + b[l]
        h = f(z)
    return h</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note line 7  where the matrix multiplication occurs &#8211; if you just use the $*$ symbol when multiplying the weights by the node input vector in numpy it will attempt to perform some sort of element-wise multiplication, rather than the true matrix multiplication that we desire. Therefore you need to use the <em>a.dot(b)</em> notation when performing matrix multiplication in numpy.</p>
<p>If we perform %timeit again using this new function and a simple 4 layer network, we only get an improvement of $24\mu s$ (a reduction from $70\mu s$ to $46\mu s$).  However, if we increase the size of the 4 layer network to layers of 100-100-50-10 nodes the results are much more impressive.  The Python looped based method takes a whopping $41ms$ &#8211; note, that is milliseconds, and the vectorised implementation only takes $84\mu s$ to forward propagate through the neural network.  By using vectorised calculations instead of Python loops we have increased the efficiency of the calculation 500 fold! That&#8217;s a huge improvement. There is even the possibility of faster implementations of matrix operations using deep learning packages such as <a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow</a> and <a href="http://www.deeplearning.net/software/theano/" target="_blank" rel="noopener noreferrer">Theano</a> which utilise your computer&#8217;s GPU (rather than the CPU), the architecture of which is more suited to fast matrix computations  (I have a <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener noreferrer">TensorFlow tutorial </a>post also).</p>
<p>That brings us to an end of the feed-forward introduction for neural networks.  The next section will deal with how to actually train a neural network so that it can perform classification tasks, using gradient descent and backpropagation.</p>
<h2 id="gradient-desc-opt">4 Gradient descent and optimisation</h2>
<p>As mentioned in <a href="#what-are-anns">Section 1</a>, the setting of the values of the weights which link the layers in the network is what constitutes the training of the system. In supervised learning, the idea is to reduce the <em>error</em> between the input and the desired output. So if we have a neural network with one output layer, and given some input $x$ we want the neural network to output a 2, yet the network actually produces a 5, a simple expression of the <em>error</em> is $abs(2-5)=3$. For the mathematically minded, this would be the $L^1$ norm of the error (don&#8217;t worry about it if you don&#8217;t know what this is).</p>
<p>The idea of supervised learning is to provide many input-output pairs of known data and vary the weights based on these samples so that the error expression is minimised. We can specify these input-output pairs as $\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}$ where $m$ is the number of training samples that we have on hand to train the weights of the network. Each of these inputs or outputs can be vectors &#8211; that is $x^{(1)}$ is not necessarily just one value, it could be an $N$ dimensional series of values. For instance, let&#8217;s say that we&#8217;re training a spam-detection neural network &#8211; in such a case $x^{(1)}$ could be a count of all the different significant words in an e-mail e.g.:</p>
<p>\begin{align}<br />
x^{(1)} &amp;=<br />
\begin{pmatrix}<br />
No. of &#8220;prince&#8221; \\<br />
No. of &#8220;nigeria&#8221; \\<br />
No. of &#8220;extension&#8221; \\<br />
\vdots \\<br />
No. of &#8220;mum&#8221; \\<br />
No. of &#8220;burger&#8221; \\<br />
\end{pmatrix} \\<br />
&amp;=<br />
\begin{pmatrix}<br />
2 \\<br />
2 \\<br />
0 \\<br />
\vdots \\<br />
0 \\<br />
1 \\<br />
\end{pmatrix}<br />
\end{align}</p>
<p>$y^{(1)}$ in this case could be a single scalar value, either a 1 or a 0 to designate whether the e-mail is spam or not. Or, in other applications it could be a $K$ dimensional vector. As an example, say we have input $x$ that is a vector of the pixel greyscale readings of a photograph.  We also have an output $y$ that is a 26 dimensional vector that designates, with a 1 or 0, what letter of the alphabet is shown in the photograph i.e. $(1, 0, \ldots, 0)$ for a, $(0, 1, \ldots, 0)$ for b and so on.  This 26 dimensional output vector could be used to classify letters in photographs.</p>
<p>In training the network with these $(x, y)$ pairs, the goal is to get the neural network better and better at predicting the correct $y$ given $x$. This is performed by varying the weights so as to minimize the error. How do we know how to vary the weights, given an error in the output of the network? This is where the concept of <strong><em>gradient descent</em></strong> comes in handy. Consider the diagram below:</p>
<p>&nbsp;</p>
<figure id="attachment_74" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-74 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent-300x156.jpg" alt="" width="300" height="156" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent-300x156.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Gradient-descent.jpg 553w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 8. Simple, one-dimensional gradient descent</figcaption></figure>
<p>In this diagram we have a blue plot of the error depending on a single scalar weight value, $w$. The minimum possible error is marked by the black cross, but we don&#8217;t know what $w$ value gives that minimum error. We start out at a random value of $w$, which gives an error marked by the red dot on the curve labelled with &#8220;1&#8221;. We need to change $w$ in a way to approach that minimum possible error, the black cross. One of the most common ways of approaching that value is called <strong><em>gradient descent</em></strong>.</p>
<p>To proceed with this method, first the gradient of the error with respect to $w$ is calculated at point &#8220;1&#8221;. For those who don&#8217;t know, the <em>gradient</em> is the slope of the error curve at that point. It is shown in the diagram above by the black arrow which &#8220;pierces&#8221; point &#8220;1&#8221;. The gradient also gives directional information &#8211; if it is positive with respect to an increase in $w$, a step in that <em>direction</em> will lead to an increase in the error. If it is negative with respect to an <em>increase</em> in $w$ (as it is in the diagram above), a step in that will lead to a <em>decrease</em> in the error. Obviously, we wish to make a step in $w$ that will lead to a decrease in the error. The magnitude of the <em>gradient</em> or the &#8220;steepness&#8221; of the slope, gives an indication of how fast the error curve or function is changing at that point. The higher the magnitude of the gradient, the faster the error is changing at that point with respect to $w$.</p>
<p>The gradient descent method uses the gradient to make an informed step change in $w$ to lead it towards the minimum of the error curve. This is an <em>iterative</em> method, that involves multiple steps. Each time, the $w$ value is updated according to:</p>
<p>\begin{equation}<br />
w_{new} = w_{old} &#8211; \alpha * \nabla error<br />
\end{equation}</p>
<p>Here $w_{new}$ denotes the new $w$ position, $w_{old}$ denotes the current or old $w$ position, $\nabla error$ is the gradient of the error at $w_{old}$ and $\alpha$ is the step size. The step size $\alpha$ will determine how quickly the solution converges on the minimum error. However, this parameter has to be tuned &#8211; if it is too large, you can imagine the solution bouncing around on either side of the minimum in the above diagram. This will result in an optimisation of $w$ that does not converge. As this iterative algorithm approaches the minimum, the gradient or change in the error with each step will reduce. You can see in the graph above that the gradient lines will &#8220;flatten out&#8221; as the solution point approaches the minimum. As the solution approaches the minimum error, because of the decreasing gradient, it will result in only small improvements to the error.  When the solution approaches this &#8220;flattening&#8221; out of the error we want to exit the iterative process.  This exit can be performed by either stopping after a certain number of iterations or via some sort of &#8220;stop condition&#8221;.  This stop condition might be when the change in the error drops below a certain limit, often called the <em>precision</em>.</p>
<h3 id="simple-example">4.1 A simple example in code</h3>
<p>Below is an example of a simple Python implementation of gradient descent for solving the minimum of the equation $f(x) = x^4 &#8211; 3x^3 + 2$ taken from <a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener noreferrer">Wikipedia.</a>  The gradient of this function is able to be calculated analytically (i.e. we can do it easily using calculus, which we can&#8217;t do with many real world applications) and is $f'(x) = 4x^3 &#8211; 9x^2$. This means at every value of $x$, we can calculate the gradient of the function by using a simple equation. Again, using calculus we can know that the exact minimum of this equation is $x = 2.25$ .</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x_old = 0 # The value does not matter as long as abs(x_new - x_old) &gt; precision
x_new = 6 # The algorithm starts at x=6
gamma = 0.01 # step size
precision = 0.00001

def df(x):
    y = 4 * x**3 - 9 * x**2
    return y

while abs(x_new - x_old) &gt; precision:
    x_old = x_new
    x_new += -gamma * df(x_old)

print(&quot;The local minimum occurs at %f&quot; % x_new)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function prints &#8220;The local minimum occurs at 2.249965&#8221;, which agrees with the exact solution within the precision.  This code implements the weight adjustment algorithm that I showed above, and can be seen to find the minimum of the function correctly within the given precision. This is a very simple example of gradient descent, and finding the gradient works quite differently when training neural networks. However, the main idea remains &#8211; we figure out the gradient of the neural network then adjust the weights in a step to try to get closer to the minimum error that we are trying to find. Another difference between this toy example of gradient descent is that the weight vector is multi-dimensional, and therefore the gradient descent method must search a multi-dimensional space for the minimum point.</p>
<p>The way we figure out the gradient of a neural network is via the famous <em><strong>backpropagation</strong></em> method, which will be discussed shortly. First however, we have to look at the error function more closely.</p>
<h3 id="the-cost-function">4.2 The cost function</h3>
<p>Previously, we&#8217;ve talked about iteratively minimising the error of the output of the neural network by varying the weights in gradient descent. However, as it turns out, there is a mathematically more generalised way of looking at things that allows us to reduce the error while also preventing things like <em>overfitting</em> (this will be discussed more in later articles). This more general optimisation formulation revolves around minimising what&#8217;s called the <strong><em>cost function</em></strong>. The equivalent cost function of a single training pair ($x^z$, $y^z$) in a neural network is:</p>
<p>\begin{align}<br />
J(w,b,x,y) &amp;= \frac{1}{2} \parallel y^z &#8211; h^{(n_l)}(x^z) \parallel ^2 \\<br />
&amp;= \frac{1}{2} \parallel y^z &#8211; y_{pred}(x^z) \parallel ^2<br />
\end{align}</p>
<p>This shows the cost function of the $z_{th}$ training sample, where $h^{(n_l)}$ is the output of the final layer of the neural network i.e. the output of the neural network. I&#8217;ve also represented $h^{(n_l)}$ as $y_{pred}$ to highlight the prediction of the neural network given $x^z$. The two vertical lines represent the $L^2$ norm of the error, or what is known as the sum-of-squares error (SSE). SSE is a very common way of representing the error of a machine learning system. Instead of taking just the absolute error $abs(y_{pred}(x^z) &#8211; y^z)$, we use the square of the error. There are many reasons why the SSE is often used which will not be discussed here &#8211; suffice to say that this is a very common way of representing the errors in machine learning. The $\frac{1}{2}$ out the front is just a constant added that tidies things up when we differentiate the cost function, which we&#8217;ll be doing when we perform backpropagation.</p>
<p>Note that the formulation for the cost function above is for a single $(x,y)$ training pair. We want to minimise the cost function over all of our $m$ training pairs. Therefore, we want to find the minimum *mean squared error* (MSE) over all the training samples:</p>
<p>\begin{align}<br />
J(w,b) &amp;= \frac{1}{m} \sum_{z=0}^m \frac{1}{2} \parallel y^z &#8211; h^{(n_l)}(x^z) \parallel ^2 \\<br />
&amp;= \frac{1}{m} \sum_{z=0}^m J(W, b, x^{(z)}, y^{(z)})<br />
\end{align}</p>
<p>So, how do you use the cost function $J$ above to train the weights of our network? Using gradient descent and backpropagation. First, let&#8217;s look at gradient descent more closely in neural networks.</p>
<h3 id="gradient-descent-in-nn">4.3 Gradient descent in neural networks</h3>
<p>Gradient descent for every weight $w_{(ij)}^{(l)}$ and every bias $b_i^{(l)}$ in the neural network looks like the following:</p>
<p>\begin{align}<br />
w_{ij}^{(l)} &amp;= w_{ij}^{(l)} &#8211; \alpha \frac{\partial}{\partial w_{ij}^{(l)}} J(w,b) \\<br />
b_{i}^{(l)} &amp;= b_{i}^{(l)} &#8211; \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(w,b)<br />
\end{align}</p>
<p>Basically, the equation above is similiar to the previously shown gradient descent algorithm: $w_{new} = w_{old} &#8211; \alpha * \nabla error$. The new and old subscripts are missing, but the values on the left side of the equation are <em>new</em> and the values on the right side are <em>old</em>. Again, we have an iterative process whereby the weights are updated in each iteration, this time based on the cost function $J(w,b)$.</p>
<p>The values $\frac{\partial}{\partial w_{ij}^{(l)}}$ and $\frac{\partial}{\partial b_{i}^{(l)}}$ are the <em>partial derivatives </em>of the single sample cost function based on the weight values. What does this mean? Recall that for the simple gradient descent example mentioned previously, each step depends on the <em>slope</em> of the error/cost term with respect to the weights. Another word for slope or gradient is the <em>derivative</em>. A normal derivative has the notation $\frac{d}{dx}$. If $x$ in this instance is a vector, then such a derivative will also be a vector, displaying the gradient in all the dimensions of $x$.</p>
<h3 id="two-dimensional">4.4 A two dimensional gradient descent example</h3>
<p>Let&#8217;s take the example of a standard two-dimensional gradient descent problem. Below is a diagram of an iterative two-dimensional gradient descent run:</p>
<figure id="attachment_81" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-81 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Two-dimensional-gradient-descent-300x203.jpg" alt="" width="300" height="203" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Two-dimensional-gradient-descent-300x203.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Two-dimensional-gradient-descent.jpg 471w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 9. Two-dimensional gradient descent</figcaption></figure>
<p>The blue lines in the above diagram are the contour lines of the cost function &#8211; designating regions with an error value that is approximately the same. As can be observed in the diagram above, each step ($p_1 \to p_2 \to p_3$) in the gradient descent involves a gradient or derivative that is an arrow/vector.  This vector spans both the $[x_1, x_2]$ dimensions, as the solution works its way towards the minimum in the centre. So, for instance, the derivative evaluated at $p_1$ might be $\frac {d}{dx} = [2.1, 0.7]$, where the derivative is a vector to designate the two directions. The <em>partial</em> derivative $\frac {\partial}{\partial x_1}$ in this case would be a scalar $\to [2.1]$ &#8211; in other words, it is the gradient in only one direction of the search space ($x_1$). In gradient descent, it is often the case that the partial derivative of all the possible search directions are calculated, then &#8220;gathered up&#8221; to determine a new, complete, step direction.</p>
<p>In neural networks, we don&#8217;t have a simple cost function where we can easily evaluate the gradient, like we did in our toy gradient descent example ($f(x) = x^4 &#8211; 3x^3 + 2$). In fact, things are even trickier. While we can compare the output of the neural network to our expected training value, $y^{(z)}$ and feasibly look at how changing the weights of the output layer would change the cost function for the sample (i.e. calculating the gradient), how on earth do we do that for all the <em>hidden</em> layers of the network?</p>
<p>The answer to that is the backpropagation method. This method allows us to &#8220;share&#8221; the cost function or error to all the weights in the network &#8211; or in other words, it allows us to determine how much of the error is caused by any given weight.</p>
<h3 id="backprop-in-depth">4.5 Backpropagation in depth</h3>
<p>In this section, I&#8217;m going to delve into the maths a little. If you&#8217;re wary of the maths of how backpropagation works, then it may be best to skip this section.  The next <a href="#implementing-nn">section</a> will show you how to implement backpropagation in code &#8211; so if you want to skip straight on to using this method, feel free to skip the rest of this section. However, if you don&#8217;t mind a little bit of maths, I encourage you to push on to the end of this section as it will give you a good depth of understanding in training neural networks. This will be invaluable to understanding some of the key ideas in deep learning, rather than just being a code cruncher who doesn&#8217;t really understand how the code works.</p>
<p>First let&#8217;s recall some of the foundational equations from <a href="#the-feed-forward-pass">Section 3</a> for the following three layer neural network:</p>
<figure id="attachment_60" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-60 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network-300x158.png" alt="" width="300" height="158" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network-300x158.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Three-layer-network.png 591w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 10. Three layer neural network (again)</figcaption></figure>
<p>The output of this neural network can be calculated by:</p>
<p>\begin{equation}<br />
h_{W,b}(x) = h_1^{(3)} = f(w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)} h_2^{(2)} + w_{13}^{(2)} h_3^{(2)} + b_1^{(2)})<br />
\end{equation}</p>
<p>We can also simplify the above to $h_1^{(3)} = f(z_1^{(2)})$ by defining $z_1^{(2)}$ as:</p>
<p>$$z_{1}^{(2)} = w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)} h_2^{(2)} + w_{13}^{(2)} h_3^{(2)} + b_1^{(2)}$$</p>
<p>Let&#8217;s say we want to find out how much a change in the weight $w_{12}^{(2)}$ has on the cost function $J$. This is to evaluate $\frac {\partial J}{\partial w_{12}^{(2)}}$. To do so, we have to use something called the chain function:</p>
<p>$$\frac {\partial J}{\partial w_{12}^{(2)}} = \frac {\partial J}{\partial h_1^{(3)}} \frac {\partial h_1^{(3)}}{\partial z_1^{(2)}} \frac {\partial z_1^{(2)}}{\partial w_{12}^{(2)}}$$</p>
<p>If you look at the terms on the right &#8211; the numerators &#8220;cancel out&#8221; the denominators, in the same way that $\frac {2}{5} \frac {5}{2} = \frac {2}{2} = 1$. Therefore we can construct $\frac {\partial J}{\partial w_{12}^{(2)}}$ by stringing together a few partial derivatives (which are quite easy, thankfully). Let&#8217;s start with $\frac {\partial z_1^{(2)}}{\partial w_{12}^{(2)}}$:</p>
<p>\begin{align}<br />
\frac {\partial z_1^{(2)}}{\partial w_{12}^{(2)}} &amp;= \frac {\partial}{\partial w_{12}^{(2)}} (w_{11}^{(1)}h_1^{(2)} + w_{12}^{(1)} h_2^{(2)} + w_{13}^{(1)} h_3^{(2)} + b_1^{(1)})\\<br />
&amp;= \frac {\partial}{\partial w_{12}^{(2)}} (w_{12}^{(1)} h_2^{(2)})\\<br />
&amp;= h_2^{(2)}<br />
\end{align}</p>
<p>The partial derivative of $z_1^{(2)}$ with respect $w_{12}^{(2)}$ only operates on one term within the parentheses, $w_{12}^{(1)} h_2^{(2)}$, as all the other terms don&#8217;t vary at all when $w_{12}^{(2)}$ does. The derivative of a constant is 1, therefore $\frac {\partial}{\partial w_{12}^{(2)}} (w_{12}^{(1)} h_2^{(2)})$ collapses to just $h_2^{(2)}$, which is simply the output of the second node in layer 2.</p>
<p>The next partial derivative in the chain is $\frac {\partial h_1^{(3)}}{\partial z_1^{(2)}}$, which is the partial derivative of the activation function of the $h_1^{(3)}$ output node. Because of the requirement to be able to derive this derivative, the activation functions in neural networks need to be <em>differentiable</em>. For the common sigmoid activation function (shown in <a href="#the-artificial-neuron">Section 2.1</a>), the derivative is:</p>
<p>$$\frac {\partial h}{\partial z} = f'(z) = f(z)(1-f(z))$$</p>
<p>Where $f(z)$ is the activation function. So far so good &#8211; now we have to work out how to deal with the first term $\frac {\partial J}{\partial h_1^{(3)}}$. Remember that $J(w,b,x,y)$ is the mean squared error loss function, which looks like (for our case):</p>
<p>$$J(w,b,x,y) = \frac{1}{2} \parallel y_1 &#8211; h_1^{(3)}(z_1^{(2)}) \parallel ^2$$</p>
<p>Here $y_1$ is the training target for the output node. Again using the chain rule:</p>
<p>\begin{align}<br />
&amp;Let\ u = \parallel y_1 &#8211; h_1^{(3)}(z_1^{(2)}) \parallel\ and\ J = \frac {1}{2} u^2\\<br />
&amp;Using\ \frac {\partial J}{\partial h} = \frac {\partial J}{\partial u} \frac {\partial u}{\partial h}:\\<br />
&amp;\frac {\partial J}{\partial h} = -(y_1 &#8211; h_1^{(3)})<br />
\end{align}</p>
<p>So we&#8217;ve now figured out how to calculate $\frac {\partial J}{\partial w_{12}^{(2)}}$, at least for the weights connecting the output layer. Before we move to any hidden layers (i.e. layer 2 in our example case), let&#8217;s introduce some simplifications to tighten up our notation and introduce $\delta$:</p>
<p>$$\delta_i^{(n_l)} = -(y_i &#8211; h_i^{(n_l)})\cdot f^\prime(z_i^{(n_l)})$$</p>
<p>Where $i$ is the node number of the output layer. In our selected example there is only one such layer, therefore $i=1$ always in this case. Now we can write the complete cost function derivative as:</p>
<p>\begin{align}<br />
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b,x, y) &amp;= h^{(l)}_j \delta_i^{(l+1)} \\<br />
\end{align}</p>
<p>Where, for the output layer in our case, $l$ = 2 and $i$ remains the node number.</p>
<h3 id="prop-in-hidden-layers">4.6 Propagating into the hidden layers</h3>
<p>What about for weights feeding into any hidden layers (layer 2 in our case)? For the weights connecting the output layer, the $\frac {\partial J}{\partial h} = -(y_i &#8211; h_i^{(n_l)})$ derivative made sense, as the cost function can be directly calculated by comparing the output layer to the training data. The output of the hidden nodes, however, have no such direct reference, rather, they are connected to the cost function only through mediating weights and potentially other layers of nodes. How can we find the variation in the cost function from changes to weights embedded deep within the neural network? As mentioned previously, we use the <em>backpropagation</em> method.</p>
<p>Now that we&#8217;ve done the hard work using the chain rule, we&#8217;ll now take a more graphical approach. The term that needs to propagate back through the network is the $\delta_i^{(n_l)}$ term, as this is the network&#8217;s ultimate connection to the cost function. What about node j in the second layer (hidden layer)? How does it contribute to $\delta_i^{(n_l)}$ in our test network? It contributes via the weight $w_{ij}^{(2)}$ &#8211; see the diagram below for the case of $j=1$ and $i=1$.</p>
<figure id="attachment_83" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-83 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration-300x204.jpg" alt="" width="300" height="204" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration-300x204.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration.jpg 446w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 11. Simple backpropagation illustration</figcaption></figure>
<p>As can be observed from above, the output layer $\delta$ is <em>communicated</em> to the hidden node by the weight of the connection. In the case where there is only one output layer node, the generalised hidden layer $\delta$ is defined as:</p>
<p>$$\delta_j^{(l)} = \delta_1^{(l+1)} w_{1j}^{(l)}\ f^\prime(z_j)^{(l)}$$</p>
<p>Where $j$ is the node number in layer $l$. What about the case where there are multiple output nodes? In this case, the weighted sum of all the communicated errors are taken to calculate $\delta_j^{(l)}$, as shown in the diagram below:</p>
<figure id="attachment_84" style="width: 300px" class="wp-caption aligncenter"><img class="wp-image-84 size-medium" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration-with-multiple-outputs-300x242.jpg" alt="" width="300" height="242" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration-with-multiple-outputs-300x242.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Backpropagation-illustration-with-multiple-outputs.jpg 448w" sizes="(max-width: 300px) 100vw, 300px" /><figcaption class="wp-caption-text">Figure 12. Backpropagation illustration with multiple outputs</figcaption></figure>
<p>As can be observed from the above, each $\delta$ value from the output layer is included in the sum used to calculate $\delta_1^{(2)}$, but each output $\delta$ is weighted according to the appropriate $w_{i1}^{(2)}$ value. In other words, node 1 in layer 2 contributes to the error of three output nodes, therefore the measured error (or cost function value) at each of these nodes has to be &#8220;passed back&#8221; to the $\delta$ value for this node. Now we can develop a generalised expression for the $\delta$ values for nodes in the hidden layers:</p>
<p>$$\delta_j^{(l)} = (\sum_{i=1}^{s_{(l+1)}} w_{ij}^{(l)} \delta_i^{(l+1)})\ f^\prime(z_j^{(l)})$$</p>
<p>Where $j$ is the node number in layer $l$ and $i$ is the node number in layer $l+1$ (which is the same notation we have used from the start). The value $s_{(l+1)}$ is the number of nodes in layer $(l+1)$.</p>
<p>So we now know how to calculate: $$\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b,x, y) = h^{(l)}_j \delta_i^{(l+1)}$$ as shown previously. What about the bias weights? I&#8217;m not going to derive them as I did with the normal weights in the interest of saving time / space. However, the reader shouldn&#8217;t have too many issues following the same steps, using the chain rule, to arrive at:</p>
<p>$$\frac{\partial}{\partial b_{i}^{(l)}} J(W,b,x, y) = \delta_i^{(l+1)}$$</p>
<p>Great &#8211; so we now know how to perform our original gradient descent problem for neural networks:</p>
<p>\begin{align}<br />
w_{ij}^{(l)} &amp;= w_{ij}^{(l)} &#8211; \alpha \frac{\partial}{\partial w_{ij}^{(l)}} J(w,b) \\<br />
b_{i}^{(l)} &amp;= b_{i}^{(l)} &#8211; \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(w,b)<br />
\end{align}</p>
<p>However, to perform this gradient descent training of the weights, we would have to resort to loops within loops. As previously shown in <a href="#vectorisation">Section 3.4</a> of this neural network tutorial, performing such calculations in Python using loops is slow for large networks. Therefore, we need to figure out how to vectorise such calculations, which the next section will show.</p>
<h3 id="vector-backprop">4.7 Vectorisation of backpropagation</h3>
<p>To consider how to vectorise the gradient descent calculations in neural networks, let&#8217;s first look at a naïve vectorised version of the gradient of the cost function (<strong>warning</strong>: this is not in a correct form yet!):</p>
<p>\begin{align}<br />
\frac{\partial J}{\partial W^{(l)}} &amp;= h^{(l)} \delta^{(l+1)}\\<br />
\frac{\partial J}{\partial b^{(l)}} &amp;= \delta^{(l+1)}<br />
\end{align}</p>
<p>Now, let&#8217;s look at what element of the above equations. What does $h^{(l)}$ look like? Pretty simple, just a $(s_l \times 1)$ vector, where $s_l$ is the number of nodes in layer $l$. What does the multiplication of $h^{(l)} \delta^{(l+1)}$ look like? Well, because we know that $\alpha \times \frac{\partial J}{\partial W^{(l)}}$ must be the same size of the weight matrix $W^{(l)}$, we know that the outcome of $h^{(l)} \delta^{(l+1)}$ must also be the same size as the weight matrix for layer $l$. In other words it has to be of size $(s_{l+1} \times s_{l})$.</p>
<p>We know that $\delta^{(l+1)}$ has the dimension $(s_{l+1} \times 1)$ and that $h^{(l)}$ has the dimension of $(s_l \times 1)$. The rules of matrix multiplication show that a matrix of dimension $(\mathbf n \times m)$ multiplied by a matrix of dimension $(o \times \mathbf p)$ will have a product matrix of size $(\mathbf n \times \mathbf p)$. If we perform a straight multiplication between $h^{(l)}$ and $\delta^{(l+1)}$, the number of columns of the first vector (i.e. 1 column) will not equal the number of rows of the second vector (i.e. 3 rows), therefore we can&#8217;t perform a proper matrix multiplication. The only way we can get a proper outcome of size $(s_{l+1} \times s_{l})$ is by using a matrix <strong><em>transpose</em></strong>. A transpose swaps the dimensions of a matrix around e.g. a $(s_l \times 1)$ sized vector becomes a $(1 \times s_l)$ sized vector, and is denoted by a superscript of $T$. Therefore, we can do the following:</p>
<p>$$\delta^{(l+1)} (h^{(l)})^T = (s_{l+1} \times 1) \times (1 \times s_l) = (s_{l+1} \times s_l)$$</p>
<p>As can be observed below, by using the transpose operation we can arrive at the outcome we desired.</p>
<p>A final vectorisation that can be performed is during the weighted addition of the errors in the backpropagation step:</p>
<p>$$\delta_j^{(l)} = (\sum_{i=1}^{s_{(l+1)}} w_{ij}^{(l)} \delta_i^{(l+1)})\ f^\prime(z_j^{(l)}) = \left((W^{(l)})^T \delta^{(l+1)}\right) \bullet f'(z^{(l)})$$</p>
<p>The $\bullet$ symbol in the above designates an element-by-element multiplication (called the Hadamard product), not a matrix multiplication.  Note that the matrix multiplication $\left((W^{(l)})^T \delta^{(l+1)}\right)$ performs the necessary summation of the weights and $\delta$ values &#8211; the reader can check that this is the case.</p>
<h3 id="imp-gradient-desc">4.8 Implementing the gradient descent step</h3>
<p>Now, how do we integrate this new vectorisation into the gradient descent steps of our soon-to-be coded algorithm? First, we have to look again at the overall cost function we are trying to minimise (not just the sample-by-sample cost function shown in the preceding equation):</p>
<p>\begin{align}<br />
J(w,b) &amp;= \frac{1}{m} \sum_{z=0}^m J(W, b, x^{(z)}, y^{(z)})<br />
\end{align}</p>
<p>As we can observe, the total cost function is the mean of all the sample-by-sample cost function calculations. Also remember the gradient descent calculation (showing the element-by-element version along with the vectorised version):</p>
<p>\begin{align}<br />
w_{ij}^{(l)} &amp;= w_{ij}^{(l)} &#8211; \alpha \frac{\partial}{\partial w_{ij}^{(l)}} J(w,b)\\<br />
W^{(l)} &amp;= W^{(l)} &#8211; \alpha \frac{\partial}{\partial W^{(l)}} J(w,b)\\<br />
&amp;= W^{(l)} &#8211; \alpha \left[\frac{1}{m} \sum_{z=1}^{m} \frac {\partial}{\partial W^{(l)}} J(w,b,x^{(z)},y^{(z)}) \right]\\<br />
\end{align}</p>
<p>So that means as we go along through our training samples or batches, we have to have a term that is summing up the partial derivatives of the individual sample cost function calculations. This term will gather up all the values for the mean calculation. Let&#8217;s call this &#8220;summing up&#8221; term $\Delta W^{(l)}$. Likewise, the equivalent bias term can be called $\Delta b^{(l)}$. Therefore, at each sample iteration of the final training algorithm, we have to perform the following steps:</p>
<p>\begin{align}<br />
\Delta W^{(l)} &amp;= \Delta W^{(l)} + \frac {\partial}{\partial W^{(l)}} J(w,b,x^{(z)},y^{(z)})\\<br />
&amp;= \Delta W^{(l)} + \delta^{(l+1)} (h^{(l)})^T\\<br />
\Delta b^{(l)} &amp;= \Delta b^{(1)} + \delta^{(l+1)}<br />
\end{align}</p>
<p>By performing the above operations at each iteration, we slowly build up the previously mentioned sum $\sum_{z=1}^{m} \frac {\partial}{\partial W^{(l)}} J(w,b,x^{(z)},y^{(z)})$ (and the same for $b$). Once all the samples have been iterated through, and the $\Delta$ values have been summed up, we update the weight parameters :</p>
<p>\begin{align}<br />
W^{(l)} &amp;= W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} \right] \\<br />
b^{(l)} &amp;= b^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]
\end{align}</p>
<h3 id="final-gradient-desc-algo">4.9 The final gradient descent algorithm</h3>
<p>So, no we&#8217;ve finally made it to the point where we can specify the entire backpropagation-based gradient descent training of our neural networks. It has taken quite a few steps to show, but hopefully it has been instructive. The final backpropagation algorithm is as follows:</p>
<p>Randomly initialise the weights for each layer $W^{(l)}$<br />
While iterations &lt; iteration limit:<br />
1. Set $\Delta W$ and $\Delta b$ to zero<br />
2. For samples 1 to m:<br />
a. Perform a feed foward pass through all the $n_l$ layers. Store the activation function outputs $h^{(l)}$<br />
b. Calculate the $\delta^{(n_l)}$ value for the output layer<br />
c. Use backpropagation to calculate the $\delta^{(l)}$ values for layers 2 to $n_l-1$<br />
d. Update the $\Delta W^{(l)}$ and $\Delta b^{(l)}$ for each layer<br />
3. Perform a gradient descent step using:</p>
<p>$W^{(l)} = W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} \right]$<br />
$b^{(l)} = b^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]$</p>
<p>As specified in the algorithm above, we would repeat the gradient descent routine until we are happy that the average cost function has reached a minimum. At this point, our network is trained and (ideally) ready for use.</p>
<p>The next part of this neural networks tutorial will show how to implement this algorithm to train a neural network that recognises hand-written digits.</p>
<h2 id="implementing-nn">5 Implementing the neural network in Python</h2>
<p>In the last <a href="#gradient-desc-opt">section</a> we looked at the theory surrounding gradient descent training in neural networks and the backpropagation method. In this article, we are going to apply that theory to develop some code to perform training and prediction on the MNIST dataset. The MNIST dataset is a kind of go-to dataset in neural network and deep learning examples, so we&#8217;ll stick with it here too. What it consists of is a record of images of hand-written digits with associated labels that tell us what the digit is. Each image is 8 x 8 pixels in size, and the image data sample is represented by 64 data points which denote the pixel intensity. In this example, we&#8217;ll be using the MNIST dataset provided in the Python Machine Learning library called <a href="http://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit learn</a>. An example of the image (and the extraction of the data from the scikit learn dataset) is shown in the code below (for an image of 1):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">from sklearn.datasets import load_digits
digits = load_digits()
print(digits.data.shape)
import matplotlib.pyplot as plt 
plt.gray() 
plt.matshow(digits.images[1]) 
plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<figure id="attachment_94" style="width: 254px" class="wp-caption aligncenter"><img class="size-full wp-image-94" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/MNIST-digit-one.png" alt="" width="254" height="258" /><figcaption class="wp-caption-text">Figure 13. MNIST digit &#8220;1&#8221;</figcaption></figure>
<p>The code above prints (1797, 64) to show the shape of input data matrix and the pixelated digit &#8220;1&#8221; in the image above.  The code we are going to write in this neural networks tutorial will try and estimate the digits that these pixels represent (using neural networks of course). First things first, we need to get the input data in shape. To do so, we need to do two things:</p>
<ol>
<li>Scale the data</li>
<li>Split the data into test and train sets</li>
</ol>
<h3 id="scaling-data">5.1 Scaling data</h3>
<p>Why do we need to scale the input data?  First, have a look at one of the dataset pixel representations:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">digits.data[0,:]
Out[2]:
array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13.,
        15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,
         8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,
         5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,   0.,
         1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,
         0.,   0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Notice that the input data ranges from 0 up to 15?  It&#8217;s standard practice to scale the input data so that it all fits mostly between either 0 to 1 or with a small range centred around 0 i.e. -1 to 1.  Why?  Well, it can help the convergence of the neural network and is especially important if we are combining different data types.  Thankfully, this is easily done using sci-kit learn:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">from sklearn.preprocessing import StandardScaler
X_scale = StandardScaler()
X = X_scale.fit_transform(digits.data)
X[0,:]
Out[3]:
array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,
       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,
        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,
       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,
       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,
        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,
        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,
       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,
       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,
        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,
        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,
       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,
       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The scikit learn standard scaler by default normalises the data by subtracting the mean and dividing by the standard deviation.  As can be observed, most of the data points are centered around zero and contained within -2 and 2.  This is a good starting point.  There is no real need to scale the output data $y$.</p>
<h3 id="test-and-train">5.2 Creating test and training datasets</h3>
<p>In machine learning, there is a phenomenon called &#8220;overfitting&#8221;. This occurs when models, during training, become too complex &#8211; they become really well adapted to predict the training data, but when they are asked to predict something based on new data that they haven&#8217;t &#8220;seen&#8221; before, they perform poorly. In other words, the models don&#8217;t <em>generalise</em> very well. To make sure that we are not creating models which are too complex, it is common practice to split the dataset into a <em>training</em> set and a <em>test </em>set. The training set is, obviously, the data that the model will be trained on, and the test set is the data that the model will be tested on after it has been trained. The amount of training data is always more numerous than the testing data, and is usually between 60-80% of the total dataset.</p>
<p>Again, scikit learn makes this splitting of the data into training and testing sets easy:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">from sklearn.model_selection import train_test_split
y = digits.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this case, we&#8217;ve made the test set to be 40% of the total data, leaving 60% to train with. The train_test_split function in scikit learn pushes the data randomly into the different datasets &#8211; in other words, it doesn&#8217;t take the first 60% of rows as the training set and the second 40% of rows as the test set. This avoids data collection artefacts from degrading the performance of the model.</p>
<h3 id="setting-up-output">5.3 Setting up the output layer</h3>
<p>As you would have been able to gather, we need the output layer to predict whether the digit represented by the input pixels is between 0 and 9. Therefore, a sensible neural network architecture would be to have an output layer of 10 nodes, with each of these nodes representing a digit from 0 to 9. We want to train the network so that when, say, an image of the digit &#8220;5&#8221; is presented to the neural network, the node in the output layer representing 5 has the highest value. Ideally, we would want to see an output looking like this: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]. However, in reality, we can settle for something like this: [0.01, 0.1, 0.2, 0.05, 0.3, 0.8, 0.4, 0.03, 0.25, 0.02]. In this case, we can take the maximum index of the output array and call that our predicted digit.</p>
<p>For the MNIST data supplied in the scikit learn dataset, the &#8220;targets&#8221; or the classification of the handwritten digits is in the form of a single number. We need to convert that single number into a vector so that it lines up with our 10 node output layer. In other words, if the target value in the dataset is &#8220;1&#8221; we want to convert it into the vector: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]. The code below does just that:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import numpy as np
def convert_y_to_vect(y):
    y_vect = np.zeros((len(y), 10))
    for i in range(len(y)):
        y_vect[i, y[i]] = 1
    return y_vect
y_v_train = convert_y_to_vect(y_train)
y_v_test = convert_y_to_vect(y_test)
y_train[0], y_v_train[0]
Out[8]:
(1, array([ 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>&nbsp;</p>
<p>As can be observed above, the MNIST target (1) has been converted into the vector [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], which is what we want.</p>
<h3 id="creating-nn">5.4 Creating the neural network</h3>
<p>The next step is to specify the structure of the neural network. For the input layer, we know we need 64 nodes to cover the 64 pixels in the image. As discussed, we need 10 output layer nodes to predict the digits. We&#8217;ll also need a hidden layer in our network to allow for the complexity of the task. Usually, the number of hidden layer nodes is somewhere between the number of input layers and the number of output layers. Let&#8217;s define a simple Python list that designates the structure of our network:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">nn_structure = [64, 30, 10]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We&#8217;ll use sigmoid activation functions again, so let&#8217;s setup the sigmoid function and its derivative:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def f(x):
    return 1 / (1 + np.exp(-x))
def f_deriv(x):
    return f(x) * (1 - f(x))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Ok, so we now have an idea of what our neural network will look like. How do we train it? Remember the algorithm from <a href="#final-gradient-desc-algo">Section 4.9</a> , which we&#8217;ll repeat here for ease of access and review:</p>
<p>Randomly initialise the weights for each layer $W^{(l)}$<br />
While iterations &lt; iteration limit:<br />
1. Set $\Delta W$ and $\Delta b$ to zero<br />
2. For samples 1 to m:<br />
a. Perform a feed foward pass through all the $n_l$ layers. Store the activation function outputs $h^{(l)}$<br />
b. Calculate the $\delta^{(n_l)}$ value for the output layer<br />
c. Use backpropagation to calculate the $\delta^{(l)}$ values for layers 2 to $n_l-1$<br />
d. Update the $\Delta W^{(l)}$ and $\Delta b^{(l)}$ for each layer<br />
3. Perform a gradient descent step using:</p>
<p>$W^{(l)} = W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} \right]$<br />
$b^{(l)} = b^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]$</p>
<p>So the first step is to initialise the weights for each layer. To make it easy to organise the various layers, we&#8217;ll use Python dictionary objects (initialised by {}). Finally, the weights have to be initialised with random values &#8211; this is to ensure that the neural network will converge correctly during training. We use the numpy library random_sample function to do this. The weight initialisation code is shown below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import numpy.random as r
def setup_and_init_weights(nn_structure):
    W = {}
    b = {}
    for l in range(1, len(nn_structure)):
        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))
        b[l] = r.random_sample((nn_structure[l],))
    return W, b</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The next step is to set the mean accumulation values $\Delta W$ and $\Delta b$ to zero (they need to be the same size as the weight and bias matrices):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def init_tri_values(nn_structure):
    tri_W = {}
    tri_b = {}
    for l in range(1, len(nn_structure)):
        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))
        tri_b[l] = np.zeros((nn_structure[l],))
    return tri_W, tri_b</code></pre> <div class="code-embed-infos"> </div> </div>
<p>If we now step into the gradient descent loop, the first step is to perform a feed forward pass through the network. The code below is a variation on the feed forward function created in <a href="#the-feed-forward-pass">Section 3</a>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def feed_forward(x, W, b):
    h = {1: x}
    z = {}
    for l in range(1, len(W) + 1):
        # if it is the first layer, then the input into the weights is x, otherwise, 
        # it is the output from the last layer
        if l == 1:
            node_in = x
        else:
            node_in = h[l]
        z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  
        h[l+1] = f(z[l+1]) # h^(l) = f(z^(l)) 
    return h, z</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Finally, we have to then calculate the output layer delta $\delta^{(n_l)}$ and any hidden layer delta values $\delta^{(l)}$ to perform the backpropagation pass:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def calculate_out_layer_delta(y, h_out, z_out):
    # delta^(nl) = -(y_i - h_i^(nl)) * f&#039;(z_i^(nl))
    return -(y-h_out) * f_deriv(z_out)

def calculate_hidden_delta(delta_plus_1, w_l, z_l):
    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f&#039;(z^(l))
    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we can put all the steps together into the final function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):
    W, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print(&#039;Starting gradient descent for {} iterations&#039;.format(iter_num))
    while cnt &lt; iter_num:
        if cnt%1000 == 0:
            print(&#039;Iteration {} of {}&#039;.format(cnt, iter_num))
        tri_W, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        for i in range(len(y)):
            delta = {}
            # perform the feed forward pass and return the stored h and z values, to be used in the
            # gradient descent step
            h, z = feed_forward(X[i, :], W, b)
            # loop from nl-1 to 1 backpropagating the errors
            for l in range(len(nn_structure), 0, -1):
                if l == len(nn_structure):
                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])
                    avg_cost += np.linalg.norm((y[i,:]-h[l]))
                else:
                    if l &gt; 1:
                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])
                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))
                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[l][:,np.newaxis])) 
                    # trib^(l) = trib^(l) + delta^(l+1)
                    tri_b[l] += delta[l+1]
        # perform the gradient descent step for the weights in each layer
        for l in range(len(nn_structure) - 1, 0, -1):
            W[l] += -alpha * (1.0/m * tri_W[l])
            b[l] += -alpha * (1.0/m * tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return W, b, avg_cost_func</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The function above deserves a bit of explanation. First, we aren&#8217;t setting a termination of the gradient descent process based on some change or precision of the cost function. Rather, we are just running it for a set number of iterations (3,000 in this case) and we&#8217;ll monitor how the average cost function changes as we progress through the training (avg_cost_func list in the above code). In each iteration of the gradient descent, we cycle through each training sample (range(len(y)) and perform the feed forward pass and then the backpropagation. The backpropagation step is an iteration through the layers starting at the output layer and working backwards &#8211; range(len(nn_structure), 0, -1). We calculate the average cost, which we are tracking during the training, at the output layer (l == len(nn_structure)). We also update the mean accumulation values, $\Delta W$ and $\Delta b$, designated as tri_W and tri_b, for every layer apart from the output layer (there are no weights connecting the output layer to any further layer).</p>
<p>Finally, after we have looped through all the training samples, accumulating the tri_W and tri_b values, we perform a gradient descent step change in the weight and bias values:<br />
$$W^{(l)} = W^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta W^{(l)} \right]$$<br />
$$b^{(l)} = b^{(l)} &#8211; \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]$$</p>
<p>After the process is completed, we return the trained weight and bias values, along with our tracked average cost for each iteration. Now it&#8217;s time to run the function &#8211; NOTE: this may take a few minutes depending on the capabilities of your computer.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we can have a look at how the average cost function decreased as we went through the gradient descent iterations of the training, slowly converging on a minimum in the function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">plt.plot(avg_cost_func)
plt.ylabel(&#039;Average J&#039;)
plt.xlabel(&#039;Iteration number&#039;)
plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<p><img class="wp-image-98 size-medium aligncenter" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Average-J-vs-iterations-300x205.png" alt="" width="300" height="205" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Average-J-vs-iterations-300x205.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Average-J-vs-iterations.png 389w" sizes="(max-width: 300px) 100vw, 300px" /></p>
<p>We can see in the above plot, that by 3,000 iterations of our gradient descent our average cost function value has started to &#8220;plateau&#8221; and therefore any further increases in the number of iterations isn&#8217;t likely to improve the performance of the network by much.</p>
<h3 id="assessing">5.5 Assessing the accuracy of the trained model</h3>
<p>Now that we&#8217;ve trained our MNIST neural network, we want to see how it performs on the test set. Is our model any good? Given a test input (64 pixels), we need to find what the output of our neural network is &#8211; we do that by simply performing a feed forward pass through the network using our trained weight and bias values. As discussed previously, we assess the prediction of the output layer by taking the node with the maximum output as the predicted digit. We can use the numpy.argmax function for this, which returns the index of the array value with the highest value:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def predict_y(W, b, X, n_layers):
    m = X.shape[0]
    y = np.zeros((m,))
    for i in range(m):
        h, z = feed_forward(X[i, :], W, b)
        y[i] = np.argmax(h[n_layers])
    return y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Finally, we can assess the accuracy of the prediction (i.e. the percentage of times the network predicted the handwritten digit correctly), by using the scikit learn accuracy_score function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">from sklearn.metrics import accuracy_score
y_pred = predict_y(W, b, X_test, 3)
accuracy_score(y_test, y_pred)*100</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This gives an 86% accuracy of predicting the digits.  Sounds pretty good right? Well actually no, it&#8217;s pretty bad. The current state-of-the-art deep learning algorithms achieve accuracy scores of 99.7% (see <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank" rel="noopener noreferrer">here</a>), so we are a fair way off that sort of accuracy.  There are many more exciting things to learn &#8211; my next post will cover some tips and tricks on how to improve the accuracy substantially on this simple neural network.  However, beyond that, we have a whole realm of state-of-the-art deep learning algorithms to learn and investigate, from convolution neural networks to deep belief nets and recurrent neural networks.  If you followed along ok with this post, you will be in a good position to advance to these newer techniques.</p>
<p>Stick around to find out more about this rapidly advancing area of machine learning.  As a start, check out these posts:<br />
<a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/">Python TensorFlow Tutorial – Build a Neural Network</a><br />
<a href="http://adventuresinmachinelearning.com/improve-neural-networks-part-1/">Improve your neural networks – Part 1 [TIPS AND TRICKS]</a><br />
<a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/">Stochastic Gradient Descent – Mini-batch and more</a></p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;d like to dig a little deeper still and you like video courses, I&#8217;d recommend the following inexpensive Udemy course on neural networks: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1151632&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeeplearning%2F" target="new">Deep Learning A-Z: Hands-On Artificial Neural Networks</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1151632&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">Neural Networks Tutorial &#8211; A Pathway to Deep Learning</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/neural-networks-tutorial/feed/</wfw:commentRss>
		<slash:comments>29</slash:comments>
		</item>
	</channel>
</rss>
