<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>NLP &#8211; Adventures in Machine Learning</title>
	<atom:link href="http://adventuresinmachinelearning.com/category/nlp/feed/" rel="self" type="application/rss+xml" />
	<link>http://adventuresinmachinelearning.com</link>
	<description>Learn and explore machine learning</description>
	<lastBuildDate>Sun, 09 Sep 2018 07:53:16 +0000</lastBuildDate>
	<language>en-AU</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.8</generator>
	<item>
		<title>Python gensim Word2Vec tutorial with TensorFlow and Keras</title>
		<link>http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/#comments</comments>
		<pubDate>Fri, 01 Sep 2017 22:24:41 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[gensim]]></category>
		<category><![CDATA[Keras]]></category>
		<category><![CDATA[NLP]]></category>
		<category><![CDATA[TensorFlow]]></category>
		<category><![CDATA[Word2Vec]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=517</guid>
		<description><![CDATA[<p>I&#8217;ve been dedicating quite a bit of time recently to Word2Vec tutorials because of the importance of the Word2Vec concept for natural language processing (NLP) <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" title="Python gensim Word2Vec tutorial with TensorFlow and Keras">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/">Python gensim Word2Vec tutorial with TensorFlow and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>I&#8217;ve been dedicating quite a bit of time recently to Word2Vec tutorials because of the importance of the Word2Vec concept for natural language processing (NLP) and also because I&#8217;ll soon be presenting some tutorials on recurrent neural networks and LSTMs for sequence prediction/NLP (UPDATE: I&#8217;ve completed a comprehensive tutorial on these topics &#8211; <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">Recurrent neural networks and LSTM tutorial in Python and TensorFlow</a>).  There are also some very interesting ideas floating around such as <a href="https://deeplearning4j.org/thoughtvectors" target="_blank" rel="noopener">thought vectors</a> which require an understanding of the Word2Vec concept.  My two Word2Vec tutorials are <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec word embedding tutorial in Python and TensorFlow</a> and <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">A Word2Vec Keras tutorial</a> showing the concepts of Word2Vec and implementing in TensorFlow and Keras, respectively.  In this tutorial, I am going to show you how you can use the original Google Word2Vec C code to generate word vectors, using the Python gensim library which wraps this cod,e and apply the results to TensorFlow and Keras.</p>
<p>The gensim Word2Vec implementation is very fast due to its C implementation &#8211; but to use it properly you will first need to install the <a href="http://cython.org/" target="_blank" rel="noopener">Cython library</a>. In this tutorial, I&#8217;ll show how to load the resulting embedding layer generated by gensim into TensorFlow and Keras embedding implementations.  Because of gensim&#8217;s blazing fast C wrapped code, this is a good alternative to running native Word2Vec embeddings in TensorFlow and Keras.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, check out this inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Word2Vec and gensim</h1>
<p>I&#8217;ve devoted plenty of words to explaining Word2Vec in my previous tutorials (<a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">here</a> and <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">here</a>) so I&#8217;ll only briefly introduce the Word2Vec concepts here.  For further details, check out those tutorials. Here&#8217;s the (relatively) quick version &#8211; for each text data set that we create, we have to create a <em>vocabulary</em>. The <em>vocabulary</em> is the list of unique words within the text.  Often it is &gt;10,000 words for serious data sets.  Machine learning models generally can&#8217;t take raw word inputs, so we first need to convert our data set into some number format &#8211; generally a list of unique integers.</p>
<p>Neural network based models like vector inputs. We, therefore, need to convert the integers into vectors.  A naive way of converting integers into vectors is to convert them into one-hot vectors &#8211; these are vectors where all of the values are set to zero, except for one i.e. [0, 0, 0, &#8230;, 1, &#8230;, 0, 0].  The &#8220;one-hot&#8221; value is located at the array index which matches the unique integer representation of the word. Therefore, our input one-hot vector must be at least the size of the vocabulary in length &#8211; i.e. &gt;10,000 words.</p>
<p>There are two main problems with this type of representation of words &#8211; the first is that it is inefficient. Each word is represented by a 10,000 word plus vector, which for neural networks means a heck of a lot of associated weights between the input layer and the first hidden layer (generally millions).  The second is that it loses all contextual meaning of the words.  We need a way of representing words that is both efficient and yet retains some of the original meaning of the word and its relation to other words. Enter word embedding and Word2Vec.</p>
<h2>Word embedding and Word2Vec</h2>
<p>Word embedding involves creating better vector representations of words &#8211; both in terms of efficiency and maintaining meaning. For instance, a word embedding layer may involve creating a 10,000 x 300 sized matrix, whereby we look up a 300 length vector representation for each of the 10,000 words in our vocabulary.  This new, 300 length vector is obviously a lot more efficient than a 10,000 length one-hot representation.  But we also need to create this 300 length vector in such a way as to preserve some semblance of the meaning of the word.</p>
<p>Word2Vec does this by taking the <em>context </em>of words surrounding the <em>target </em>word.  So, if we have a context window of 2, the context of the <em>target</em> word &#8220;sat&#8221; in the sentence &#8220;the cat sat on the mat&#8221; is the list of words [&#8220;the&#8221;, &#8220;cat&#8221;, &#8220;on&#8221;, &#8220;the&#8221;]. In Word2Vec, the meaning of a word is roughly translatable to context &#8211; and it basically works. Target words which share similar common context words often have similar meanings. The way Word2Vec trains the embedding vectors is via a neural network of sorts &#8211; the neural network, given a one-hot representation of a <em>target</em> word, tries to predict the most likely context words.  For an introduction to neural networks, see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">this tutorial</a>.</p>
<p>Here&#8217;s a naive way of performing the neural network training using an output softmax layer:</p>
<figure id="attachment_395" style="width: 676px" class="wp-caption alignnone"><img class="size-full wp-image-395" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg" alt="gensim word embedding softmax trainer" width="676" height="425" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg 676w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax-300x189.jpg 300w" sizes="(max-width: 676px) 100vw, 676px" /><figcaption class="wp-caption-text">A word embedding softmax trainer</figcaption></figure>
<p>In this network, the 300 node hidden layer weights are training by trying to predict (via a softmax output layer) genuine, high probability context words.  Once the training is complete, the output softmax layer is discarded and what is of real value is the 10,000 x 300 weight matrix connecting the input to the hidden layer. This is our embedding matrix, and we can look up any member of our 10,000-word vocabulary and get it&#8217;s 300 length vector representation.</p>
<p>It turns out that this softmax way of training the embedding layer is very inefficient, due to the millions of weights that need to be involved in updating and calculating the softmax values. Therefore, a concept called <em>negative sampling </em>is used in the real Word2Vec, which involves training the layer with real context words and a few <em>negative samples</em> which are chosen randomly from outside the context.  For more details on this, see my <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">Word2Vec Keras tutorial</a>.</p>
<p>Now we understand what Word2Vec training of embedding layers involves, let&#8217;s talk about the gensim Word2Vec module.</p>
<h2>A gensim Word2Vec tutorial</h2>
<figure id="attachment_532" style="width: 1139px" class="wp-caption alignnone"><img class="size-full wp-image-532" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output.jpg" alt="gensim Word2Vec - nearest words" width="1139" height="327" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output.jpg 1139w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-300x86.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-768x220.jpg 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Closest-words-output-1024x294.jpg 1024w" sizes="(max-width: 1139px) 100vw, 1139px" /><figcaption class="wp-caption-text">Nearest words by cosine similarity</figcaption></figure>
<p>This section will give a brief introduction to the gensim Word2Vec module.  The gensim library is an open-source Python library that specializes in vector space and topic modeling.  It can be made very fast with the use of the Cython Python model, which allows C code to be run inside the Python environment. This is good for our purposes, as the <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">original Google Word2Vec implementation</a> is written in C, and gensim has a wrapper for this code, which will be explained below.</p>
<p>For this tutorial, we are going to use the <em>text8</em> corpus sourced from <a href="http://mattmahoney.net/dc/" target="_blank" rel="noopener">here</a> for our text data. All the code for this tutorial can be found on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>.</p>
<p>First off, we need to download the <em>text8.zip</em> file (if required) and extract it:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">url = &#039;http://mattmahoney.net/dc/&#039;
filename = maybe_download(&#039;text8.zip&#039;, url, 31344016)
root_path = &quot;C:\\Users\Andy\PycharmProjects\\adventures-in-ml-code\\&quot;
if not os.path.exists((root_path + filename).strip(&#039;.zip&#039;)):
    zipfile.ZipFile(root_path+filename).extractall()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This is all fairly straightforward Python file handling, downloading and zip file manipulation, so I won&#8217;t go into it here.</p>
<p>The next step that is required is to create an iterator for gensim to extract its data from.  We can cheat a little bit here and use a supplied iterator that gensim provides for the <em>text8</em> corpus:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">sentences = word2vec.Text8Corpus((root_path + filename).strip(&#039;.zip&#039;))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The required input to the gensim Word2Vec module is an <a href="http://pymbook.readthedocs.io/en/latest/igd.html" target="_blank" rel="noopener">iterator object</a>, which sequentially supplies sentences from which gensim will train the embedding layer. The line above shows the supplied gensim iterator for the <em>text8</em> corpus, but below shows another generic form that could be used in its place for a different data set (not actually implemented in the code for this tutorial), where the data set also contains multiple files:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname
 
    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This capability of gensim is great, as it means you can setup iterators which cycle through the data without having to load the entire data set into memory.  This is vital, as some text data sets are huge  i.e. tens of GB.</p>
<p>After we&#8217;ve setup the iterator object, it is dead simple to train our word vectors:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">logging.basicConfig(format=&#039;%(asctime)s : %(levelname)s : %(message)s&#039;, level=logging.INFO)
model = word2vec.Word2Vec(sentences, iter=10, min_count=10, size=300, workers=4)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line just lets us see the INFO logging that gensim provides as it trains. The second line will execute the training on the provided <em>sentences</em> iterator.  The first optional argument <em>iter</em> specifies how many times the training code will run through the data set to train the neural network (kind of like the number of training epochs). The gensim training code will actually run through all the data <em>iter+1</em> time, as the first pass involves collecting all the unique words, creating dictionaries etc.  The next argument,<em> min_count,</em> specifies the minimum amount of times that the word has to appear in the corpus before it is included in the vocabulary &#8211; this allows us to easily eliminate rare words and reduce our vocabulary size.  The third argument is the size of the resultant word vector &#8211; in this case, we set it to 300. In other words, each word in our vocabulary, after training, will be represented by a 300 length word vector. Finally, if we are using Cython, we can specify how many parallel workers we would like to work on the data &#8211; this will speed up the training process. There are <a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">lots of other arguments</a>, but these are the main ones to consider.</p>
<p>Let&#8217;s examine our results and see what else gensim can do.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># get the word vector of &quot;the&quot;
print(model.wv[&#039;the&#039;])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This returns a 300 length numpy vector &#8211; as you can see, each word vector can be retrieved from the model via a dictionary key i.e. a word within our vocabulary.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># get the most common words
print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The word vectors are also arranged within the <em>wv</em> object with indexes &#8211; the lowest index (i.e. 0) represents the most common word, the highest (i.e. the length of the vocabulary minus 1) the least common word.  The above code returns: &#8220;the of and&#8221;, which is unsurprising, as these are very common words.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># get the least common words
vocab_size = len(model.wv.vocab)
print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2], model.wv.index2word[vocab_size - 3])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The discovered vocabulary is found in <em>model.wv.vocab</em> &#8211; by taking the length of this dictionary, we can determine the vocabulary size (in this case, it is 47,134 elements long). The code above returns: &#8220;zanetti markschies absentia&#8221; &#8211; rare words indeed.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># find the index of the 2nd most common word (&quot;of&quot;)
print(&#039;Index of &quot;of&quot; is: {}&#039;.format(model.wv.vocab[&#039;of&#039;].index))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can also go the other way i.e. retrieve the index of a word we supply.  In this case, we are getting the index of the second most common word &#8220;of&#8221;. As expected the above code returns &#8220;Index of &#8220;of&#8221; is: 1&#8243;.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># some similarity fun
print(model.wv.similarity(&#039;woman&#039;, &#039;man&#039;), model.wv.similarity(&#039;man&#039;, &#039;elephant&#039;))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can also easily extract similarity measures between word vectors (gensim uses <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">cosine similarity</a>). The above code returns &#8220;0.6599 0.2955&#8221;, which again makes sense given the context such words are generally used in.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># what doesn&#039;t fit?
print(model.wv.doesnt_match(&quot;green blue red zebra&quot;.split()))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This fun function determines which word doesn&#8217;t match the context of the others &#8211; in this case, &#8220;zebra&#8221; is returned.</p>
<p>We also want to able to convert our data set from a list of words to a list of integer indexes, based on the vocabulary developed by gensim.  To do so, we can use the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># convert the input data into a list of integer indexes aligning with the wv indexes
# Read the data into a list of strings.
def read_data(filename):
    &quot;&quot;&quot;Extract the first file enclosed in a zip file as a list of words.&quot;&quot;&quot;
    with zipfile.ZipFile(filename) as f:
        data = f.read(f.namelist()[0]).split()
    return data

def convert_data_to_index(string_data, wv):
    index_data = []
    for word in string_data:
        if word in wv:
            index_data.append(wv.vocab[word].index)
    return index_data

str_data = read_data(root_path + filename)
index_data = convert_data_to_index(str_data, model.wv)
print(str_data[:4], index_data[:4])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first function, <em>read_data</em> simply extracts the zip file data and returns a list of strings in the same order as our original text data set.  The second function loops through each word in the data set, determines if it is in the vocabulary*, and if so, adds the matching integer index to a list.  The code above returns: &#8220;[&#8216;anarchism&#8217;, &#8216;originated&#8217;, &#8216;as&#8217;, &#8216;a&#8217;] [5237, 3080, 11, 5]&#8221;.</p>
<p>* Remember that some words in the data set will be missing from the vocabulary if they are very rare in the corpus.</p>
<p>We can also save and reload our trained word vectors/embeddings by the following simple code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># save and reload the model
model.save(root_path + &quot;mymodel&quot;)
model = gensim.models.Word2Vec.load(root_path + &quot;mymodel&quot;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Finally, I&#8217;ll show you how we can extract the embedding weights from the gensim Word2Vec embedding layer and store it in a numpy array, ready for use in TensorFlow and Keras.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># convert the wv word vectors into a numpy matrix that is suitable for insertion
# into our TensorFlow and Keras models
embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))
for i in range(len(model.wv.vocab)):
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this case, we first create an appropriately sized numpy zeros array.  Then we loop through each word in the vocabulary, grabbing the word vector associated with that word by using the <em>wv</em> dictionary.  We then add the word vector into our numpy array.</p>
<p>So there we have it &#8211; gensim Word2Vec is a great little library that can execute the word embedding process very quickly, and also has a host of other useful functionality.</p>
<p>Now I will show how you can use pre-trained gensim embedding layers in our TensorFlow and Keras models.</p>
<h1>Using gensim Word2Vec embeddings in TensorFlow</h1>
<p>For this application, we&#8217;ll setup a dummy TensorFlow network with an embedding layer and measure the similarity between some words.  If you&#8217;re not up to speed with TensorFlow, I suggest you check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a> or this online course <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.772462&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdata-science-deep-learning-in-theano-tensorflow%2F" target="new">Data Science: Practical Deep Learning in Theano + TensorFlow</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.772462&amp;type=2&amp;subid=0" width="1" height="1" border="0" />.  Also, it&#8217;s probably a good idea to check out my <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec TensorFlow tutorial</a> to understand how the embedding layer works.</p>
<p>The first step is to select some random words from the top 100 most common words in our text data set.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The last line saves the array of 16 random words into a TensorFlow constant <em>valid_dataset.</em></p>
<p>For the next step, we take the embedding matrix from our gensim Word2Vec simulation and &#8220;implant it&#8221; into a TensorFlow variable which we use as our embedding layer.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># embedding layer weights are frozen to avoid updating embeddings while training
saved_embeddings = tf.constant(embedding_matrix)
embedding = tf.Variable(initial_value=saved_embeddings, trainable=False)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that in the second line above for the TensorFlow variable declaration, I&#8217;ve set the <em>trainable</em> argument to <em>False. </em>If we were using this layer in, say, training a recurrent neural network, if we didn&#8217;t set this argument to <em>False</em> our embedding layer would be trained in TensorFlow with negative performance impacts. It&#8217;s probably not an overall bad strategy, i.e. starting with a gensim embedding matrix and then training further using something like a recurrent NN, but if you want your embedding layer fixed for performance reasons, you need to set <em>trainable</em> to <em>False</em>.</p>
<p>The next chunk of code calculates the similarity between each of the word vectors using the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">cosine similarity</a> measure. It is explained more fully in my <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec TensorFlow tutorial</a>, but basically it calculates the norm of all the embedding vectors, then performs a dot product between the validation words and all other word vectors.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the cosine similarity operations
norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))
normalized_embeddings = embedding / norm
valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we can run our TensorFlow session and sort the eight words which are closest to our validation example words.  Again, this code is explained in more detail in the previously mentioned tutorial.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Add variable initializer.
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    # call our similarity operation
    sim = similarity.eval()
    # run through each valid example, finding closest words
    for i in range(valid_size):
        valid_word = wv.index2word[i]
        top_k = 8  # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = &#039;Nearest to %s:&#039; % valid_word
            for k in range(top_k):
            close_word = wv.index2word[nearest[k]]
            log_str = &#039;%s %s,&#039; % (log_str, close_word)
        print(log_str)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code will produce lines like:</p>
<blockquote><p>Nearest to two: three, five, zero, four, six, one, seven, eight</p></blockquote>
<p>As you can see, our Word2Vec embeddings produced by gensim have the expected results &#8211; in this example, we have number words being grouped together in similarity which makes sense.</p>
<p>Next up, let&#8217;s see how we can use the gensim Word2Vec embeddings in Keras.</p>
<h1>Using gensim Word2Vec embeddings in Keras</h1>
<p>We can perform similar steps with a Keras model. In this case, following the example code previously shown in <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">the Keras Word2Vec tutorial</a>, our model takes two single word samples as input and finds the similarity between them.  The top 8 closest words loop is therefore slightly different than the previous example:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)
# input words - in this case we do sample by sample evaluations of the similarity
valid_word = Input((1,), dtype=&#039;int32&#039;)
other_word = Input((1,), dtype=&#039;int32&#039;)
# setup the embedding layer
embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],
                      weights=[embedding_matrix])
embedded_a = embeddings(valid_word)
embedded_b = embeddings(other_word)
similarity = merge([embedded_a, embedded_b], mode=&#039;cos&#039;, dot_axes=2)
# create the Keras model
k_model = Model(input=[valid_word, other_word], output=similarity)

def get_sim(valid_word_idx, vocab_size):
    sim = np.zeros((vocab_size,))
    in_arr1 = np.zeros((1,))
        in_arr2 = np.zeros((1,))
    in_arr1[0,] = valid_word_idx
    for i in range(vocab_size):
        in_arr2[0,] = i
        out = k_model.predict_on_batch([in_arr1, in_arr2])
        sim[i] = out
    return sim

# now run the model and get the closest words to the valid examples
for i in range(valid_size):
    valid_word = wv.index2word[valid_examples[i]]
    top_k = 8  # number of nearest neighbors
    sim = get_sim(valid_examples[i], len(wv.vocab))
    nearest = (-sim).argsort()[1:top_k + 1]
    log_str = &#039;Nearest to %s:&#039; % valid_word
    for k in range(top_k):
        close_word = wv.index2word[nearest[k]]
        log_str = &#039;%s %s,&#039; % (log_str, close_word)
    print(log_str)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As you can see when I setup the <em>embeddings</em> layer (using Keras&#8217; dedicated <em>Embedding()</em> layer), all we need to do is specify the input and output dimensions (vocabulary size and embedding vector length, respectively) and then assign the gensim <em>embedding_matrix </em>to the <em>weights</em> argument. All the remaining logic is a copy from the <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">Keras Word2Vec tutorial</a>, so check that post out for more details.</p>
<p>The code produces lines like:</p>
<blockquote><p>Nearest to when: unless, if, where, whenever, then, before, once, finally</p></blockquote>
<p>Here we can see that <a href="https://en.wikipedia.org/wiki/Conjunction_(grammar)" target="_blank" rel="noopener">subordinating conjunction</a> word types have been grouped together &#8211; which is a good, expected result.</p>
<p>So that wraps up the tutorial &#8211; in this post, I&#8217;ve shown you how to use gensim to create Word2Vec word embeddings in a quick and efficient fashion.  I then gave an overview of how to &#8220;upload&#8221; these learned embeddings into TensorFlow and Keras.  I hope it has been helpful.</p>
<p>&nbsp;</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, check out this inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/">Python gensim Word2Vec tutorial with TensorFlow and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>A Word2Vec Keras tutorial</title>
		<link>http://adventuresinmachinelearning.com/word2vec-keras-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/word2vec-keras-tutorial/#comments</comments>
		<pubDate>Wed, 30 Aug 2017 11:08:40 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Keras]]></category>
		<category><![CDATA[NLP]]></category>
		<category><![CDATA[Word2Vec]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=496</guid>
		<description><![CDATA[<p>Understanding Word2Vec word embedding is a critical component in your machine learning journey.  Word embedding is a necessary step in performing efficient natural language processing <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" title="A Word2Vec Keras tutorial">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/">A Word2Vec Keras tutorial</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Understanding Word2Vec word embedding is a critical component in your machine learning journey.  Word embedding is a necessary step in performing efficient natural language processing in your machine learning models.  This tutorial will show you how to perform Word2Vec word embeddings in the Keras deep learning framework &#8211; to get an introduction to Keras, check out <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">my tutorial</a> (or the recommended course below).  In a <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">previous post</a>, I introduced Word2Vec implementations in TensorFlow.  In that tutorial, I showed how using a naive, softmax-based word embedding training regime results in an extremely slow training of our embedding layer when we have large word vocabularies.  To get around this problem, a technique called &#8220;negative sampling&#8221; has been proposed, and a custom loss function has been created in TensorFlow to allow this (<em>nce_loss</em>).</p>
<p>Unfortunately, this loss function doesn&#8217;t exist in Keras, so in this tutorial, we are going to implement it ourselves.  This is a fortunate omission, as implementing it ourselves will help us to understand how negative sampling works and therefore better understand the Word2Vec Keras process.</p>
<hr />
<p><strong>Recommended online courses: </strong>If you&#8217;d like to dig deeper into Keras via a video course, check out this inexpensive online Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1140660&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fzero-to-deep-learning%2F" target="new">Zero to Deep Learning with Python and Keras</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1140660&amp;type=2&amp;subid=0" width="1" height="1" border="0" />.  Also, if you&#8217;d like to do a video course in natural language processing concepts, check out this Udemy course:  <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Word embedding</h1>
<p>If we have a document or documents that we are using to try to train some sort of natural language machine learning system (i.e. a chatbot), we need to create a vocabulary of the most common words in that document.  This vocabulary can be greater than 10,000 words in length in some instances. To represent a word to our machine learning model, a naive way would be to use a one-hot vector representation i.e. a 10,000-word vector full of zeros except for one element, representing our word, which is set to 1.  However, this is an inefficient way of doing things &#8211; a 10,000-word vector is an unwieldy object to train with.  Another issue is that these one-hot vectors hold no information about the meaning of the word, how it is used in language and what is its usual context (i.e. what other words it generally appears close to).</p>
<p>Enter word embeddings &#8211; word embeddings try to &#8220;compress&#8221; large one-hot word vectors into much smaller vectors (a few hundred elements) which preserve some of the meaning and context of the word. Word2Vec is the most common process of word embedding and will be explained below.</p>
<h2>Context, Word2Vec and the skip-gram model</h2>
<p>The context of the word is the key measure of meaning that is utilized in Word2Vec.  The context of the word &#8220;sat&#8221; in the sentence &#8220;the cat sat on the mat&#8221; is (&#8220;the&#8221;, &#8220;cat&#8221;, &#8220;on&#8221;, &#8220;the&#8221;, &#8220;mat&#8221;).  In other words, it is the words which commonly occur around the <em>target </em>word &#8220;sat&#8221;. Words which have similar contexts share meaning under Word2Vec, and their reduced vector representations will be similar.  In the skip-gram model version of Word2Vec (more on this later), the goal is to take a <em>target</em> word i.e. &#8220;sat&#8221; and predict the surrounding context words.  This involves an iterative learning process.</p>
<p>The end product of this learning will be an embedding layer in a network &#8211; this embedding layer is a kind of lookup table &#8211; the rows are vector representations of each word in our vocabulary.  Here&#8217;s a simplified example (using dummy values) of what this looks like, where <em>vocabulary_size=7 </em>and <em>embedding_size=3</em>:</p>
<p>\begin{equation}<br />
\begin{array}{c|c c c}<br />
anarchism &amp; 0.5 &amp; 0.1 &amp; -0.1\\<br />
originated &amp; -0.5 &amp; 0.3 &amp; 0.9 \\<br />
as &amp; 0.3 &amp; -0.5 &amp; -0.3 \\<br />
a &amp; 0.7 &amp; 0.2 &amp; -0.3\\<br />
term &amp; 0.8 &amp; 0.1 &amp; -0.1 \\<br />
of &amp; 0.4 &amp; -0.6 &amp; -0.1 \\<br />
abuse &amp; 0.7 &amp; 0.1 &amp; -0.4<br />
\end{array}<br />
\end{equation}</p>
<p>As you can see, each word (row) is represented by a vector of size 3.  Learning this embedding layer/lookup table can be performed using a simple neural network and an output softmax layer &#8211; see the diagram below:</p>
<figure id="attachment_395" style="width: 604px" class="wp-caption aligncenter"><img class=" wp-image-395" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg" alt="Word2Vec softmax trainer" width="604" height="380" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg 676w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax-300x189.jpg 300w" sizes="(max-width: 604px) 100vw, 604px" /><figcaption class="wp-caption-text">A softmax trainer for word embedding</figcaption></figure>
<p>The idea of the neural network above is to supply our input <em>target</em> words as one-hot vectors.  Then, via a hidden layer, we want to train the neural network to increase the probability of valid context words, while decreasing the probability of invalid context words (i.e. words that never show up in the surrounding context of the target words).  This involves using a softmax function on the output layer.  Once training is complete, the output layer is discarded, and our embedding vectors are the weights of the hidden layer.</p>
<p>There are two variants of the Word2Vec paradigm &#8211; skip-gram and CBOW.  The skip-gram variant takes a target word and tries to predict the surrounding context words, while the CBOW (continuous bag of words) variant takes a set of context words and tries to predict a target word.  In this case, we will be considering the skip-gram variant (for more details &#8211; see <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">this tutorial</a>).</p>
<h2>The softmax issue and negative sampling</h2>
<p>The problem with using a full softmax output layer is that it is very computationally expensive.  Consider the definition of the softmax function:</p>
<p>$$P(y = j \mid x) = \frac{e^{x^T w_j}}{\sum_{k=1}^K e^{x^T w_k}}$$</p>
<p>Here the probability of the output being class <em>j</em> is calculated by multiplying the output of the hidden layer and the weights connecting to the class <em>j</em> output on the numerator and dividing it by the same product but over <em>all the remaining weights</em>.  When the output is a 10,000-word one-hot vector, we are talking millions of weights that need to be updated in any gradient based training of the output layer.  This gets seriously time-consuming and inefficient, as demonstrated in my <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">TensorFlow Word2Vec tutorial</a>.</p>
<p>There&#8217;s another solution called negative sampling.  It is described in the <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener">original Word2Vec paper</a> by Mikolov et al.  It works by reinforcing the strength of weights which link a target word to its context words, but rather than reducing the value of <em>all</em> those weights which aren&#8217;t in the context, it simply samples a small number of them &#8211; these are called the &#8220;negative samples&#8221;.</p>
<p>To train the embedding layer using negative samples in Keras, we can re-imagine the way we train our network.  Instead of constructing our network so that the output layer is a multi-class softmax layer, we can change it into a simple binary classifier.  For words that are in the context of the target word, we want our network to output a 1, and for our negative samples, we want our network to output a 0. Therefore, the output layer of our Word2Vec Keras network is simply a single node with a sigmoid activation function.</p>
<p>We also need a way of ensuring that, as the network trains, words which are similar end up having similar embedding vectors.  Therefore, we want to ensure that the trained network will always output a 1 when it is supplied words which are in the same context, but 0 when it is supplied words which are never in the same context. Therefore, we need a vector similarity score supplied to the output sigmoid layer &#8211; with similar vectors outputting a high score and un-similar vectors outputting a low score.  The most typical similarity measure used between two vectors is the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">cosine similarity</a> score:</p>
<p>$$similarity = cos(\theta) = \frac{\textbf{A}\cdot\textbf{B}}{\parallel\textbf{A}\parallel_2 \parallel \textbf{B} \parallel_2}$$</p>
<p>The denominator of this measure acts to normalize the result &#8211; the real similarity operation is on the numerator: the <a href="https://en.wikipedia.org/wiki/Dot_product" target="_blank" rel="noopener">dot product</a> between vectors<strong> <em>A </em></strong>and <strong><em>B</em></strong>.  In other words, to get a simple, non-normalized measure of similarity between two vectors, you simply apply a dot product operation between them.</p>
<p>So with all that in mind, our new negative sampling network for the planned Word2Vec Keras implementation features:</p>
<ul>
<li>An (integer) input of a target word and a real or negative context word</li>
<li>An embedding layer lookup (i.e. looking up the integer index of the word in the embedding matrix to get the word vector)</li>
<li>The application of a dot product operation</li>
<li>The output sigmoid layer</li>
</ul>
<p>This architecture of this implementation looks like:</p>
<figure id="attachment_505" style="width: 931px" class="wp-caption alignnone"><img class="size-full wp-image-505" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/08/Negative-sampling-architecture-1.jpg" alt="Word2Vec Keras - negative sampling architecture" width="931" height="211" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/08/Negative-sampling-architecture-1.jpg 931w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/08/Negative-sampling-architecture-1-300x68.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/08/Negative-sampling-architecture-1-768x174.jpg 768w" sizes="(max-width: 931px) 100vw, 931px" /><figcaption class="wp-caption-text">Word2Vec Keras &#8211; negative sampling architecture</figcaption></figure>
<p>Let&#8217;s go through this architecture more carefully.  First, each of the words in our vocabulary is assigned an integer index between 0 and the size of our vocabulary (in this case, 10,000).  We pass two words into the network, one the target word and the other either a word from the surrounding context or a negative sample.  We &#8220;look up&#8221; these indexes as the rows of our embedding layer (10,000 x 300 weight tensor) to retrieve our 300 length word vectors.  We then perform a dot product operation between these vectors to get the similarity.  Finally, we output the similarity to a sigmoid layer to give us a 1 or 0 indicator which we can match with the label given to the Context word (1 for a true context word, 0 for a negative sample).</p>
<p>The back-propagation of our errors will work to update the embedding layer to ensure that words which are truly similar to each other (i.e. share contexts) have vectors such that they return high similarity scores. Let&#8217;s now implement this architecture in Keras and we can test whether this turns out to be the case.</p>
<h1>A Word2Vec Keras implementation</h1>
<p>This section will show you how to create your own Word2Vec Keras implementation &#8211; the code is hosted on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>.</p>
<h2>Data extraction</h2>
<p>To develop our Word2Vec Keras implementation, we first need some data.  As in my <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec TensorFlow tutorial</a>, we&#8217;ll be using a document data set from <a href="http://mattmahoney.net/dc/" target="_blank" rel="noopener">here</a>.  To extract the information, I&#8217;ll be using some of the same text extraction functions from the aforementioned <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec tutorial</a>, in particular, the <em>collect_data</em> function &#8211; check out that tutorial for further details.  Basically, the function calls other functions which download the data, then a function that converts the text data into a string of integers &#8211; with each word in the vocabulary represented by a unique integer.  To call this function, we run:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">vocab_size = 10000
data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first 7 words in the dataset are:</p>
[&#8216;anarchism&#8217;, &#8216;originated&#8217;, &#8216;as&#8217;, &#8216;a&#8217;, &#8216;term&#8217;, &#8216;of&#8217;, &#8216;abuse&#8217;]
<p>After running collect_data, the new representation of these words (<em>data</em>) is:</p>
[5239, 3082, 12, 6, 195, 2, 3134]
<p>There are also two dictionaries returned from <em>collect_data &#8211; </em>the first where you can look up a word and get its integer representation, and the second the reverse i.e. you look up a word&#8217;s integer and you get its actual English representation.</p>
<p>Next, we need to define some constants for the training and also create a validation set of words so we can check the learning progress of our word vectors.</p>
<h2>Constants and the validation set</h2>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">window_size = 3
vector_dim = 300
epochs = 1000000

valid_size = 16     # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first constant, <em>window_size</em>, is the window of words around the target word that will be used to draw the context words from.  The second constant<em>, vector_dim</em>, is the size of each of our word embedding vectors &#8211; in this case, our embedding layer will be of size 10,000 x 300.  Finally, we have a large <em>epochs </em>variable &#8211; this designates the number of training iterations we are going to run.  Word embedding, even with negative sampling, can be a time-consuming process.</p>
<p>The next set of commands relate to the words we are going to check to see what other words grow in similarity to this validation set. During training, we will check which words begin to be deemed similar by the word embedding vectors and make sure these line up with our understanding of the meaning of these words.  In this case, we will select 16 words to check, and pick these words randomly from the top 100 most common words in the data-set (<em>collect_data </em>has assigned the most common words in the data set integers in ascending order i.e. the most common word is assigned 1, the next most common 2, etc.).</p>
<p>Next, we are going to look at a handy function in Keras which does all the skip-gram / context processing for us.</p>
<h2>The skip-gram function in Keras</h2>
<p>To train our data set using negative sampling and the skip-gram method, we need to create data samples for both valid context words and for negative samples. This involves scanning through the data set and picking target words, then randomly selecting context words from within the window of words around the target word (i.e. if the target word is &#8220;on&#8221; from &#8220;the cat sat on the mat&#8221;, with a window size of 2 the words &#8220;cat&#8221;, &#8220;sat&#8221;, &#8220;the&#8221;, &#8220;mat&#8221; could all be randomly selected as valid context words).  It also involves randomly selecting negative samples outside of the selected target word context. Finally, we also need to set a label of 1 or 0, depending on whether the supplied context word is a true context word or a negative sample.  Thankfully, Keras has a function (<em>skipgrams</em>) which does all that for us &#8211; consider the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">sampling_table = sequence.make_sampling_table(vocab_size)
couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype=&quot;int32&quot;)
word_context = np.array(word_context, dtype=&quot;int32&quot;)

print(couples[:10], labels[:10])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Ignoring the first line for the moment (<em>make_sampling_table)</em>, the Keras <em>skipgrams</em> function does exactly what we want of it &#8211; it returns the word couples in the form of (<em>target, context</em>) and also gives a matching label of 1 or 0 depending on whether <em>context</em> is a true context word or a negative sample. By default, it returns randomly shuffled <em>couples</em> and <em>labels</em>.  In the code above, we then split the <em>couples </em>tuple into separate <em>word_target </em>and <em>word_context</em> variables and make sure they are the right type.  The print function produces the following instructive output:</p>
<p><em>couples</em>:</p>
[[6503, 5], [187, 6754], [1154, 3870], [670, 1450], [4554, 1], [1037, 250], [734, 4521], [1398, 7], [4495, 3374], [2881, 8637]]
<p><em>labels</em>:</p>
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
<p>The <em>make_sampling_table()</em> operation creates a table that <em>skipgrams </em>uses to ensure it produces negative samples in a balanced manner and not just the most common words.  The <em>skipgrams</em> operation by default selects the same amount of negative samples as it does true context words.</p>
<p>We&#8217;ll feed the produced arrays (<em>word_target, word_context</em>) into our Keras model later &#8211; now onto the Word2Vec Keras model itself.</p>
<h2>The Keras functional API and the embedding layers</h2>
<p>In this Word2Vec Keras implementation, we&#8217;ll be using the Keras <a href="https://keras.io/getting-started/functional-api-guide/" target="_blank" rel="noopener">functional API</a>.  In my <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">previous Keras tutorial</a>, I used the Keras sequential layer framework. This sequential layer framework allows the developer to easily bolt together layers, with the tensor outputs from each layer flowing easily and implicitly into the next layer.  In this case, we are going to do some things which are a little tricky &#8211; the sharing of a single embedding layer between two tensors, and an auxiliary output to measure similarity &#8211; and therefore we can&#8217;t use a straightforward sequential implementation.</p>
<p>Thankfully, the functional API is also pretty easy to use.  I&#8217;ll introduce it as we move through the code. The first thing we need to do is specify the structure of our model, as per the architecture diagram which I have shown above. As an initial step, we&#8217;ll create our input variables and embedding layer:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create some input variables
input_target = Input((1,))
input_context = Input((1,))

embedding = Embedding(vocab_size, vector_dim, input_length=1, name=&#039;embedding&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First off, we need to specify what tensors are going to be input to our model, along with their size. In this case, we are just going to supply individual target and context words, so the input size for each input variable is simply (1,).  Next, we create an embedding layer, which Keras already has specified as a layer for us &#8211; Embedding().  The first argument to this layer definition is the number of rows of our embedding layer &#8211; which is the size of our vocabulary (10,000).  The second is the size of each word&#8217;s embedding vector (the columns) &#8211; in this case, 300. We also specify the input length to the layer &#8211; in this case, it matches our input variables i.e. 1.  Finally, we give it a name, as we will want to access the weights of this layer after we&#8217;ve trained it, and we can easily access the layer weights using the name.</p>
<p>The weights for this layer are initialized automatically, but you can also specify an optional <em>embeddings_initializer </em>argument whereby you supply a <a href="https://keras.io/initializers/" target="_blank" rel="noopener">Keras initializer object</a>.  Next, as per our architecture, we need to look up an embedding vector (length = 300) for our target and context words, by supplying the embedding layer with the word&#8217;s unique integer value:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">target = embedding(input_target)
target = Reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = Reshape((vector_dim, 1))(context)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As can be observed in the code above, the embedding vector is easily retrieved by supplying the word integer (i.e. <em>input_target </em>and <em>input_context</em>) in brackets to the previously created <em>embedding</em> operation/layer. For each word vector, we then use a Keras <em>Reshape</em> layer to reshape it ready for our upcoming dot product and similarity operation, as per our architecture.</p>
<p>The next layer involves calculating our cosine similarity between the supplied word vectors:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># setup a cosine similarity operation which will be output in a secondary model
similarity = merge([target, context], mode=&#039;cos&#039;, dot_axes=0)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As can be observed, Keras supplies a <em>merge </em>operation with a <em>mode </em>argument which we can set to &#8216;cos&#8217; &#8211; this is the cosine similarity between the two word vectors, <em>target</em>, and <em>context.</em> This <em>similarity</em> operation will be returned via the output of a secondary model &#8211; but more on how this is performed later.</p>
<p>The next step is to continue on with our primary model architecture, and the dot product as our measure of similarity which we are going to use in the primary flow of the negative sampling architecture:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># now perform the dot product operation to get a similarity measure
dot_product = merge([target, context], mode=&#039;dot&#039;, dot_axes=1)
dot_product = Reshape((1,))(dot_product)
# add the sigmoid output layer
output = Dense(1, activation=&#039;sigmoid&#039;)(dot_product)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Again, we use the Keras <em>merge</em> operation and apply it to our <em>target</em> and <em>context</em> word vectors, with the <em>mode</em> argument set to &#8216;dot&#8217; to get the simple dot product.  We then do another Reshape layer, and take the reshaped dot product value (a single data point/scalar) and apply it to a Keras <em>Dense</em> layer, with the activation function of the layer set to &#8216;sigmoid&#8217;.  This is the output of our Word2Vec Keras architecture.</p>
<p>Next, we need to gather everything into a Keras model and compile it, ready for training:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the primary training model
model = Model(input=[input_target, input_context], output=output)
model.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;rmsprop&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here, we create the functional API based model for our Word2Vec Keras architecture.  What the model definition requires is a specification of the input arrays to the model (these need to be <a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a> arrays) and an output tensor &#8211; these are supplied as per the previously explained architecture.  We then compile the model, by supplying a loss function that we are going to use (in this case, binary cross entropy i.e. cross entropy when the labels are either 0 or 1) and an optimizer (in this case, <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="noopener">rmsprop</a>).  The loss function is applied to the <em>output</em> variable.</p>
<p>The question now is, if we want to use the <em>similarity </em>operation which we defined in the architecture to allow us to check on how things are progressing during training, how do we access it? We could output it via the <em>model</em> definition (i.e. <em>output</em>=[<em>similarity</em>, <em>output</em>]) but then Keras would be trying to apply the loss function and the optimizer to this value during training and this isn&#8217;t what we created the operation for.</p>
<p>There is another way, which is quite handy &#8211; we create another model:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create a secondary validation model to run our similarity checks during training
validation_model = Model(input=[input_target, input_context], output=similarity)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can now use this <em>validation_model</em> to access the <em>similarity </em>operation, and this model will actually <em>share</em> the embedding layer with the primary model.  Note, because this model won&#8217;t be involved in training, we don&#8217;t have to run a Keras <em>compile</em> operation on it.</p>
<p>Now we are ready to train the model &#8211; but first, let&#8217;s setup a function to print out the words with the closest similarity to our validation examples (<em>valid_examples</em>).</p>
<h2>The similarity callback</h2>
<p>We want to create a &#8220;callback&#8221; which we can use to figure out which words are closest in similarity to our validation examples, so we can monitor the training progress of our embedding layer.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class SimilarityCallback:
    def run_sim(self):
        for i in range(valid_size):
            valid_word = reverse_dictionary[valid_examples[i]]
            top_k = 8  # number of nearest neighbors
            sim = self._get_sim(valid_examples[i])
            nearest = (-sim).argsort()[1:top_k + 1]
            log_str = &#039;Nearest to %s:&#039; % valid_word
            for k in range(top_k):
                close_word = reverse_dictionary[nearest[k]]
                log_str = &#039;%s %s,&#039; % (log_str, close_word)
            print(log_str)

    @staticmethod
    def _get_sim(valid_word_idx):
        sim = np.zeros((vocab_size,))
        in_arr1 = np.zeros((1,))
        in_arr2 = np.zeros((1,))
        for i in range(vocab_size):
            in_arr1[0,] = valid_word_idx
            in_arr2[0,] = i
            out = validation_model.predict_on_batch([in_arr1, in_arr2])
            sim[i] = out
        return sim
sim_cb = SimilarityCallback()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This class runs through all the <em>valid_examples </em>and gets the similarity score between the given validation word and all the other words in the vocabulary.  It gets the similarity score by running <em>_get_sim</em>(), which features a loop which runs through each word in the vocabulary, and runs a <em>predict_on_batch</em>() operation on the validation model &#8211; this basically looks up the embedding vectors for the two supplied words (the <em>valid_example</em> and the looped vocabulary example) and returns the <em>similarity </em>operation result.  The main loop then sorts the similarity in descending order and creates a string to print out the top 8 words with the closest similarity to the validation example.</p>
<p>The output of this callback will be seen during our training loop, which is presented below.</p>
<h2>The training loop</h2>
<p>The main training loop of the model is:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">arr_1 = np.zeros((1,))
arr_2 = np.zeros((1,))
arr_3 = np.zeros((1,))
for cnt in range(epochs):
    idx = np.random.randint(0, len(labels)-1)
    arr_1[0,] = word_target[idx]
    arr_2[0,] = word_context[idx]
    arr_3[0,] = labels[idx]
    loss = model.train_on_batch([arr_1, arr_2], arr_3)
    if i % 100 == 0:
        print(&quot;Iteration {}, loss={}&quot;.format(cnt, loss))
    if cnt % 10000 == 0:
        sim_cb.run_sim()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this loop, we run through the total number of epochs.  First, we select a random index from our <em>word_target, word_context </em>and <em>labels</em> arrays and place the values in dummy numpy arrays.  Then we supply the input ([<em>word_target, word_context</em>]) and outputs (<em>labels</em>) to the primary model and run a <em>train_on_batch</em>() operation.  This returns the current loss evaluation, <em>loss,</em> of the model and prints it. Every 10,000 iterations we also run functions in the SimilarityCallback.</p>
<p>Here are some of the word similarity outputs for the validation example word &#8220;eight&#8221; as we progress through the training iterations:</p>
<p><strong>Iterations = 0:</strong></p>
<p>Nearest to eight: much, holocaust, representations, density, fire, senators, dirty, fc</p>
<p><strong>Iterations = 50,000:</strong></p>
<p>Nearest to eight: six, finest, championships, mathematical, floor, pg, smoke, recurring</p>
<p><strong>Iterations = 200,000:</strong></p>
<p>Nearest to eight: six, five, two, one, nine, seven, three, four</p>
<p>As can be observed, at the start of the training, all sorts of random words are associated with &#8220;six&#8221;.  However, as the training iterations increase, slowly other word numbers are associated with &#8220;six&#8221; until finally all of the closest 8 words are number words.</p>
<p>There you have it &#8211; in this Word2Vec Keras tutorial, I&#8217;ve shown you how the Word2Vec methodology works with negative sampling, and how to implement it in Keras using its functional API.  In the <a href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" target="_blank" rel="noopener">next tutorial</a>, I will show you how to reload trained embedding weights into both Keras and TensorFlow. You can also checkout how embedding layers work in LSTM networks in <a href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/">this tutorial</a>.</p>
<p>&nbsp;</p>
<hr />
<p><strong>Recommended online courses: </strong>If you&#8217;d like to dig deeper into Keras via a video course, check out this inexpensive online Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1140660&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fzero-to-deep-learning%2F" target="new">Zero to Deep Learning with Python and Keras</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1140660&amp;type=2&amp;subid=0" width="1" height="1" border="0" />.  Also, if you&#8217;d like to do a video course in natural language processing concepts, check out this Udemy course:  <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" />.</p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/">A Word2Vec Keras tutorial</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/word2vec-keras-tutorial/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Word2Vec word embedding tutorial in Python and TensorFlow</title>
		<link>http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/</link>
		<comments>http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/#comments</comments>
		<pubDate>Fri, 21 Jul 2017 23:45:40 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[NLP]]></category>
		<category><![CDATA[TensorFlow]]></category>
		<category><![CDATA[Word2Vec]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=392</guid>
		<description><![CDATA[<p>In coming tutorials on this blog I will be dealing with how to create deep learning models that predict text sequences.  However, before we get <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" title="Word2Vec word embedding tutorial in Python and TensorFlow">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/">Word2Vec word embedding tutorial in Python and TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In coming tutorials on this blog I will be dealing with how to create deep learning models that predict text sequences.  However, before we get to that point we have to understand some key Natural Language Processing (NLP) ideas.  One of the key ideas in NLP is how we can efficiently convert words into numeric vectors which can then be &#8220;fed into&#8221; various machine learning models to perform predictions.  The current key technique to do this is called &#8220;Word2Vec&#8221; and this is what will be covered in this tutorial.  After discussing the relevant background material, we will be implementing Word2Vec embedding using TensorFlow (which makes our lives a lot easier).  To get up to speed in TensorFlow, check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a>. Also, if you prefer Keras &#8211; check out my <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">Word2Vec Keras tutorial</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, check out this inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Why do we need Word2Vec?</h1>
<p>If we want to feed words into machine learning models, unless we are using tree based methods, we need to convert the words into some set of numeric vectors.  A straight-forward way of doing this would be to use a &#8220;one-hot&#8221; method of converting the word into a sparse representation with only one element of the vector set to 1, the rest being zero.  This is the same method we use for classification tasks &#8211; see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/#setting-up-output" target="_blank" rel="noopener">this tutorial</a>.</p>
<p>So, for the sentence &#8220;the cat sat on the mat&#8221; we would have the following vector representation:</p>
<p>\begin{equation}<br />
\begin{pmatrix}<br />
the \\<br />
cat \\<br />
sat \\<br />
on \\<br />
the \\<br />
mat \\<br />
\end{pmatrix}<br />
=<br />
\begin{pmatrix}<br />
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\<br />
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\<br />
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\<br />
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\<br />
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\<br />
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br />
\end{pmatrix}<br />
\end{equation}</p>
<p>Here we have transformed a six word sentence into a 6&#215;5 matrix, with the 5 being the size of the <em>vocabulary </em>(&#8220;the&#8221; is repeated).  In practical applications, however, we will want machine and deep learning models to learn from gigantic vocabularies i.e. 10,000 words plus.  You can begin to see the efficiency issue of using &#8220;one hot&#8221; representations of the words &#8211; the input layer into any neural network attempting to model such a vocabulary would have to be at least 10,000 nodes.  Not only that, this method strips away any local context of the words &#8211; in other words, it strips away information about words which commonly appear close together in sentences (or between sentences).</p>
<p>For instance, we might expect to see &#8220;United&#8221; and &#8220;States&#8221; to appear close together, or &#8220;Soviet&#8221; and &#8220;Union&#8221;.  Or &#8220;food&#8221; and &#8220;eat&#8221;, and so on.  This method loses all such information, which, if we are trying to model natural language, is a large omission.  Therefore, we need an efficient representation of the text data which also conserves information about local word context.  This is where the Word2Vec methodology comes in.</p>
<h1>The Word2Vec methodology</h1>
<p>As mentioned previously, there is two components to the Word2Vec methodology.  The first is the mapping of a high dimensional one-hot style representation of words to a lower dimensional vector. This might involve transforming a 10,000 columned matrix into a 300 columned matrix, for instance. This process is called word embedding.  The second goal is to do this while still maintaining word context and therefore, to some extent, meaning. One approach to achieving these two goals in the Word2Vec methodology is by taking an input word and then attempting to estimate the probability of other words appearing close to that word.  This is called the skip-gram approach.  The alternative method, called Continuous Bag Of Words (CBOW), does the opposite &#8211; it takes some context words as input and tries to find the single word that has the highest probability of fitting that context.  In this tutorial, we will concentrate on the skip-gram method.</p>
<p>What&#8217;s a gram?  A gram is a group of <em>n </em>words, where <em>n</em> is the gram window size.  So for the sentence &#8220;The cat sat on the mat&#8221;, a 3-gram representation of this sentence would be &#8220;The cat sat&#8221;, &#8220;cat sat on&#8221;, &#8220;sat on the&#8221;, &#8220;on the mat&#8221;.  The &#8220;skip&#8221; part refers to the number of times an input word is repeated in the data-set with different context words (more on this later).  These grams are fed into the Word2Vec context prediction system. For instance, assume the input word is &#8220;cat&#8221; &#8211; the Word2Vec tries to predict the context (&#8220;the&#8221;, &#8220;sat&#8221;) from this supplied input word.  The Word2Vec system will move through all the supplied grams and input words and attempt to learn appropriate mapping vectors (embeddings) which produce high probabilities for the right context given the input words.</p>
<p>What is this Word2Vec prediction system?  Nothing other than a neural network.</p>
<h2>The softmax Word2Vec method</h2>
<p>Consider the diagram below &#8211; in this case we&#8217;ll assume the sentence &#8220;The cat sat on the mat&#8221; is part of a much larger text database, with a very large vocabulary &#8211; say 10,000 words in length.  We want to reduce this to a 300 length embedding.</p>
<figure id="attachment_395" style="width: 501px" class="wp-caption aligncenter"><img class=" wp-image-395" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg" alt="Word2Vec softmax trainer" width="501" height="315" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg 676w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax-300x189.jpg 300w" sizes="(max-width: 501px) 100vw, 501px" /><figcaption class="wp-caption-text">A Word2Vec softmax trainer</figcaption></figure>
<p>With respect to the diagram above, if we take the word &#8220;cat&#8221; it will be one of the words in the 10,000 word vocabulary.  Therefore we can represent it as a 10,000 length one-hot vector.  We then interface this input vector to a 300 node hidden layer (if you need to scrub up on neural networks, see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">this tutorial</a>).  The weights connecting this layer will be our new word vectors &#8211; more on this soon.  The activations of the nodes in this hidden layer are simply linear summations of the weighted inputs (i.e. no non-linear activation, like a sigmoid or tanh, is applied).  These nodes are then fed into a softmax output layer.  During training, we want to change the weights of this neural network so that words surrounding &#8220;cat&#8221; have a higher probability in the softmax output layer.  So, for instance, if our text data set has a lot of Dr Seuss books, we would want our network to assign large probabilities to words like &#8220;the&#8221;, &#8220;sat&#8221; and &#8220;on&#8221; (given lots of sentences like &#8220;the cat sat on the mat&#8221;).</p>
<p>By training this network, we would be creating a 10,000 x 300 weight matrix connecting the 10,000 length input with the 300 node hidden layer.  Each row in this matrix corresponds to a word in our 10,000 word vocabulary &#8211; so we have effectively reduced 10,000 length one-hot vector representations of our words to 300 length vectors.  The weight matrix essentially becomes a look-up or encoding table of our words.  Not only that, but these weight values contain context information due to the way we&#8217;ve trained our network.  Once we&#8217;ve trained the network, we abandon the softmax layer and just use the 10,000 x 300 weight matrix as our word embedding lookup table.</p>
<p>What does this look like in code?</p>
<h2>The softmax Word2Vec method in TensorFlow</h2>
<p>As with any machine learning problem, there are two components &#8211; the first is getting all the data into a usable format, and the next is actually performing the training, validation and testing.  First I&#8217;ll go through how the data can be gathered into a usable format, then we&#8217;ll talk about the TensorFlow graph of the model.  Note that the code that I will be going through can be found in its entirety at this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>.  In this case, the code is mostly based on the TensorFlow Word2Vec tutorial <a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/word2vec/word2vec_basic.py" target="_blank" rel="noopener">here</a> with some personal changes.</p>
<h3>Preparing the text data</h3>
<p>The previously mentioned TensorFlow tutorial has a few functions that take a text database and transform it so that we can extract input words and their associated grams in mini-batches for training the Word2Vec system / embeddings (if you&#8217;re not sure what &#8220;mini-batch&#8221; means, check out <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener">this tutorial</a>).  I&#8217;ll briefly talk about each of these functions in turn:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def maybe_download(filename, url, expected_bytes):
    &quot;&quot;&quot;Download a file if not present, and make sure it&#039;s the right size.&quot;&quot;&quot;
    if not os.path.exists(filename):
        filename, _ = urllib.request.urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
        print(&#039;Found and verified&#039;, filename)
    else:
        print(statinfo.st_size)
        raise Exception(
            &#039;Failed to verify &#039; + filename + &#039;. Can you get to it with a browser?&#039;)
    return filename</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function checks to see if the <em>filename</em> already has been downloaded from the supplied <em>url</em>.  If not, it uses the urllib.request Python module which retrieves a file from the given <em>url</em> argument, and downloads the file into the local code directory.  If the file already exists (i.e. <em>os.path.exists(filename)</em> returns true), then the function does not try to download the file again.  Next, the function checks the size of the file and makes sure it lines up with the expected file size, <em>expected_bytes</em>.  If all is well, it returns the <em>filename </em>object which can be used to extract the data from.  To call the function with the data-set we are using in this example, we execute the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">url = &#039;http://mattmahoney.net/dc/&#039;
filename = maybe_download(&#039;text8.zip&#039;, url, 31344016)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The next thing we have to do is take the <em>filename</em> object, which points to the downloaded file, and extract the data using the Python <em>zipfile</em> module.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Read the data into a list of strings.
def read_data(filename):
    &quot;&quot;&quot;Extract the first file enclosed in a zip file as a list of words.&quot;&quot;&quot;
    with zipfile.ZipFile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Using <em>zipfile.ZipFile()</em> to extract the zipped file, we can then use the reader functionality found in this <em>zipfile</em> module.  First, the <em>namelist()</em> function retrieves all the members of the archive &#8211; in this case there is only one member, so we access this using the zero index.  Then we use the <em>read()</em> function which reads all the text in the file and pass this through the TensorFlow function <em>as_str</em> which ensures that the text is created as a string data-type.  Finally, we use <em>split()</em> function to create a list with all the words in the text file, separated by white-space characters.  We can see some of the output here:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">vocabulary = read_data(filename)
print(vocabulary[:7])
[&#039;anarchism&#039;, &#039;originated&#039;, &#039;as&#039;, &#039;a&#039;, &#039;term&#039;, &#039;of&#039;, &#039;abuse&#039;]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As you can observe, the returned vocabulary data contains a list of plain English words, ordered as they are in the sentences of the original extracted text file.  Now that we have all the words extracted in a list, we have to do some further processing to enable us to create our skip-gram batch data.  These further steps are:</p>
<ol>
<li>Extract the top 10,000 most common words to include in our embedding vector</li>
<li>Gather together all the unique words and index them with a unique integer value &#8211; this is what is required to create an equivalent one-hot type input for the word.  We&#8217;ll use a dictionary to do this</li>
<li>Loop through every word in the dataset (<em>vocabulary</em> variable) and assign it to the unique integer word identified, created in Step 2 above.  This will allow easy lookup / processing of the word data stream</li>
</ol>
<p>The function which performs all this magic is shown below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def build_dataset(words, n_words):
    &quot;&quot;&quot;Process raw inputs into a dataset.&quot;&quot;&quot;
    count = [[&#039;UNK&#039;, -1]]
    count.extend(collections.Counter(words).most_common(n_words - 1))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary[&#039;UNK&#039;]
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reversed_dictionary</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step is setting up a &#8220;counter&#8221; list, which will store the number of times a word is found within the data-set.  Because we are restricting our vocabulary to only 10,000 words, any words not within the top 10,000 most common words will be marked with an &#8220;UNK&#8221; designation, standing for &#8220;unknown&#8221;.  The initialized <em>count</em> list is then extended, using the Python collections module and the <em>Counter() </em>class and the associated <em>most_common()</em> function.  These count the number of words in the given argument (<em>words</em>) and then returns the <em>n </em>most common words in a list format.</p>
<p>The next part of this function creates a dictionary, called <em>dictionary</em> which is populated by keys corresponding to each unique word.  The value assigned to each unique word key is simply an increasing integer count of the size of the dictionary.  So, for instance, the most common word will receive the value 1, the second most common the value 2, the third most common word the value 3, and so on (the integer 0 is assigned to the &#8216;UNK&#8217; words).   This step creates a unique integer value for each word within the vocabulary &#8211; accomplishing the second step of the process which was defined above.</p>
<p>Next, the function loops through each <em>word</em> in our full <em>words </em>data set &#8211; the data set which was output from the read_data() function.  A list called <em>data</em> is created, which will be the same length as <em>words</em> but instead of being a list of individual words, it will instead be a list of integers &#8211; with each word now being represented by the unique integer that was assigned to this word in <em>dictionary.  </em>So, for the first sentence of our data-set [&#8216;anarchism&#8217;, &#8216;originated&#8217;, &#8216;as&#8217;, &#8216;a&#8217;, &#8216;term&#8217;, &#8216;of&#8217;, &#8216;abuse&#8217;], now looks like this in the <em>data</em> variable: [5242, 3083, 12, 6, 195, 2, 3136].  This part of the function addresses step 3 in the list above.</p>
<p>Finally, the function creates a dictionary called <em>reverse_dictionary</em> that allows us to look up a word based on its unique integer identifier, rather than looking up the identifier based on the word i.e. the original <em>dictionary.  </em></p>
<p>The final aspect of setting up our data is now to create a data set comprising of our input words and associated grams, which can be used to train our Word2Vec embedding system.  The code to do this is:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">data_index = 0
# generate batch data
def generate_batch(data, batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # input word at the center of the buffer
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word
            context[i * num_skips + j, 0] = buffer[target]  # these are the context words
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    # Backtrack a little bit to avoid skipping words in the end of a batch
    data_index = (data_index + len(data) - span) % len(data)
    return batch, context</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function will generate mini-batches to use during our training (again, see <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener">here</a> for information on mini-batch training).  These batches will consist of input words (stored in <em>batch</em>) and random associated context words within the gram as the labels to predict (stored in <i>context</i>).  For instance, in the 5-gram &#8220;the cat sat on the&#8221;, the input word will be center word i.e. &#8220;sat&#8221; and the context words that will be predicted will be drawn randomly from the remaining words of the gram: [&#8216;the&#8217;, &#8216;cat&#8217;, &#8216;on&#8217;, &#8216;the&#8217;].  In this function, the number of words drawn randomly from the surrounding context is defined by the argument <em>num_skips</em>.  The size of the window of context words to draw from around the input word is defined in the argument <em>skip_window &#8211; </em>in the example above (&#8220;the cat sat on the&#8221;), we have a skip window width of 2 around the input word &#8220;sat&#8221;.</p>
<p>In the function above, first the batch and label outputs are defined as variables of size <em>batch_size</em>.  Then the span size is defined, which is basically the size of the word list that the input word and context samples will be drawn from.  In the example sub-sentence above &#8220;the cat sat on the&#8221;, the span is 5 = 2 x skip window + 1.  After this a buffer is created:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">buffer = collections.deque(maxlen=span)
for _ in range(span):
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This buffer will hold a maximum of <em>span</em> elements and will be a kind of moving window of words that samples are drawn from.  Whenever a new word index is added to the buffer, the left most element will drop out of the buffer to allow room for the new word index being added.  The position of the buffer in the input text stream is stored in a global variable <em>data_index</em> which is incremented each time a new word is added to the buffer.  If it gets to the end of the text stream, the &#8220;% len(data)&#8221; component of the index update will basically reset the count back to zero.</p>
<p>The code below fills out the batch and context variables: <!--?prettify linenums=true?--></p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">for i in range(batch_size // num_skips):
    target = skip_window  # input word at the center of the buffer
    targets_to_avoid = [skip_window]
    for j in range(num_skips):
        while target in targets_to_avoid:
            target = random.randint(0, span - 1)
        targets_to_avoid.append(target)
        batch[i * num_skips + j] = buffer[skip_window]  # this is the input word
        context[i * num_skips + j, 0] = buffer[target]  # these are the context words
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first &#8220;target&#8221; word selected is the word at the center of the span of words and is therefore the input word.  Then other words are randomly selected from the span of words, making sure that the input word is not selected as part of the context, and each context word is unique.  The <em>batch</em> variable will feature repeated input words (<em>buffer[skip_window]</em>) which are matched with each context word in <em>context</em>.</p>
<p>The <em>batch </em>and <em>context</em> variables are then returned &#8211; and now we have a means of drawing batches of data from the data set.  We are now in a position to create our Word2Vec training code in TensorFlow.  However, before we get to that, we&#8217;ll first create a validation data-set that we can use to test how our model is doing.  We do that by measuring the vectors closest together in vector-space, and make sure these words indeed are similar using our knowledge of English.  This will be discussed more in the next section.  However, for now, the code below shows how to grab some random validation words from the most common words in our vocabulary:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent.
valid_size = 16     # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The code above randomly chooses 16 integers from 0-100 &#8211; this corresponds to the integer indexes of the most common 100 words in our text data.  These will be the words we examine to assess how our learning is progressing in associating related words together in the vector-space.  Now, onto creating the TensorFlow model.</p>
<h3>Creating the TensorFlow model</h3>
<p>For a refresher on TensorFlow, check out <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">this tutorial</a>.  Below I will step through the process of creating our Word2Vec word embeddings in TensorFlow.  What does this involve?  Simply, we need to setup the neural network which I previously presented, with a word embedding matrix acting as the hidden layer and an output softmax layer in TensorFlow.  By training this model, we&#8217;ll be learning the best word embedding matrix and therefore we&#8217;ll be learning a reduced, context maintaining, mapping of words to vectors.</p>
<p>The first thing to do is set-up some variables which we&#8217;ll use later on in the code &#8211; the purposes of these variables will become clear as we progress:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1       # How many words to consider left and right.
num_skips = 2         # How many times to reuse an input to generate a context.</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next we setup some TensorFlow placeholders that will hold our input words (their integer indexes) and context words which we are trying to predict.  We also need to create a constant to hold our validation set indexes in TensorFlow:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, we need to setup the embedding matrix variable / tensor &#8211; this is straight-forward using the TensorFlow <em>embedding_lookup() </em>function, which I&#8217;ll explain shortly:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Look up embeddings for inputs.
embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
embed = tf.nn.embedding_lookup(embeddings, train_inputs)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step in the code above is to create the embeddings variable, which is effectively the weights of the connections to the linear hidden layer.  We initialize the variable with a random uniform distribution between -1.0 to 1.0.  The size of this variable is (<em>vocabulary_size, </em><em>embedding_size</em>) &#8211; the <em>vocabulary_size </em>is the 10,000 words that we have used to setup our data in the previous section.  This is basically our one-hot vector input, where the only element with a value of &#8220;1&#8221; is the current input word, all the other values are set to &#8220;0&#8221;.  The second dimension, <em>embedding_size,</em> is our hidden layer size, and is the length of our new, smaller, representation of our words.  We can also think of this tensor as a big lookup table &#8211; the rows are each word in our vocabulary, and the columns are our new vector representation of each of these words.  Here&#8217;s a simplified example (using dummy values), where <em>vocabulary_size=7 </em>and <em>embedding_size=3</em>:</p>
<p>\begin{equation}<br />
\begin{array}{c|c c c}<br />
anarchism &amp; 0.5 &amp; 0.1 &amp; -0.1\\<br />
originated &amp; -0.5 &amp; 0.3 &amp; 0.9 \\<br />
as &amp; 0.3 &amp; -0.5 &amp; -0.3 \\<br />
a &amp; 0.7 &amp; 0.2 &amp; -0.3\\<br />
term &amp; 0.8 &amp; 0.1 &amp; -0.1 \\<br />
of &amp; 0.4 &amp; -0.6 &amp; -0.1 \\<br />
abuse &amp; 0.7 &amp; 0.1 &amp; -0.4<br />
\end{array}<br />
\end{equation}</p>
<p>As can be observed, &#8220;anarchism&#8221; (which would actually be represented by a unique integer or one-hot vector) is now expressed as [0.5, 0.1, -0.1].  We can &#8220;look up&#8221; anarchism by finding its integer index and searching the rows of <em>embeddings</em> to find the embedding vector: [0.5, 0.1, -0.1].</p>
<p>The next line in the code involves the <em>tf.nn.embedding_lookup() </em>function, which is a useful helper function in TensorFlow for this type of task.  Here&#8217;s how it works &#8211; it takes an input vector of integer indexes &#8211; in this case our <em>train_input</em> tensor of training input words, and &#8220;looks up&#8221; these indexes in the supplied embeddings tensor.  Therefore, this command will return the current embedding vector for each of the supplied input words in the training batch.  The full embedding tensor will be optimized during the training process.</p>
<p>Next we have to create some weights and bias values to connect the output softmax layer, and perform the appropriate multiplication and addition.  This looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Construct the variables for the softmax
weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],
                          stddev=1.0 / math.sqrt(embedding_size)))
biases = tf.Variable(tf.zeros([vocabulary_size]))
hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The weight variable, as it is connecting the hidden layer and the output layer, is of size (<em>out_layer_size, hidden_layer_size) = (vocabulary_size, embedding_size)</em>.  The biases, as usual, will only be single dimensional and the size of the output layer.  We then multiply the embedded variable (<em>embed</em>) by the weights and add the bias.  Now we are ready to create a softmax operation and we will use cross entropy loss to optimize the weights, biases and embeddings of the model.  To do this easily, we will use the TensorFlow function <em>softmax_cross_entropy_with_logits()</em>.  However, to use this function we first have to convert the context words / integer indices into one-hot vectors.  The code below performs both of these steps, and also adds a gradient descent optimization operation:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># convert train_context to a one-hot format
train_one_hot = tf.one_hot(train_context, vocabulary_size)
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, 
    labels=train_one_hot))
# Construct the SGD optimizer using a learning rate of 1.0.
optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, we need to perform our similarity assessments to check on how the model is performing as it trains.  To determine which words are similar to each other, we need to perform some sort of operation that measures the &#8220;distances&#8221; between the various word embedding vectors for the different words.  In this case, we will use the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">cosine similarity</a> measure of distance between vectors.  It is defined as:</p>
<p>$$similarity = cos(\theta) = \frac{\textbf{A}\cdot\textbf{B}}{\parallel\textbf{A}\parallel_2 \parallel \textbf{B} \parallel_2}$$</p>
<p>Here the bolded <strong><em>A</em></strong> and <strong><em>B</em></strong> are the two vectors that we are measuring the similarity between.  The double parallel lines with the <em>2</em> subscript ($\parallel\textbf{A}\parallel_2$) refers to the L2 norm of the vector.  To get the L2 norm of a vector, you square every dimension of the vector (in this case <em>n</em><em>=300, </em>the width of our embedding vector), sum up the squared elements then take the square root of the product i.e.:</p>
<p>$$\sqrt{\sum_{i=1}^n A_{i}^2}$$</p>
<p>The best way to calculate the cosine similarity in TensorFlow is to normalize each vector like so:</p>
<p>$$\frac{\textbf{A}}{\parallel\textbf{A}\parallel_2}$$</p>
<p>Then we can simply multiply these normalized vectors together to get the cosine similarity.  We will multiply the validation vectors/words that were discussed earlier with all of the words in our embedding vector, then we can sort in descending order to get those words most similar to our validation words.</p>
<p>First, we calculate the L2 norm of each vector using the <em>tf.square()</em>, <em>tf.reduce_sum()</em> and <em>tf.sqrt()</em> functions to calculate the square, summation and square root of the norm, respectively:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Compute the cosine similarity between minibatch examples and all embeddings.
norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
normalized_embeddings = embeddings / norm</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we can look up our validation words / vectors using the <em>tf.nn.embedding_lookup() </em>that we discussed earlier:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As before, we are supplying a list of integers (that correspond to our validation vocabulary words) to the <em>embedding_lookup() </em>function, which looks up these rows in the <em>normalized_embeddings </em>tensor, and returns the subset of <em>validation</em> normalized embeddings.  Now that we have the normalized validation tensor, <em>valid_embeddings</em>, we can multiply this by the full normalized vocabulary (<em>normalized_embedding</em>) to finalize our similarity calculation:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">similarity = tf.matmul(
      valid_embeddings, normalized_embeddings, transpose_b=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This operation will return a (<em>validation_size, vocabulary_size</em>) sized tensor, where each row refers to one of our validation words and the columns refer to the similarity between the validation word and all the other words in the vocabulary.</p>
<h3>Running the TensorFlow model</h3>
<p>The code below initializes the variables and feeds in each data batch to the training loop, printing the average loss every 2000 iterations.  If this code doesn&#8217;t make sense to you, check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a>.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">with tf.Session(graph=graph) as session:
  # We must initialize all variables before we use them.
  init.run()
  print(&#039;Initialized&#039;)

  average_loss = 0
  for step in range(num_steps):
    batch_inputs, batch_context = generate_batch(data,
        batch_size, num_skips, skip_window)
    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}

    # We perform one update step by evaluating the optimizer op (including it
    # in the list of returned values for session.run()
    _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
      if step &gt; 0:
        average_loss /= 2000
      # The average loss is an estimate of the loss over the last 2000 batches.
      print(&#039;Average loss at step &#039;, step, &#039;: &#039;, average_loss)
      average_loss = 0</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, we want to print out the words which are most similar to our validation words &#8211; we do this by calling the similarity operation we defined above and sorting the results (note, this is only performed every 10,000 iterations as it is computationally expensive):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Note that this is expensive (~20% slowdown if computed every 500 steps)
if step % 10000 == 0:
    sim = similarity.eval()
    for i in range(valid_size):
        valid_word = reverse_dictionary[valid_examples[i]]
        top_k = 8  # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = &#039;Nearest to %s:&#039; % valid_word
        for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log_str = &#039;%s %s,&#039; % (log_str, close_word)
        print(log_str)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function first evaluates the similarity operation, which returns an array of cosine similarity values for each of the validation words.  Then we iterate through each of the validation words, taking the top 8 closest words by using argsort() on the negative of the similarity to arrange the values in descending order.  The code then prints out these 8 closest words so we can monitor how the embedding process is performing.</p>
<p>Finally, after all the training iterations are finished, we can assign the final embeddings to a separate tensor for use later (most likely in some sort of other deep learning or machine learning process):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">final_embeddings = normalized_embeddings.eval()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>So now we&#8217;re done &#8211; or are we?  The code for this softmax method of Word2Vec is on this site&#8217;s Github repository &#8211; you could try running it, but I wouldn&#8217;t recommend it.  Why?  Because it is seriously slow.</p>
<h2>Speeding things up &#8211; the &#8220;true&#8221; Word2Vec method</h2>
<p>The fact is, performing softmax evaluations and updating the weights over a 10,000 word output/vocabulary is really slow.  Why&#8217;s that?  Consider the softmax definition:</p>
<p>$$P(y = j \mid x) = \frac{e^{x^T w_j}}{\sum_{k=1}^K e^{x^T w_k}}$$</p>
<p>In the context of what we are working on, the softmax function will predict what words have the highest probability of being in the context of the input word.  To determine that probability however, the denominator of the softmax function has to evaluate <em>all </em>the possible context words in the vocabulary.  Therefore, we need 300 x 10,000 = 3M weights, all of which need to be trained for the softmax output.  This slows things down.</p>
<p>There is an alternative, faster scheme called <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Noise Contrastive Estimation (NCE)</a>.  Instead of taking the probability of the context word compared to <em>all </em>of the possible context words in the vocabulary, this method randomly samples 2-20 possible context words and evaluates the probability only from these.  I won&#8217;t go into the nitty gritty details here, but suffice to say that this method has been shown to perform well and drastically speeds up the training process.</p>
<p>TensorFlow has helped us out here, and has supplied an NCE loss function that we can use called <em>tf.nn.nce_loss()</em> which we can supply weight and bias variables to.  Using this function, the time to perform 100 training iterations reduced from 25 seconds with the softmax method to less than 1 second using the NCE method.  An awesome improvement!  We replace the softmax lines with the following in our code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Construct the variables for the NCE loss
nce_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

nce_loss = tf.reduce_mean(
        tf.nn.nce_loss(weights=nce_weights,
                       biases=nce_biases,
                       labels=train_context,
                       inputs=embed,
                       num_sampled=num_sampled,
                       num_classes=vocabulary_size))

optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we are good to run the code.  You can get the full code <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">here</a>.  As discussed, every 10,000 iterations the code outputs the validation words and the words that the Word2Vec system deems are similar.  Below, you can see the improvement for some selected validation words between the random initialization and at the 50,000 iteration mark:</p>
<p>At the beginning:</p>
<blockquote><p>Nearest to nine: heterosexual, scholarly, scandal, serves, humor, realized, cave, himself</p>
<p>Nearest to this: contains, alter, numerous, harmonica, nickname, ghana, bogart, marxist</p></blockquote>
<p>After 10,000 iterations:</p>
<blockquote><p>Nearest to nine: zero, one, and, coke, in, UNK, the, jpg</p>
<p>Nearest to this: the, a, UNK, killing, meter, afghanistan, ada, indiana</p></blockquote>
<p>Finally after 50,000 iterations:</p>
<blockquote><p>Nearest to nine: eight, one, zero, seven, six, two, five, three</p>
<p>Nearest to this: that, the, a, UNK, one, it, he, an</p></blockquote>
<p>By examining the outputs above, we can first see that the word &#8220;nine&#8221; becomes increasingly associated with other number words (&#8220;eight&#8221;, &#8220;one&#8221;, &#8220;seven&#8221; etc.).  This makes sense. The word &#8220;this&#8221;, which acts as a pronoun and definite article in sentences, becomes associated with other pronouns (&#8220;he&#8221;, &#8220;it&#8221;) and other definite articles (&#8220;the&#8221;, &#8220;that&#8221;, etc.) the more iterations we run.</p>
<p>In summary then, we have learnt how to use the Word2Vec methodology to reduce large one-hot word vectors to much reduced word embedding vectors which preserve the context and meaning of the original words.  These word embedding vectors can then be used as a more efficient and effective input to deep learning techniques which aim to model natural language.  These techniques, such as recurrent neural networks, will be the subject of future posts.</p>
<hr />
<p><strong>Recommended online course: </strong>If you would like to learn more in a video format, check out this well rated and inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.918390&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fnatural-language-processing-with-deep-learning-in-python%2F" target="new">Natural Language Processing with Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.918390&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/">Word2Vec word embedding tutorial in Python and TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/feed/</wfw:commentRss>
		<slash:comments>18</slash:comments>
		</item>
	</channel>
</rss>
