<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Reinforcement learning &#8211; Adventures in Machine Learning</title>
	<atom:link href="http://adventuresinmachinelearning.com/category/reinforcement-learning/feed/" rel="self" type="application/rss+xml" />
	<link>http://adventuresinmachinelearning.com</link>
	<description>Learn and explore machine learning</description>
	<lastBuildDate>Sun, 09 Sep 2018 07:53:16 +0000</lastBuildDate>
	<language>en-AU</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.8</generator>
	<item>
		<title>Reinforcement learning tutorial with TensorFlow</title>
		<link>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/</link>
		<comments>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/#comments</comments>
		<pubDate>Fri, 06 Jul 2018 01:16:59 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Reinforcement learning]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=959</guid>
		<description><![CDATA[<p>Reinforcement learning has gained significant attention with the relatively recent success of DeepMind&#8217;s AlphaGo system defeating the world champion Go player. The AlphaGo system was <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/" title="Reinforcement learning tutorial with TensorFlow">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/">Reinforcement learning tutorial with TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Reinforcement learning has gained significant attention with the relatively recent success of DeepMind&#8217;s AlphaGo system defeating the world champion Go player. The AlphaGo system was trained in part by reinforcement learning on deep neural networks. This type of learning is a different aspect of machine learning from the classical supervised and unsupervised paradigms. In reinforcement learning using deep neural networks, the network reacts to environmental data (called the <em>state</em>) and controls the <em>actions</em> of an <em>agent</em> to attempt to maximize a <em>reward</em>. This process allows a network to learn to play <em>games, </em>such as Atari or other video games, or any other problem that can be recast as some form of game. In this tutorial, I&#8217;ll introduce the broad concepts of Q learning, a popular reinforcement learning paradigm, and I&#8217;ll show how to implement deep Q learning in TensorFlow. If you need to get up to speed in TensorFlow, check out <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">my introductory tutorial</a>.</p>
<hr />
<p><strong>Recommended online course &#8211; </strong>If you are more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1153742&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-reinforcement-learning-in-python%2F">Advanced AI: Deep Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1153742&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Introduction to reinforcement learning</h1>
<p>As stated above, reinforcement learning comprises of a few fundamental entities or concepts. They are: an <em>environment</em> which produces a <em>state</em> and <em>reward</em>, and an <em>agent </em>which performs <em>actions</em> in the given environment. This interaction can be seen in the diagram below:</p>
<figure id="attachment_768" style="width: 381px" class="wp-caption aligncenter"><img class="size-full wp-image-768" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png" alt="Reinforcement learning with Python and Keras - Reinforcement learning environment" width="381" height="211" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png 381w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment-300x166.png 300w" sizes="(max-width: 381px) 100vw, 381px" /><figcaption class="wp-caption-text">Reinforcement learning environment</figcaption></figure>
<p>The goal of the agent in such an environment is to examine the state and the reward information it receives, and choose an action which maximizes the reward feedback it receives.  The agent learns by repeated interaction with the environment, or, in other words, repeated playing of the game.</p>
<p>To be successful, the agent needs to:</p>
<ol>
<li>Learn the interaction between states, actions and subsequent rewards</li>
<li>Determine which is the best action to choose given (1)</li>
</ol>
<p>The implementation of (1) involves determining some set of <em>values</em> which can be used to inform (2), and (2) is called the action <em>policy</em>. One of the most common ways of implementing (1) and (2) using deep learning is via the Deep Q network and the <em>epsilon-greedy </em>policy. I&#8217;ll cover both of these concepts in the next two sections.</p>
<h2>Q learning</h2>
<p>Q learning is a value based method of supplying information to inform which action an agent should take. An initially intuitive idea of creating values upon which to base actions is to create a table which sums up the rewards of taking action <em>a </em>in state <em>s</em> over multiple game plays. This could keep track of which moves are the most advantageous. For instance, let&#8217;s consider a simple game which has 3 states and two possible actions in each state &#8211; the rewards for this game can be represented in a table:</p>
<figure id="attachment_961" style="width: 246px" class="wp-caption aligncenter"><img class=" wp-image-961" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-value-table-reinforcement-learning.png" alt="Reinforcement learning - simple state-action-reward table" width="246" height="109" /><figcaption class="wp-caption-text">Simple state-action-reward table</figcaption></figure>
<p>In the table above, you can see that for this simple game, when the agent is State 1 and takes Action 2, it will receive a reward of 10 but zero reward if it takes Action 1. In State 2, the situation is reversed and finally State 3 resembles State 1. If an agent randomly explored this game, and summed up which actions received the most reward in each of the three states (storing this information in an array, say), then it would basically learn the functional form of the table above.</p>
<p>In other words, if the agent simply chooses the action which it learnt had yielded the highest reward in the past (effectively learning some form of the table above) it would have learnt how to play the game successfully. Why do we need fancy concepts such as Q learning and neural networks then, when simply creating tables by summation is sufficient?</p>
<h3>Deferred reward</h3>
<p>Well, the first obvious answer is that the game above is clearly very simple, with only 3 states and 2 actions per state. Real games are significantly more complex. The other significant concept that is missing in the example above is the idea of <em>deferred reward</em>. To adequately play most realistic games, an agent needs to learn to be able to take actions which may not immediately lead to a reward, but may result in a large reward further down the track.</p>
<p>Consider another game, defined by the table below:</p>
<figure id="attachment_965" style="width: 273px" class="wp-caption aligncenter"><img class=" wp-image-965" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning.png" alt="Reinforcement learning with TensorFlow - simple delayed reward value table" width="273" height="138" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning.png 313w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning-300x151.png 300w" sizes="(max-width: 273px) 100vw, 273px" /><figcaption class="wp-caption-text">Simple delayed reward value table</figcaption></figure>
<p>In the game defined above, in all states, if Action 2 is taken, the agent moves back to State 1 i.e. it goes back to the beginning. In States 1 to 3, it also receives a reward of 5 when it does so. However, in all States 1 &#8211; 3, if Action 1 is taken, the agent moves forward to the next state, but doesn&#8217;t receive a reward until it reaches State 4 &#8211; at which point it receives a reward of 20. In other words, an agent is better off if it doesn&#8217;t take Action 2 to get an instantaneous reward of 5, but rather it should choose Action 1 consistently to progress through the states to get the reward of 20. The agent needs to be able to select actions which result in a <em>delayed </em><em>reward, </em>if the delayed reward value is sufficiently large.</p>
<h3>The Q learning rule</h3>
<p>This allows us to define the Q learning rule. In deep Q learning, the neural network needs to take the current state, <em>s</em>, as a variable and return a Q value for each possible action, <em>a</em>, in that state &#8211; i.e. it needs to return $Q(s,a)$ for all <em>s</em> and <em>a</em>. This $Q(s,a)$ needs to be updated in training via the following rule:</p>
<p>$$Q(s,a) \leftarrow  Q(s,a) + \alpha [r + \gamma \max_{a&#8217;} Q(s&#8217;, a&#8217;) &#8211; Q(s,a)]$$</p>
<p>This updating rule needs a bit of unpacking. First, you can see that the new value of $Q(s,a)$ involves updating it&#8217;s current value by adding on some extra bits on the right hand side of the equation above. Moving left to right, ignore the $\alpha$ for a bit. We see inside the square brackets the first term is <em>r</em> which stands for the reward that is received for taking action <em>a</em> in state <em>s</em>. This is the immediate reward, no delayed gratification is involved yet.</p>
<p>The next term is the delayed reward calculation. First, we have the $\gamma$ value which discounts the delayed reward impact &#8211; it is always between 0 and 1. More on that in a second. The next term $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$ is the maximum Q value possible in the next state. Let&#8217;s make that a bit clearer &#8211; the agent starts in state <em>s</em>, takes action <em>a, </em>ends up in state <em>s&#8217;</em> and then the code determines the maximum Q value in state <em>s&#8217;</em>  i.e. $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$.</p>
<p>So why is the value $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$ considered? It is considered because it represents the maximum future reward coming to the agent if it takes action <em>a </em>in state <em>s</em>. However, this value is discounted by $\gamma$ to take into account that it isn&#8217;t ideal for the agent to wait forever for a future reward &#8211; it is best for the agent to aim for the maximum award in the least period of time. Note that the value $Q(s&#8217;,a&#8217;)$ implicitly also holds the maximum discounted reward for the state after that, i.e. $Q(s&#8221;, a&#8221;)$ and likewise, it holds the discounted reward for the state $Q(s&#8221;&#8217;, a&#8221;&#8217;)$ and so on. This is how the agent can choose the action <em>a </em>based on not just the immediate reward <em>r</em>, but also based on possible future discounted rewards.</p>
<p>The final components in the formula above are the $\alpha$ value, which is the learning rate during the updating, and finally the current state, $Q(s,a)$, which is subtracted from the square bracket sum. This is done to normalize the updating. Both $\alpha$ and the $Q(s,a)$ subtraction are not required to be explicitly defined in deep Q learning, as the neural network will take care of that during its optimized learning process. This process will be discussed in the next section.</p>
<h3>Deep Q learning</h3>
<p>Deep Q learning applies the Q learning updating rule during the training process. In other words, a neural network is created which takes the state <em>s </em>as its input, and then the network is trained to output appropriate <em>Q(s,a)</em> values for each action in state <em>s</em>. The action <em>a </em>of the agent can then be chosen by taking the action with the greatest <em>Q(s,a)</em> value (by taking an <em>argmax</em> of the output of the neural network). This can be seen in the first step of the diagram below:</p>
<figure id="attachment_984" style="width: 1252px" class="wp-caption alignnone"><img class="size-full wp-image-984" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment.png" alt="Reinforcement learning TensorFlow - action and training steps" width="1252" height="675" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment.png 1252w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-300x162.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-768x414.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-1024x552.png 1024w" sizes="(max-width: 1252px) 100vw, 1252px" /><figcaption class="wp-caption-text">Action selecting and training steps &#8211; Deep Q learning</figcaption></figure>
<p>Once this step has been taken and an action has been selected, the agent can perform that action. The agent will then receive feedback on what reward is received by taking that action from that state. Now, the next step that we want to perform is to train the network according to the Q learning rule. This can be seen in the second part of the diagram above. The <em>x</em> input array for training the network is the state vector <em>s</em>, and the <em>y</em> output training sample is the <em>Q(s,a) </em>vector retrieved during the action selection step. However, one of the <em>Q(s,a)</em> values, corresponding to action <em>a</em>, is set to have a target of $r + \gamma Q(s&#8217;, a&#8217;)$ &#8211; this can be observed in the figure above.</p>
<p>By training the network in this way, the <em>Q(s,a)</em> output vector from the network will over time become better at informing the agent what action will be the best to select for its long term gain. There is a bit more to the story about action selection, however, which will be discussed in the next section.</p>
<h2>The epsilon-greedy policy</h2>
<p>In the explanation above, the action selection policy was simply the action which corresponded to the highest Q output from the neural network. However, this policy isn&#8217;t the most effective. Why is that? It is because, when the neural network is randomly initialized, it will be predisposed to select certain sub-optimal actions randomly. This may cause the agent to fall into sub-optimal behavior patterns without thoroughly exploring the game and action / reward space. As such, the agent won&#8217;t find the best strategies to play the game.</p>
<p>It is useful here to introduce two concepts &#8211; <em>exploration </em>and <em>exploitation. </em>At the beginning of an optimization problem, it is best to allow the problem space to be explored extensively in the hope of finding good local (or even global) minima. However, once the problem space has been adequately searched, it is now best for the optimization algorithm to focus on exploiting what it has found by converging on the best minima to arrive at a good solution.</p>
<p>Therefore, in reinforcement learning, it is best to allow some randomness in the action selection at the beginning of the training. This randomness is determined by the <em>epsilon</em> parameter. Essentially, a random number is drawn between 0 and 1, and if it is less than <em>epsilon, </em>then a random action is selection. If not, an action is selected based on the output of the neural network. The <em>epsilon </em>variable usually starts somewhere close to 1, and is slowly decayed to somewhere around 0 during training. This allows a large exploration of the game at the beginning, but then the decay of the <em>epsilon </em>value allows the network to zero in on a good solution.</p>
<p>We&#8217;re almost at the point where we can check out the game that will be used in this example, and begin to build our deep Q network. However, there is just one final important point to consider.</p>
<h2>Batching in reinforcement learning</h2>
<p>If a deep Q network is trained at each step in the game i.e. after each action is performed and the reward collected, there is a strong risk of over-fitting in the network. This is because game play is highly correlated i.e. if the game starts from the same place and the agent performs the same actions, there will likely be similar results each time (not exactly the same though, because of randomness in some games). Therefore, after each action it is a good idea to add all the data about the state, reward, action and the new state into some sort of <em>memory</em>. This memory can then be randomly sampled in batches to avoid the risk of over-fitting.</p>
<p>The network can therefore still be trained after each step if you desire (or less frequently, it&#8217;s up to the developer), but it is extracting the training data not from the agent&#8217;s ordered steps through the game, but rather a randomized memory of previous steps and outcomes that the agent has experienced. You&#8217;ll be able to see how this works in the code below.</p>
<p>We are now ready to examine the game/environment that we will develop our network to learn.</p>
<h1>The Mountain Car Environment and Open AI Gym</h1>
<p>In this reinforcement learning tutorial, the deep Q network that will be created will be trained on the Mountain Car environment/game. This can be accessed through the open source reinforcement learning library called <a href="https://gym.openai.com/" target="_blank" rel="noopener">Open AI Gym</a>. A screen capture from the rendered game can be observed below:</p>
<figure id="attachment_992" style="width: 383px" class="wp-caption aligncenter"><img class=" wp-image-992" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame.png" alt="Reinforcement learning TensorFlow - Mountain Car game" width="383" height="258" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame.png 895w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame-300x202.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame-768x517.png 768w" sizes="(max-width: 383px) 100vw, 383px" /><figcaption class="wp-caption-text">Mountain Car game</figcaption></figure>
<p>The object of this game is to get the car to go up the right-side hill to get to the flag. There&#8217;s one problem however, the car doesn&#8217;t have enough power to motor all the way up the hill. Instead, the car / agent needs to learn that it must motor up one hill for a bit, then accelerate down the hill and back up the other side, and repeat until it builds up enough momentum to make it to the top of the hill.</p>
<p>As stated above, Open AI Gym is an open source reinforcement learning package that allows developers to interact easily with games such as the Mountain Car environment. You can find details about the Mountain Car environment <a href="https://github.com/openai/gym/wiki/MountainCar-v0" target="_blank" rel="noopener">here</a>. Basically, the environment is represented by a two-element state vector, detailed below:</p>
<figure id="attachment_994" style="width: 302px" class="wp-caption aligncenter"><img class=" wp-image-994" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState.png" alt="Reinforcement learning - TensorFlow state vector" width="302" height="115" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState.png 462w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState-300x114.png 300w" sizes="(max-width: 302px) 100vw, 302px" /><figcaption class="wp-caption-text">Mountain Car state vector</figcaption></figure>
<p>As can be observed, the agent&#8217;s state is represented by the car&#8217;s position and velocity. The goal/flag is sitting at a position = 0.5. The actions available to the agent are shown below:</p>
<figure id="attachment_995" style="width: 188px" class="wp-caption aligncenter"><img class=" wp-image-995" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareActions.png" alt="Reinforcement learning TensorFlow - mountain car actions" width="188" height="154" /><figcaption class="wp-caption-text">Mountain Car actions</figcaption></figure>
<p>As can be observed, there are three actions available to the agent &#8211; accelerate to the left, right and no acceleration.</p>
<p>In the game&#8217;s default arrangement, for each time step where the car&#8217;s position is &lt;0.5, it receives a reward of -1, up to a maximum of 200 time steps. So the incentive for the agent is to get the car&#8217;s position to &gt;0.5 as soon as possible, after which the game ends. This will minimize the negative reward, which is the aim of the game.</p>
<p>However, in this default arrangement, it will take a significant period of time of random exploration before the car stumbles across the positive feedback of getting to the flag. As such, to speed things up a bit, in this example we&#8217;ll alter the reward structure to:</p>
<ul>
<li>Position &gt; 0.1, r += 10</li>
<li>Position &gt; 0.25 r += 20</li>
<li>Position &gt; 0.5 r += 100</li>
</ul>
<p>This new reward structure gives the agent better positive feedback when it starts learning how to ascend the hill on the right hand side toward the flag. The position of 0.1 is just over half way up the right-hand hill.</p>
<p>Ok, so now you know the environment, let&#8217;s write some code!</p>
<h1>Reinforcement learning in TensorFlow</h1>
<p>In this reinforcement learning implementation in TensorFlow, I&#8217;m going to split the code up into three main classes, these classes are:</p>
<ul>
<li>Model: This class holds the TensorFlow operations and model definitions</li>
<li>Memory: This class is where the memory of the actions, rewards and states are stored and retrieved from</li>
<li>GameRunner: This class is the main training and agent control class</li>
</ul>
<p>As stated before, I&#8217;ll be assuming some prior knowledge of TensorFlow here. If you&#8217;re not up to speed your welcome to wing it. Otherwise check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a>. All the code for this tutorial can be found on <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github repository</a>.</p>
<p>I&#8217;ll go through each of the classes in turn in the sub-sections below.</p>
<h2>The Model class</h2>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Model:
    def __init__(self, num_states, num_actions, batch_size):
        self._num_states = num_states
        self._num_actions = num_actions
        self._batch_size = batch_size
        # define the placeholders
        self._states = None
        self._actions = None
        # the output operations
        self._logits = None
        self._optimizer = None
        self._var_init = None
        # now setup the model
        self._define_model()

    def _define_model(self):
        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)
        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)
        # create a couple of fully connected hidden layers
        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)
        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)
        self._logits = tf.layers.dense(fc2, self._num_actions)
        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)
        self._optimizer = tf.train.AdamOptimizer().minimize(loss)
        self._var_init = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first function within the class is of course the initialization function. All you need to pass into the Model definition is the number of states of the environment (2 in this game), the number of possible actions (3 in this game) and the batch size. The function simply sets up a few internal variables and operations, some of which are exposed as public properties later in the class definition. At the end of the initialization, the second method displayed above <em>_define_model() </em>is called. This method sets up the model structure and the main operations.</p>
<p>First, two placeholders are created <em>_states</em> and <em>_q_s_a</em> &#8211; these hold the state data and the $Q(s,a)$ training data respectively. The first dimension of these placeholders is set to <em>None, </em>so that it will automatically adapt when a batch of training data is fed into the model and also when single predictions from the model are required. The next lines create two fully connected layers <em>fc1 </em>and <em>fc2 </em>using the handy TensorFlow layers module. These hidden layers have 50 nodes each, and they are activated using the ReLU activation function (if you want to know more about the ReLU, check out my <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">vanishing gradient and ReLU tutorial</a>).</p>
<p>The next layer is the output layer <em>_logits</em> &#8211; this is another fully connected or dense layer, but with no activation supplied. When no activation function is supplied to the dense layer API in TensorFlow, it defaults to a &#8216;linear&#8217; activation i.e. no activation. This is what we want, as we want the network to learn continuous $Q(s,a)$ values across all possible real numbers.</p>
<p>Next comes the <em>loss </em>&#8211; this isn&#8217;t a classification problem, so a good loss to use is simply a mean squared error loss. The next line specifies the optimizer &#8211; in this example, we&#8217;ll just use the generic Adam optimizer. Finally, the TensorFlow boiler plate global variable initializer operation is assigned to <em>_var_init</em>.</p>
<p>So far so good. Next, some methods of the Model class are created to perform prediction and training:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def predict_one(self, state, sess):
        return sess.run(self._logits, feed_dict={self._states:
                                                     state.reshape(1, self.num_states)})

    def predict_batch(self, states, sess):
        return sess.run(self._logits, feed_dict={self._states: states})

    def train_batch(self, sess, x_batch, y_batch):
        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first method <em>predict_one</em> simply returns the output of the network (i.e. by calling the <em>_logits</em> operation) with an input of a single state. Note the reshaping operation that is used to ensure that the data has a size (1, num_states). This is called whenever action selection by the agent is required. The next method, <em>predict_batch</em> predicts a whole batch of outputs when given a whole bunch of input states &#8211; this is used to perform batch evaluation of $Q(s,a)$ and $Q(s&#8217;,a&#8217;)$ values for training. Finally, there is a method called <em>train_batch</em> which takes a batch training step of the network.</p>
<p>That&#8217;s the Model class, now it is time to consider the Memory class.</p>
<h2>The Memory class</h2>
<p>The next class to consider in the code is the Memory class &#8211; this class stores all the results of the action of the agent in the game, and also handles the retrieval. These can be used to batch train the network.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Memory:
    def __init__(self, max_memory):
        self._max_memory = max_memory
        self._samples = []

    def add_sample(self, sample):
        self._samples.append(sample)
        if len(self._samples) &gt; self._max_memory:
            self._samples.pop(0)

    def sample(self, no_samples):
        if no_samples &gt; len(self._samples):
            return random.sample(self._samples, len(self._samples))
        else:
            return random.sample(self._samples, no_samples)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, when the Memory class is initialized, it is necessary to supply a maximum memory argument &#8211; this will control the maximum number of (state, action, reward, next_state) tuples the <em>_samples</em> list can hold. The bigger the better, as it ensures better random mixing of the samples, but you have to make sure you don&#8217;t run into memory errors.</p>
<p>The first method, <em>add_sample</em> takes an individual (state, action, reward, next_state) tuple and appends it to the <em>_samples </em>list. After this, a check is made &#8211; if the number of samples is now larger than the allowable memory size, the first element in <em>_samples</em> is removed using the Python .pop() list functionality.</p>
<p>The final method, <em>sample</em> returns a random selection of <em>no_samples</em> in length. However, if the <em>no_samples</em> argument is larger than the actual memory, whatever is available in the memory is returned.</p>
<p>The final class is called GameRunner.</p>
<h2>The GameRunner class</h2>
<p>The GameRunner class in this example is where all the model dynamics, agent action and training is organised.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class GameRunner:
    def __init__(self, sess, model, env, memory, max_eps, min_eps,
                 decay, render=True):
        self._sess = sess
        self._env = env
        self._model = model
        self._memory = memory
        self._render = render
        self._max_eps = max_eps
        self._min_eps = min_eps
        self._decay = decay
        self._eps = self._max_eps
        self._steps = 0
        self._reward_store = []
        self._max_x_store = []</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the GameRunner initialization, some internal variables are created. Note, it takes as first argument a TensorFlow session object, then a neural network Model, an Open AI gym environment and a Memory class instance. The next arguments <em>max_eps </em>and <em>min_eps</em> dictate the maximum and minimum epsilon values respectively &#8211; during training the actual $\epsilon$ will decay from the maximum to the minimum based on the following argument <em>decay</em>. Finally, <em>render </em>is a boolean which determines whether the game environment is rendered to the screen.</p>
<p>The next method is <em>run()</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def run(self):
        state = self._env.reset()
        tot_reward = 0
        max_x = -100
        while True:
            if self._render:
                self._env.render()

            action = self._choose_action(state)
            next_state, reward, done, info = self._env.step(action)
            if next_state[0] &gt;= 0.1:
                reward += 10
            elif next_state[0] &gt;= 0.25:
                reward += 20
            elif next_state[0] &gt;= 0.5:
                reward += 100

            if next_state[0] &gt; max_x:
                max_x = next_state[0]
            # is the game complete? If so, set the next state to
            # None for storage sake
            if done:
                next_state = None

            self._memory.add_sample((state, action, reward, next_state))
            self._replay()

            # exponentially decay the eps value
            self._steps += 1
            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \
                                      * math.exp(-LAMBDA * self._steps)

            # move the agent to the next state and accumulate the reward
            state = next_state
            tot_reward += reward

            # if the game is done, break the loop
            if done:
                self._reward_store.append(tot_reward)
                self._max_x_store.append(max_x)
                break

        print(&quot;Step {}, Total reward: {}, Eps: {}&quot;.format(self._steps, tot_reward, self._eps))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We&#8217;ll go through each step in the code above. First, the environment is reset by calling the Open AI Gym command .reset(). Then an infinite loop is entered into &#8211; this will be exited by calling a <em>break</em> command. If the boolean _<em>render</em> is True, then the output of the game will be shown on the screen. The <em>action</em> of the agent is determined by calling the internal method <em>_choose_action(state)</em> &#8211; this will discussed later. Next, the agent takes <em>action </em>by calling the Open AI Gym command <em>step(action)</em>. This command returns a tuple containing the new state of the agent, the reward received by taking <em>action</em>, a <em>done </em>boolean indicating whether the game has finished, and an information object (we won&#8217;t using <em>info </em>in this example).</p>
<p>The next step in the code is where there are some manual adjustments to the Mountain Car reward system. If you recall, earlier I mentioned that in order to speed up the training of the network, it was useful to add some more reward steps the closer the car got to the goal (rather than the default reward which was only received when the car reached the goal/flag). The maximum <em>x</em> value achieved in the given episode is also tracked and this will be stored once the game is complete.</p>
<p>The next step is a check to see if the game has completed i.e. <em>done == True</em> &#8211; this will occur after 200 turns. If it has completed, we want to set the <em>next_state</em> to None. This will be picked up during the training / replay step of the class, and the state will be set to an array of zeros whenever <em>next_state</em> is equal to None.</p>
<p>After this, the data about the agent is stored in the memory class &#8211; i.e.its original <em>state</em>, its chosen <em>action</em>, the <em>reward </em>it received for that action and finally the <em>next_state</em> of the agent. After this takes place, the training / replay step of the deep Q network is run &#8211; this step will be discussed more below. At this point the <em>epsilon </em>value is also exponentially decayed. Finally, the agent&#8217;s state is moved to <em>next_state</em>, the total reward during the game is accumulated, and there is some printing and breaking of the loop and storing of relevant variables if the game is complete.</p>
<p>The next part of the GameRunner class is the agent action selection method:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _choose_action(self, state):
        if random.random() &lt; self._eps:
            return random.randint(0, self._model.num_actions - 1)
        else:
            return np.argmax(self._model.predict_one(state, self._sess))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This method executes our <em>epsilon </em>greedy + Q policy. In the first case, if a random number is less than the <em>_eps</em> value, then the returned action will simply be an action chosen at random from the set of possible actions. Otherwise, the action will be chosen based on an argmax of the output from the neural network. Recall that _<em>predict_one</em> from the model will take a single state as input, then output $Q(s,a)$ values for each of the possible actions available &#8211; the action with the highest $Q(s,a)$ value is that action with the highest expected current + future discounted reward.</p>
<p>The final method within the GameRunner class is the <em>_replay</em> method, where the batching and training takes place:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _replay(self):
        batch = self._memory.sample(self._model.batch_size)
        states = np.array([val[0] for val in batch])
        next_states = np.array([(np.zeros(self._model.num_states)
                                 if val[3] is None else val[3]) for val in batch])
        # predict Q(s,a) given the batch of states
        q_s_a = self._model.predict_batch(states, self._sess)
        # predict Q(s&#039;,a&#039;) - so that we can do gamma * max(Q(s&#039;a&#039;)) below
        q_s_a_d = self._model.predict_batch(next_states, self._sess)
        # setup training arrays
        x = np.zeros((len(batch), self._model.num_states))
        y = np.zeros((len(batch), self._model.num_actions))
        for i, b in enumerate(batch):
            state, action, reward, next_state = b[0], b[1], b[2], b[3]
            # get the current q values for all actions in state
            current_q = q_s_a[i]
            # update the q value for action
            if next_state is None:
                # in this case, the game completed after action, so there is no max Q(s&#039;,a&#039;)
                # prediction possible
                current_q[action] = reward
            else:
                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])
            x[i] = state
            y[i] = current_q
        self._model.train_batch(self._sess, x, y)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step in the <em>_replay</em> method is to retrieve a randomized batch of data from memory. Next, we want to setup our batch state variables so that we can:</p>
<ol>
<li>For each state, produce baseline $Q(s,a)$ values &#8211; one of which will be given a target of $r + \gamma \max_{a&#8217;} Q(s&#8217;, a&#8217;)$</li>
<li> For each <em>next_state</em>, predict $Q(s&#8217;,a&#8217;)$ from the model, as required in (1)</li>
</ol>
<p>Now, if you recall, each sample in memory has the form of a tuple: <em>state, action, reward, next_state </em>which was extracted from the game play. To setup a batch of initial states, then, we simply use Python list comprehension to extract the first tuple value from each sample in the batch. Likewise, we do the same for the fourth value in the tuple to extract the <em>next_state</em> value for each sample in the batch. Note that whenever the <em>next_state</em> corresponds to a case where the game finished (i.e. <em>next_state </em>is None) the next state value is replaced by a vector of zeros corresponding in size to the number of states in the game.</p>
<p>Next, the batch of $Q(s, a)$ and $Q(s&#8217;,a&#8217;)$ values are extracted from the model from <em>states </em>and <em>next_states</em> respectively. The <em>x </em>and <em>y</em> training arrays are then created, but initially filled with zeros. After this, a loop is entered into to accumulate the <em>x </em>and <em>y</em> values on which to train the model. Within this loop, we extract the memory values from the batch, then set a variable designating the Q values for the current state. If the <em>next_state</em> value is actually zero, there is no discounted future rewards to add, so the <em>current_q</em> corresponding to <em>action</em> is set a target of the <em>reward</em> only. Alternatively, if there is a valid <em>next_state, </em>then the <em>current_q</em> corresponding to <em>action </em>is set a target of the <em>reward</em> plus the discounted future reward i.e. $max_{a&#8217;} Q(s&#8217;, a&#8217;)$.</p>
<p>The <em>state</em> and <em>current_q</em> are then loaded into the <em>x </em>and <em>y </em>values for the given batch, until the batch data is completely extracted. Then the network is trained by calling <em>_train_batch()</em> on the model.</p>
<p>That completes the review of the main classes within the TensorFlow reinforcement learning example. All that is left is to setup the classes and enter the training loop.</p>
<h2>The main function</h2>
<p>The code below sets up the environment and the classes, and runs multiple games to perform the learning:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if __name__ == &quot;__main__&quot;:
    env_name = &#039;MountainCar-v0&#039;
    env = gym.make(env_name)

    num_states = env.env.observation_space.shape[0]
    num_actions = env.env.action_space.n

    model = Model(num_states, num_actions, BATCH_SIZE)
    mem = Memory(50000)

    with tf.Session() as sess:
        sess.run(model.var_init)
        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,
                        LAMBDA)
        num_episodes = 300
        cnt = 0
        while cnt &lt; num_episodes:
            if cnt % 10 == 0:
                print(&#039;Episode {} of {}&#039;.format(cnt+1, num_episodes))
            gr.run()
            cnt += 1
        plt.plot(gr.reward_store)
        plt.show()
        plt.close(&quot;all&quot;)
        plt.plot(gr.max_x_store)
        plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first couple of lines, we create an Open AI Gym Mountain Car environment. Next, the number of states and actions are extracted from the environment object itself.</p>
<p>The network model and memory objects are then created &#8211; in this case, we&#8217;re using a batch size of 50 and a total number of samples in the memory of 50,000.</p>
<p>The TensorFlow session object is created, along with the variable initialization &#8211; then the GameRunner class is created. The number of episodes of the Mountain Car game which will be run in this training example is 300. For each of these episodes, we run the game by using the GameRunner <em>run()</em> method.</p>
<p>After all the episodes are run, some plotting is performed on the total reward for each episode, and the maximum x-axis value the cart reaches in the game (remembering that the goal is at x = 0.5). These plots can be observed below:</p>
<figure id="attachment_1006" style="width: 473px" class="wp-caption aligncenter"><img class=" wp-image-1006" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards.png" alt="Mountain Car rewards - from the TensorFlow reinforcement learning example" width="473" height="355" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-80x60.png 80w" sizes="(max-width: 473px) 100vw, 473px" /><figcaption class="wp-caption-text">The Mountain Car rewards from the TensorFlow reinforcement learning example</figcaption></figure>
<p>As can be observed, the network starts out controlling the agent rather poorly, while it is exploring the environment and accumulating memory. However once it starts to receive positive rewards by ascending the right-hand hill, the rewards rapidly increase.</p>
<figure id="attachment_1007" style="width: 485px" class="wp-caption aligncenter"><img class=" wp-image-1007" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX.png" alt="Mountain Car maximum X - from the TensorFlow reinforcement learning example" width="485" height="364" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-80x60.png 80w" sizes="(max-width: 485px) 100vw, 485px" /><figcaption class="wp-caption-text">The Mountain Car maximum x values from the TensorFlow reinforcement learning example</figcaption></figure>
<p>As can be observed above, while there is some volatility, the network learns that the best rewards are achieved by reaching the top of the right-hand hill and, towards the end of the training, consistently controls the car/agent to reach there.</p>
<p>This reinforcement learning tutorial in TensorFlow has shown you:</p>
<ol>
<li>The basics of Q learning</li>
<li>The epsilon greed action selection policy</li>
<li> The importance of batching in training deep Q reinforcement learning networks, and</li>
<li>How to implement a deep Q reinforcement learning network in TensorFlow</li>
</ol>
<p>I hope it has been instructive &#8211; keep an eye out for future tutorials in reinforcement learning where more complicated games and techniques will be reviewed.</p>
<hr />
<p><strong>Recommended online course &#8211; </strong>If you are more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1153742&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-reinforcement-learning-in-python%2F">Advanced AI: Deep Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1153742&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/">Reinforcement learning tutorial with TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Reinforcement learning tutorial using Python and Keras</title>
		<link>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/</link>
		<comments>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments</comments>
		<pubDate>Sat, 03 Mar 2018 03:15:48 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Keras]]></category>
		<category><![CDATA[Reinforcement learning]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=763</guid>
		<description><![CDATA[<p>In this post, I&#8217;m going to introduce the concept of reinforcement learning, and show you how to build an autonomous agent that can successfully play <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/" title="Reinforcement learning tutorial using Python and Keras">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/">Reinforcement learning tutorial using Python and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In this post, I&#8217;m going to introduce the concept of reinforcement learning, and show you how to build an autonomous agent that can successfully play a simple game. Reinforcement learning is an active and interesting area of machine learning research, and has been spurred on by recent successes such as the <a href="https://en.wikipedia.org/wiki/AlphaGo" target="_blank" rel="noopener">AlphaGo system</a>, which has convincingly beat the best human players in the world. This occurred in a game that was thought too difficult for machines to learn. In this tutorial, I&#8217;ll first detail some background theory while dealing with a toy game in the Open AI Gym toolkit. We&#8217;ll then create a Q table of this game using simple Python, and then create a Q network using Keras. If you&#8217;d like to scrub up on Keras, check out my <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">introductory Keras tutorial</a>. All code present in this tutorial is available on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github page</a>.</p>
<hr />
<p><strong>Recommended online course </strong>&#8211;<strong> </strong>If you&#8217;re more of a video based learner, I&#8217;d recommend the following inexpensive Udemy online course in reinforcement learning: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1080408&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fartificial-intelligence-reinforcement-learning-in-python%2F" target="new">Artificial Intelligence: Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Reinforcement learning &#8211; the basics</h1>
<p>Reinforcement learning can be considered the third genre of the machine learning triad &#8211; unsupervised learning, supervised learning and reinforcement learning. In supervised learning, we supply the machine learning system with curated (x, y) training pairs, where the intention is for the network to learn to map x to y. In reinforcement learning, we create an <em>agent</em> which performs <em>actions </em>in an <em>environment </em>and the agent receives various <em>rewards</em> depending on what <em>state </em>it is in when it performs the action. In other words, an agent explores a kind of game, and it is trained by trying to maximize rewards in this game. This cycle is illustrated in the figure below:</p>
<figure id="attachment_768" style="width: 381px" class="wp-caption aligncenter"><img class="size-full wp-image-768" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png" alt="Reinforcement learning with Python and Keras - Reinforcement learning environment" width="381" height="211" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png 381w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment-300x166.png 300w" sizes="(max-width: 381px) 100vw, 381px" /><figcaption class="wp-caption-text">Reinforcement learning environment</figcaption></figure>
<p>As can be observed above, the agent performs some action in the environment. An interpreter views this action in the environment, and feeds back an updated state that the agent now resides in, and also the reward for taking this action. The environment is not known by the agent beforehand, but rather it is discovered by the agent taking incremental steps in time. So, for instance, at time <em>t </em>the agent, in state <em>$s_{t}$</em>,  may take action <em>a</em>. This results in a new state <em>$s_{t+1}$ </em>and a reward <em>r.</em> This reward can be a positive real number, zero, or a negative real number. It is the goal of the agent to learn which <em>state dependent action</em> to take which maximizes its rewards. The way which the agent optimally learns is the subject of reinforcement learning theory and methodologies.</p>
<p>To more meaningfully examine the theory and possible approaches behind reinforcement learning, it is useful to have a simple example in which to work through. This simple example will come from an environment available on <a href="https://gym.openai.com/envs/NChain-v0/" target="_blank" rel="noopener">Open AI Gym called NChain</a>.</p>
<h2>Open AI Gym example</h2>
<p>The NChain example on Open AI Gym is a simple 5 state environment. There are two possible actions in each state, move forward (action 0) and move backwards (action 1). When action 1 is taken, i.e. move backwards, there is an immediate reward of 2 given to the agent &#8211; and the agent is returned to state 0 (back to the beginning of the chain). However, when a move forward action is taken (action 0), there is no immediate reward until state 4. When the agent moves forward while in state 4, a reward of 10 is received by the agent. The agent stays in state 4 at this point also, so the reward can be repeated. There is also a random chance that the agent&#8217;s action is &#8220;flipped&#8221; by the environment (i.e. an action 0 is flipped to an action 1 and vice versa). The diagram below demonstrates this environment:</p>
<figure id="attachment_769" style="width: 648px" class="wp-caption aligncenter"><img class=" wp-image-769" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png" alt="Reinforcement learning - Python and Keras - NChain environment" width="648" height="207" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png 906w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration-300x96.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration-768x245.png 768w" sizes="(max-width: 648px) 100vw, 648px" /><figcaption class="wp-caption-text">Open AI Gym&#8217;s NChain environment</figcaption></figure>
<p>You can play around with this environment by first installing the Open AI Gym Python package &#8211; see instructions <a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">here</a>. Then simply open up your Python command prompt and have a play &#8211; see the figure below for an example of some of the commands available:</p>
<figure id="attachment_771" style="width: 449px" class="wp-caption aligncenter"><img class=" wp-image-771" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround.png" alt="Reinforcement learning - Python and Keras - NChain Python playaround" width="449" height="329" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround.png 795w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-300x220.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-768x562.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-80x60.png 80w" sizes="(max-width: 449px) 100vw, 449px" /><figcaption class="wp-caption-text">NChain Python playaround</figcaption></figure>
<p>If you examine the code above, you can observe that first the Python module is imported, and then the environment is loaded via the gym.make() command. The first step is to initalize / reset the environment by running env.reset() &#8211; this command returns the initial state of the environment &#8211; in this case 0. The first command I then run is env.step(1) &#8211; the value in the bracket is the action ID. As explained previously, action 1 represents a step back to the beginning of the chain (state 0). The step() command returns 4 variables in a tuple, these are (in order):</p>
<ul>
<li>The new state after the action</li>
<li>The reward due to the action</li>
<li>Whether the game is &#8220;done&#8221; or not &#8211; the NChain game is done after 1,000 steps</li>
<li>Debugging information &#8211; not relevant in this example</li>
</ul>
<p>As can be observed, starting in state 0 and taking step(1) action, the agent stays in state 0 and gets 2 for its reward. Next, I sent a series of action 0 commands. After every action 0 command, we would expect the progression of the agent along the chain, with the state increasing in increments (i.e. 0 -&gt; 1 -&gt; 2 etc.). However, you&#8217;ll observe after the first step(0) command, that the agent stays in state 0 and gets a 2 reward. This is because of the random tendency of the environment to &#8220;flip&#8221; the action occasionally, so the agent <em>actually</em> performed a 1 action. This is just unlucky.</p>
<p>Nevertheless, I persevere and it can be observed that the state increments as expected, but there is no immediate reward for doing so for the agent until it reaches state 4. When in state 4, an action of 0 will keep the agent in step 4 <em>and </em>give the agent a 10 reward. Not only that, the environment allows this to be done repeatedly, as long as it doesn&#8217;t produce an unlucky &#8220;flip&#8221;, which would send the agent back to state 0 &#8211; the beginning of the chain.</p>
<p>Now that we understand the environment that will be used in this tutorial, it is time to consider what method can be used to train the agent.</p>
<h2>A first naive heuristic for reinforcement learning</h2>
<p>In order to train the agent effectively, we need to find a good <em>policy $\pi$ </em>which maps states to actions in an optimal way to maximize reward. There are various ways of going about finding a good or optimal policy, but first, let&#8217;s consider a naive approach.</p>
<p>Let&#8217;s conceptualize a table, and call it a reward table, which looks like this:</p>
<p>$$<br />
\begin{bmatrix}<br />
r_{s_0,a_0} &amp; r_{s_0,a_1} \\<br />
r_{s_1,a_0} &amp; r_{s_1,a_1} \\<br />
r_{s_2,a_0} &amp; r_{s_2,a_1} \\<br />
r_{s_3,a_0} &amp; r_{s_3,a_1} \\<br />
r_{s_4,a_0} &amp; r_{s_4,a_1} \\<br />
\end{bmatrix}<br />
$$</p>
<p>Each of the rows corresponds to the 5 available states in the NChain environment, and each column corresponds to the 2 available actions in each state &#8211; forward and backward, 0 and 1. The value in each of these table cells corresponds to some measure of reward that the agent has &#8220;learnt&#8221; occurs when they are in that state and perform that action. So, the value $r_{s_0,a_0}$ would be, say, the sum of the rewards that the agent has received when in the past they have been in state 0 and taken action 0. This table would then let the agent choose between actions based on the summated (or average, median etc. &#8211; take your pick) amount of reward the agent has received in the past when taking actions 0 or 1.</p>
<p>This might be a good policy &#8211; choose the action resulting in the greatest previous summated reward. Let&#8217;s give it a try, the code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def naive_sum_reward_agent(env, num_episodes=500):
    # this is the table that will hold our summated rewards for
    # each action in each state
    r_table = np.zeros((5, 2))
    for g in range(num_episodes):
        s = env.reset()
        done = False
        while not done:
            if np.sum(r_table[s, :]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with highest cummulative reward
                a = np.argmax(r_table[s, :])
            new_s, r, done, _ = env.step(a)
            r_table[s, a] += r
            s = new_s
    return r_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the function definition, the environment is passed as the first argument, then the number of episodes (or number of games) that we will train the r_table on. We first create the r_table matrix which I presented previously and which will hold our summated rewards for each state and action. Then there is an outer loop which cycles through the number of episodes. The env.reset() command starts the game afresh each time a new episode is commenced. It also returns the starting state of the game, which is stored in the variable <em>s</em>.</p>
<p>The second, inner loop continues until a &#8220;done&#8221; signal is returned after an action is passed to the environment. The <em>if</em> statement on the first line of the inner loop checks to see if there are any existing values in the r_table for the current state &#8211; it does this by confirming if the sum across the row is equal to 0. If it is zero, then an action is chosen at random &#8211; there is no better information available at this stage to judge which action to take.</p>
<p>This condition will only last for a short period of time. After this point, there will be a value stored in at least one of the actions for each state, and the action will be chosen based on which column value is the largest for the row state <em>s.</em> In the code, this choice of the maximum column is executed by the numpy <em>argmax</em> function &#8211; this function returns the index of the vector / matrix with the highest value. For example, if the agent is in state 0 and we have the r_table with values [100, 1000] for the first row, action 1 will be selected as the index with the highest value is column 1.</p>
<p>After the action has been selected and stored in <em>a</em>, this action is fed into the environment with env.step(a). This command returns the new state, the reward for this action, whether the game is &#8220;done&#8221; at this stage and the debugging information that we are not interested in. In the next line, the r_table cell corresponding to state <em>s </em>and action <em>a </em>is updated by adding the reward to whatever is already existing in the table cell.</p>
<p>Finally the state <em>s </em>is updated to <em>new_s</em> &#8211; the new state of the agent.</p>
<p>If we run this function, the r_table will look something like:</p>
<p><img class="alignnone wp-image-788" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Naive-RL-model-r_table-1.png" alt="" width="177" height="101" /></p>
<p>Examining the results above, you can observe that the most common state for the agent to be in is the first state, seeing as any action 1 will bring the agent back to this point. The least occupied state is state 4, as it is difficult for the agent to progress from state 0 to 4 without the action being &#8220;flipped&#8221; and the agent being sent back to state 0. You can get different results if you run the function multiple times, and this is because of the stochastic nature of both the environment and the algorithm.</p>
<p>Clearly &#8211; something is wrong with this table. One would expect that in state 4, the most rewarding action for the agent would be to choose action 0, which would reward the agent with 10 points, instead of the usual 2 points for an action of 1. Not only that, but it has chosen action 0 for <em>all </em>states &#8211; this goes against intuition &#8211; surely it would be best to sometimes shoot for state 4 by choosing multiple action 0&#8217;s in a row, and that way reap the reward of multiple possible 10 scores.</p>
<p>In fact, there are a number of issues with this way of doing reinforcement learning:</p>
<ul>
<li>First, once there is a reward stored in one of the columns, the agent will always choose that action from that point on. This will lead to the table being &#8220;locked in&#8221; with respect to actions after just a few steps in the game.</li>
<li>Second, because no reward is obtained for most of the states when action 0 is picked, this model for training the agent has no way to encourage acting on <em>delayed reward</em> signal when it is appropriate for it to do so.</li>
</ul>
<p>Let&#8217;s see how these problems could be fixed.</p>
<h2>Delayed reward reinforcement learning</h2>
<p>If you want to be a medical doctor, you&#8217;re going to have to go through some pain to get there. You&#8217;ll be studying a long time before you&#8217;re free to practice on your own, and the rewards will be low while you are doing so. However, once you get to be a fully fledged MD, the rewards will be great. During your time studying, you would be operating under a delayed reward or delayed gratification paradigm in order to reach that greater reward. However, you might only be willing to undertake that period of delayed reward for a given period of time &#8211; you wouldn&#8217;t want to be studying forever, or at least, for decades.</p>
<p>We can bring these concepts into our understanding of reinforcement learning. Let&#8217;s say we are in state 3 &#8211; in the previous case, when the agent chose action 0 to get to state 3, the reward was zero and therefore r_table[3, 0] = 0. Obviously the agent would not see this as an attractive step compared to the alternative for this state i.e. r_table[3, 1] &gt;= 2. But what if we assigned to this state the reward the agent would received if it chose action 0 in state 4? It would look like this: r_table[3, 0] = r + 10 = 10 &#8211; a much more attractive alternative!</p>
<p>This idea of propagating possible reward from the best possible actions in future states is a core component of what is called <em>Q learning</em>. In Q learning, the Q value for each action in each state is updated when the relevant information is made available. The Q learning rule is:</p>
<p>$$Q(s, a) = Q(s, a) + \alpha (r + \gamma \max\limits_{a&#8217;} Q(s&#8217;, a&#8217;) &#8211; Q(s, a))$$</p>
<p>First, as you can observe, this is an <em>updating </em>rule &#8211; the existing Q value is added to, not replaced. Ignoring the $\alpha$ for the moment, we can concentrate on what&#8217;s inside the brackets. The first term, <em>r</em><em>, </em>is the reward that was obtained when action <em>a</em> was taken in state <em>s</em>. Next, we have an expression which is a bit more complicated. Ignore the $\gamma$ for the moment and focus on $\max\limits_{a&#8217;} Q(s&#8217;, a&#8217;)$. What this means is that we look at the next state <em>s&#8217;</em> after action <em>a</em> and return the maximum possible Q value in the next state. In other words, return the maximum Q value for the best possible action in the next state. In this way, the agent is looking forward to determine the best possible future rewards before making the next step <em>a</em>.</p>
<p>The $\gamma$ value is called the <em>discounting </em>factor &#8211; this decreases the impact of future rewards on the immediate decision making in state <em>s</em>. This is important, as this represents a limited patience in the agent &#8211; it won&#8217;t study forever to get that medical degree. So $\gamma$ will always be less than 1. The &#8211; Q(s, a) term acts to restrict the growth of the Q value as the training of the agent progresses through many iterations. Finally, this whole sum is multiplied by a <em>learning rate </em>$\alpha$ which restricts the updating to ensure it doesn&#8217;t &#8220;race&#8221; to a solution &#8211; this is important for optimal convergence (see my <em> </em><a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">neural networks tutorial</a> for more on learning rate).</p>
<p>Note that while the learning rule only examines the best action in the following state, in reality, discounted rewards still cascade down from future states. For instance, if we think of the cascading rewards from all the 0 actions (i.e. moving forward along the chain) and start at state 3, the Q reward will be $r + \gamma \max_a Q(s&#8217;, a&#8217;) = 0 + 0.95 * 10 = 9.5$ (with a $\gamma$ = 0.95). If we work back from state 3 to state 2 it will be 0 + 0.95 * 9.5 = 9.025. Likewise, the cascaded, discounted reward from to state 1 will be 0 + 0.95 * 9.025 = 8.57, and so on. Therefore, while the immediate updating calculation only looks at the maximum Q value for the next state, &#8220;upstream&#8221; rewards that have previously been discovered by the agent still cascade down into the present state and action decision. This is a simplification, due to the learning rate and random events in the environment, but represents the general idea.</p>
<p>Now that you (hopefully) understand Q learning, let&#8217;s see what it looks like in practice:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    lr = 0.8
    for i in range(num_episodes):
        s = env.reset()
        done = False
        while not done:
            if np.sum(q_table[s,:]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with largest q value in state s
                a = np.argmax(q_table[s, :])
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])
            s = new_s
    return q_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function is almost exactly the same as the previous naive r_table function that was discussed. The additions and changes are:</p>
<ul>
<li>The variables <em>y</em> which specifies the discounting factor $\gamma$ and <em>lr</em> which is the Q table updating learning rate</li>
<li>The line:<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])</code></pre> <div class="code-embed-infos"> </div> </div></li>
</ul>
<p>This line executes the Q learning rule that was presented previously. The np.max(q_table[new_s, :]) is an easy way of selecting the maximum value in the q_table for the row new_s. After this function is run, an example q_table output is:</p>
<p><img class="alignnone wp-image-796" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table.png" alt="" width="248" height="107" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table.png 313w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table-300x129.png 300w" sizes="(max-width: 248px) 100vw, 248px" /></p>
<p>This output is strange, isn&#8217;t it? Again, we would expect at least the state 4 &#8211; action 0 combination to have the highest Q score, but it doesn&#8217;t.  We might also expect the reward from this action in this state to have cascaded down through the states 0 to 3. Something has clearly gone wrong &#8211; and the answer is that there isn&#8217;t enough <em>exploration</em> going on within the agent training method.</p>
<h2>Q learning with $\epsilon$-greedy action selection</h2>
<p>If we think about the previous iteration of the agent training model using Q learning, the action selection policy is based solely on the maximum Q value in any given state. It is conceivable that, given the random nature of the environment, that the agent initially makes &#8220;bad&#8221; decisions. The Q values arising from these decisions may easily be &#8220;locked in&#8221; &#8211; and from that time forward, bad decisions may continue to be made by the agent because it can only ever select the maximum Q value in any given state, even if these values are not necessarily optimal. This action selection policy is called a <em>greedy </em>policy.</p>
<p>So we need a way for the agent to eventually always choose the &#8220;best&#8221; set of actions in the environment, yet at the same time allowing the agent to not get &#8220;locked in&#8221; and giving it some space to explore alternatives. What is required is the $\epsilon$-greedy policy.</p>
<p>The $\epsilon$-greedy policy in reinforcement learning is basically the same as the <em>greedy </em>policy, except that there is a value $\epsilon$ (which may be set to decay over time) where, if a random number is selected which is less than this value, an action is chosen completely at random. This step allows some random <em>exploration</em> of the value of various actions in various states, and can be scaled back over time to allow the algorithm to concentrate more on <em>exploiting </em>the best strategies that it has found. This mechanism can be expressed in code as:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def eps_greedy_q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    eps = 0.5
    lr = 0.8
    decay_factor = 0.999
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        done = False
        while not done:
            # select the action with highest cummulative reward
            if np.random.random() &lt; eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])
            # pdb.set_trace()
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])
            s = new_s
    return q_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code shows the introduction of the $\epsilon$ value &#8211; <em>eps</em>. There is also an associated <em>eps</em> decay_factor which exponentially decays eps with each episode <em>eps *= decay_factor</em>. The $\epsilon$-greedy based action selection can be found in this code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            if np.random.random() &lt; eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first component of the <em>if </em>statement shows a random number being selected, between 0 and 1, and determining if this is below <em>eps</em>. If so, the action will be selected randomly from the two possible actions in each state. The second part of the <em>if </em>statement is a random selection if there are no values stored in the q_table so far. If neither of these conditions hold true, the action is selected as per normal by taking the action with the highest q value.</p>
<p>The rest of the code is the same as the standard <em>greedy </em>implementation with Q learning discussed previously. This code produces a q_table which looks something like the following:</p>
<p><img class="alignnone wp-image-800" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table.png" alt="" width="242" height="102" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table.png 306w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table-300x126.png 300w" sizes="(max-width: 242px) 100vw, 242px" /></p>
<p>Finally we have a table which favors action 0 in state 4 &#8211; in other words what we would expect to happen given the reward of 10 that is up for grabs via that action in that state. Notice also that, as opposed to the previous tables from the other methods, that there are no actions with a 0 Q value &#8211; this is because the full action space has been explored via the randomness introduced by the $\epsilon$-greedy policy.</p>
<h2>Comparing the methods</h2>
<p>Let&#8217;s see if the last agent training model actually produces an agent that gathers the most rewards in any given game. The code below shows the three models trained and then tested over 100 iterations to see which agent performs the best over a test game. The models are trained as well as tested in each iteration because there is significant variability in the environment which messes around with the efficacy of the training &#8211; so this is an attempt to understand average performance of the different models. The main testing code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def test_methods(env, num_iterations=100):
    winner = np.zeros((3,))
    for g in range(num_iterations):
        m0_table = naive_sum_reward_agent(env, 500)
        m1_table = q_learning_with_table(env, 500)
        m2_table = eps_greedy_q_learning_with_table(env, 500)
        m0 = run_game(m0_table, env)
        m1 = run_game(m1_table, env)
        m2 = run_game(m2_table, env)
        w = np.argmax(np.array([m0, m1, m2]))
        winner[w] += 1
        print(&quot;Game {} of {}&quot;.format(g + 1, num_iterations))
    return winner</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, this method creates a numpy zeros array of length 3 to hold the results of the winner in each iteration &#8211; the winning method is the method that returns the highest rewards after training and playing. The run_game function looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def run_game(table, env):
    s = env.reset()
    tot_reward = 0
    done = False
    while not done:
        a = np.argmax(table[s, :])
        s, r, done, _ = env.step(a)
        tot_reward += r
    return tot_reward</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here, it can be observed that the trained table given to the function is used for action selection, and the total reward accumulated during the game is returned. A sample outcome from this experiment (i.e. the vector w) is shown below:</p>
[13, 22, 65]
<p>&nbsp;</p>
<p>As can be observed, of the 100 experiments the $\epsilon$-greedy, Q learning algorithm (i.e. the third model that was presented) wins 65 of them. This is followed by the standard greedy implementation of Q learning, which won 22 of the experiments. Finally the naive accumulated rewards method only won 13 experiments. So as can be seen, the $\epsilon$-greedy Q learning method is quite an effective way of executing reinforcement learning.</p>
<p>So far, we have been dealing with explicit tables to hold information about the best actions and which actions to choose in any given state. However, while this is perfectly reasonable for a small environment like NChain, the table gets far too large and unwieldy for more complicated environments which have a huge number of states and potential actions.</p>
<p>This is where neural networks can be used in reinforcement learning. Instead of having explicit tables, instead we can train a neural network to predict Q values for each action in a given state. This will be demonstrated using <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras</a> in the next section.</p>
<h1>Reinforcement learning with Keras</h1>
<p>To develop a neural network which can perform Q learning, the input needs to be the current state (plus potentially some other information about the environment) and it needs to output the relevant Q values for each action in that state. The Q values which are output should approach, as training progresses, the values produced in the Q learning updating rule. Therefore, the loss or cost function for the neural network should be:</p>
<p>$$\text{loss} = (\underbrace{r + \gamma \max_{a&#8217;} Q'(s&#8217;, a&#8217;)}_{\text{target}} &#8211; \underbrace{Q(s, a)}_{\text{prediction}})^2$$</p>
<p>The reinforcement learning architecture that we are going to build in Keras is shown below:</p>
<figure style="width: 340px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Reinforcement-learning-Keras.png" alt="Reinforcement learning Python Keras - architecture" width="340" height="335" /><figcaption class="wp-caption-text">Reinforcement learning Keras architecture</figcaption></figure>
<p>The input to the network is the one-hot encoded state vector. For instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]. In this case, a hidden layer of 10 nodes with sigmoid activation will be used. The output layer is a linear activated set of two nodes, corresponding to the two Q values assigned to each state to represent the two possible actions. Linear activation means that the output depends only on the linear summation of the inputs and the weights, with no additional function applied to that summation. For more on neural networks, check out my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">comprehensive neural network tutorial</a>.</p>
<p>Building this network is easy in Keras &#8211; to learn more about how to use Keras, check out <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">my tutorial</a>. The code below shows how it can be done in a few lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = Sequential()
model.add(InputLayer(batch_input_shape=(1, 5)))
model.add(Dense(10, activation=&#039;sigmoid&#039;))
model.add(Dense(2, activation=&#039;linear&#039;))
model.compile(loss=&#039;mse&#039;, optimizer=&#039;adam&#039;, metrics=[&#039;mae&#039;])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, the model is created using the Keras Sequential API. Then an input layer is added which takes inputs corresponding to the one-hot encoded state vectors. Then the sigmoid activated hidden layer with 10 nodes is added, followed by the linear activated output layer which will yield the Q values for each action. Finally the model is compiled using a mean-squared error loss function (to correspond with the loss function defined previously) with the <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam optimizer</a> being used in its default Keras state.</p>
<p>To use this model in the training environment, the following code is run which is similar to the previous $\epsilon$-greedy Q learning methodology with an explicit Q table:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    # now execute the q learning
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        if i % 100 == 0:
            print(&quot;Episode {} of {}&quot;.format(i + 1, num_episodes))
        done = False
        r_sum = 0
        while not done:
            if np.random.random() &lt; eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            s = new_s
            r_sum += r
        r_avg_list.append(r_sum / 1000)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first major difference in the Keras implementation is the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            if np.random.random() &lt; eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first condition in the <em>if </em>statement is the implementation of the $\epsilon$-greedy action selection policy that has been discussed already. The second condition uses the Keras model to produce the two Q values &#8211; one for each possible state. It does this by calling the model.predict() function. Here the numpy identity function is used, with vector slicing, to produce the one-hot encoding of the current state <em>s</em>. The standard numpy argmax function is used to select the action with the highest Q value returned from the Keras model prediction.</p>
<p>The second major difference is the following four lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line sets the target as the Q learning updating rule that has been previously presented. It is the reward <em>r</em> plus the discounted maximum of the predicted Q values for the new state, <em>new_s. </em>This is the value that we want the Keras model to learn to predict for state <em>s </em>and action <em>a</em> i.e. Q(s,a). However, our Keras model has an output for each of the two actions &#8211; we don&#8217;t want to alter the value for the other action, only the action <em>a </em>which has been chosen. So on the next line, <em>target_vec</em> is created which extracts both predicted Q values for state <em>s</em>. On the following line, only the Q value corresponding to the action <em>a</em> is changed to <em>target &#8211; </em>the other action&#8217;s Q value is left untouched.</p>
<p>The final line is where the Keras model is updated in a single training step. The first argument is the current state &#8211; i.e. the one-hot encoded input to the model. The second is our target vector which is reshaped to make it have the required dimensions of (1, 2). The third argument tells the fit function that we only want to train for a single iteration and finally the <em>verbose</em> flag simply tells Keras not to print out the training progress.</p>
<p>Running this training over 1000 game episodes reveals the following average reward for each step in the game:</p>
<figure id="attachment_811" style="width: 455px" class="wp-caption aligncenter"><img class=" wp-image-811" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training.png" alt="Reinforcement learning Python Keras - training improvement in reward" width="455" height="341" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-80x60.png 80w" sizes="(max-width: 455px) 100vw, 455px" /><figcaption class="wp-caption-text">Reinforcement learning in Keras &#8211; average reward improvement over number of episodes trained</figcaption></figure>
<p>As can be observed, the average reward per step in the game increases over each game episode, showing that the Keras model is learning well (if a little slowly).</p>
<p>We can also run the following code to get an output of the Q values for each of the states &#8211; this is basically getting the Keras model to reproduce our explicit Q table that was generated in previous methods:</p>
<p>State 0 &#8211; action [[62.734287 61.350456]]
<p>State 1 &#8211; action [[66.317955 62.27209 ]]
<p>State 2 &#8211; action [[70.82501 63.262383]]
<p>State 3 &#8211; action [[76.63797 64.75874]]
<p>State 4 &#8211; action [[84.51073 66.499725]]
<p>This output looks sensible &#8211; we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy.</p>
<p>So there you have it &#8211; you should now be able to understand some basic concepts in reinforcement learning, and understand how to build Q learning models in Keras. This is just scraping the surface of reinforcement learning, so stay tuned for future posts on this topic (or check out the recommended course below) where more interesting games are played!</p>
<hr />
<p><strong>Recommended online course </strong>&#8211;<strong> </strong>If you&#8217;re more of a video based learner, I&#8217;d recommend the following inexpensive Udemy online course in reinforcement learning: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1080408&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fartificial-intelligence-reinforcement-learning-in-python%2F" target="new">Artificial Intelligence: Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/">Reinforcement learning tutorial using Python and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		</item>
	</channel>
</rss>
