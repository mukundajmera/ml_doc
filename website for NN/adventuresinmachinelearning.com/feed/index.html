<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Adventures in Machine Learning</title>
	<atom:link href="http://adventuresinmachinelearning.com/feed/" rel="self" type="application/rss+xml" />
	<link>http://adventuresinmachinelearning.com</link>
	<description>Learn and explore machine learning</description>
	<lastBuildDate>Sun, 09 Sep 2018 07:53:16 +0000</lastBuildDate>
	<language>en-AU</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.8</generator>
	<item>
		<title>TensorFlow Eager tutorial</title>
		<link>http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/#respond</comments>
		<pubDate>Sun, 05 Aug 2018 20:43:05 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=1013</guid>
		<description><![CDATA[<p>TensorFlow is a great deep learning framework. In fact, it is still the reigning monarch within the deep learning framework kingdom. However, it has some <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/" title="TensorFlow Eager tutorial">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/">TensorFlow Eager tutorial</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>TensorFlow is a great deep learning framework. In fact, it is still the reigning monarch within the deep learning framework kingdom. However, it has some frustrating limitations. One of these is the difficulties that arise during debugging. In TensorFlow, it&#8217;s difficult to diagnose what is happening in your model. This is due to its <em>static graph</em> structure (for details, see <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/">my TensorFlow tutorial</a>) &#8211; in TensorFlow the developer has to first create the full set of graph operations, and only then are these operations compiled with a TensorFlow session object and fed data. Wouldn&#8217;t it be great if you could define operations, then immediately run  data through them to observe what the output was? Or wouldn&#8217;t it be great to set standard Python debug breakpoints within your code, so you can step into your deep learning training loops wherever and whenever you like and examine the tensors and arrays in your models? This is now possible using the TensorFlow Eager API, available in the latest version of TensorFlow.</p>
<p>The TensorFlow Eager API allows you to dynamically create your model in an imperative programming framework. In other words, you can create tensors, operations and other TensorFlow objects by typing the command into Python, and run them straight way without the need to set up the usual session infrastructure. This is useful for debugging, as mentioned above, but also it allows dynamic adjustments of deep learning models as training progresses. In fact, in natural language processing, the ability to create dynamic graphs is useful, given that sentences and other utterances in natural language have varying lengths. In this TensorFlow Eager tutorial, I&#8217;ll show you the basics of the new API and also show how you can use it to create a fully fledged convolutional neural network.</p>
<hr />
<p><strong>Recommended video course &#8211; </strong>If you&#8217;d like to learn more about TensorFlow, and you&#8217;re more of a video learner, check out this cheap online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>TensorFlow Eager basics</h1>
<p>The first thing you need to do to use TensorFlow Eager is to enable Eager execution. To do so, you can run the following (note, you can type this directly into your Python interpreter):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import tensorflow as tf
tf.enable_eager_execution()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now you can define TensorFlow operations and run them on the fly. In the code below, a numpy range from 0 to 9 is multiplied by a scalar value of 10, using the TensorFlow <em>multiply</em> operation:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># simple example
z = tf.constant(np.arange(10))
z_tf = tf.multiply(z, np.array(10))
print(z_tf)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code snippet will output the following:</p>
<p>tf.Tensor([ 0 10 20 30 40 50 60 70 80 90], shape=(10,), dtype=int32)</p>
<p>Notice we can immediately access the results of the operation. If we ran the above without running the <em>tf.enable_eager_execution()</em> command, we would instead see the definition of the TensorFlow operation i.e.:</p>
<p>Tensor(&#8220;Mul:0&#8221;, shape=(10,), dtype=int32)</p>
<p>Notice also how easily TensorFlow Eager interacts with the numpy framework. So far, so good. Now, the main component of any deep learning API is how gradients are handled &#8211; this will be addressed in the next section.</p>
<h2>Gradients in TensorFlow Eager</h2>
<p>Gradient calculation is necessary in neural networks during the back-propagation stage (if you&#8217;d like to know more, check out <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">my neural networks tutorial</a>). The gradient calculations in the TensorFlow Eager API work similarly to the <a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener">autograd</a> package used in <a href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">PyTorch</a>. To calculate the gradient of an operation using Eager, you can use the <em>gradients_function()</em> operation. The code below calculates the gradient for an $x^3$ function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import tensorflow.contrib.eager as tfe
def f_cubed(x):
    return x**3
grad = tfe.gradients_function(f_cubed)
grad(3.)[0].numpy()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Notice the use of <em>tfe.gradients_function(f_cubed</em>) &#8211; when called, this operation will return the gradient of <em>df/dx</em> for the <em>x</em> value. The code above returns the value 27 &#8211; this makes sense as the derivative of $x^3$ is $3x^2 = 3 * 3^2 = 27$. The final line shows the <em>grad</em> operation, and then the conversion of the output to a numpy scalar i.e. a float value.</p>
<p>We can show the use of this <em>gradients_function</em> in a more complicated example &#8211; polynomial line fitting. In this example, we will use TensorFlow Eager to discover the weights of a noisy 3rd order polynomial. This is what the line looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = np.arange(0, 5, 0.1)
y = x**3 - 4*x**2 - 2*x + 2
y_noise = y + np.random.normal(0, 1.5, size=(len(x),))
plt.close(&quot;all&quot;)
plt.plot(x, y)
plt.scatter(x, y_noise)</code></pre> <div class="code-embed-infos"> </div> </div>
<figure id="attachment_1017" style="width: 380px" class="wp-caption aligncenter"><img class="size-full wp-image-1017" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial.png" alt="TensorFlow Eager tutorial - noisy polynomial" width="380" height="252" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial.png 380w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial-300x199.png 300w" sizes="(max-width: 380px) 100vw, 380px" /><figcaption class="wp-caption-text">A noisy polynomial to fit</figcaption></figure>
<p>As can be observed from the code, the polynomial is expressed as $x^3 &#8211; 4x^2 &#8211; 2x +2$ with some random noise added. Therefore, we want our code to find a &#8220;weight&#8221; vector of approximately [1, -4, -2, 2]. First, let&#8217;s define a few functions:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def get_batch(x, y, batch_size=20):
    idxs = np.random.randint(0, len(x), (batch_size))
    return x[idxs], y[idxs]

class PolyModel(object):
    def __init__(self):
        self.w = tfe.Variable(tf.random_normal([4]))
        
    def f(self, x):
        return self.w[0] * x ** 3 + self.w[1] * x ** 2 + self.w[2] * x + self.w[3]

def loss(model, x, y):
    err = model.f(x) - y
    return tf.reduce_mean(tf.square(err))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first function is a simple randomized batching function.  The second is a class definition for our polynomial model. Upon initialization, we create a weight variable <em>self.w</em> and set to a TensorFlow Eager variable type, randomly initialized as a 4 length vector. Next, we define a function <i>f</i> which returns the weight vector by the third order polynomial form. Finally, we have a loss function defined, which returns the mean squared error between the current model output and the noisy <em>y </em>vector.</p>
<p>To train the model, we can run the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = PolyModel()
grad = tfe.implicit_gradients(loss)
optimizer = tf.train.AdamOptimizer()
iters = 20000
for i in range(iters):
    x_batch, y_batch = get_batch(x, y)
    optimizer.apply_gradients(grad(model, x_batch, y_batch))
    if i % 1000 == 0:
        print(&quot;Iteration {}, loss: {}&quot;.format(i+1, loss(model, x_batch, y_batch).numpy()))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, we create a model and then use a TensorFlow Eager function called <em>implicit_gradients</em>. This function will detect any upstream or parent gradients involved in calculating the loss, which is handy. We are using a standard Adam optimizer for this task. Finally a loop begins, which supplies the batch data and the model to the gradient function. Then the program applies the returned gradients to the optimizer to perform the optimizing step.</p>
<p>After running this code, we get the following output graph:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">plt.close(&quot;all&quot;)
plt.plot(x, y)
plt.plot(x, model.f(x).numpy())
plt.scatter(x, y_noise)</code></pre> <div class="code-embed-infos"> </div> </div>
<figure id="attachment_1019" style="width: 380px" class="wp-caption aligncenter"><img class="size-full wp-image-1019" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial-fitted.png" alt="TensorFlow Eager - noisy polynomial fit" width="380" height="252" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial-fitted.png 380w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/08/Noisy-polynomial-fitted-300x199.png 300w" sizes="(max-width: 380px) 100vw, 380px" /><figcaption class="wp-caption-text">A noisy polynomial with a fitted function</figcaption></figure>
<p>The orange line is the fitted line, the blue is the &#8220;ground truth&#8221;. Not perfect, but not too bad.</p>
<p>Next, I&#8217;ll show you how to use TensorFlow Eager to create a proper neural network classifier trained on the MNIST dataset.</p>
<h1>A neural network with TensorFlow Eager</h1>
<p>In the code below, I&#8217;ll show you how to create a Convolutional Neural Network to classify MNIST images using TensorFlow Eager. If you&#8217;re not sure about Convolutional Neural Networks, you can check out <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/">my tutorial here</a>. The first part of the code shows you how to extract the MNIST dataset:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the case above, we are making use of the Keras datasets now available in TensorFlow (by the way, the Keras deep learning framework is now heavily embedded within TensorFlow &#8211; to learn more about <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/">Keras see my tutorial</a>). The raw MNIST image dataset has values ranging from 0 to 255 which represent the grayscale values &#8211; these need to be scaled to between 0 and 1. The function below accomplishes this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def scale(x, min_val=0.0, max_val=255.0):
    x = tf.to_float(x)
    return tf.div(tf.subtract(x, min_val), tf.subtract(max_val, min_val))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, in order to setup the Keras image dataset into a TensorFlow Dataset object, we use the following code. This code creates a scaled training and testing dataset. This dataset is also randomly shuffled and ready for batch extraction. It also applies the <em>tf.one_hot </em>function to the labels to convert the integer label to a one hot vector of length 10 (one for each hand-written digit). If you&#8217;re not familiar with the TensorFlow Dataset API, check out <a href="http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/">my TensorFlow Dataset tutorial</a>.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_ds = train_ds.map(lambda x, y: (scale(x), tf.one_hot(y, 10))).shuffle(10000).batch(30)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_ds = test_ds.map(lambda x, y: (scale(x), tf.one_hot(y, 10))).shuffle(10000).batch(30)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The next section of code creates the MNIST model itself, which will be trained. The best practice at the moment for TensorFlow Eager is to create a class definition for the model which inherits from the tf.keras.Model class. This is useful for a number of reasons, but the main one for our purposes is the ability to call on the model.variables property when determining Eager gradients, and this &#8220;gathers together&#8221; all the trainable variables within the model. The code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class MNISTModel(tf.keras.Model):
    def __init__(self, device=&#039;cpu:0&#039;):
        super(MNISTModel, self).__init__()
        self.device = device
        self._input_shape = [-1, 28, 28, 1]
        self.conv1 = tf.layers.Conv2D(32, 5,
                                  padding=&#039;same&#039;,
                                  activation=tf.nn.relu)
        self.max_pool2d = tf.layers.MaxPooling2D((2, 2), (2, 2), padding=&#039;same&#039;)
        self.conv2 = tf.layers.Conv2D(64, 5,
                                      padding=&#039;same&#039;,
                                      activation=tf.nn.relu)
        self.fc1 = tf.layers.Dense(750, activation=tf.nn.relu)
        self.dropout = tf.layers.Dropout(0.5)
        self.fc2 = tf.layers.Dense(10)
    
    def call(self, x):
        x = tf.reshape(x, self._input_shape)
        x = self.max_pool2d(self.conv1(x))
        x = self.max_pool2d(self.conv2(x))
        x = tf.layers.flatten(x)
        x = self.dropout(self.fc1(x))
        return self.fc2(x)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the model definition, we create layers to implement the following network structure:</p>
<ol>
<li>32 channel, 5&#215;5 convolutional layer with ReLU activation</li>
<li>2&#215;2 max pooling, with (2,2) strides</li>
<li>64 channel 5&#215;5 convolutional layer with ReLU activation</li>
<li>Flattening</li>
<li>Dense/Fully connected layer with 750 nodes, ReLU activation</li>
<li>Dropout layer</li>
<li>Dense/Fully connected layer with 10 nodes, no activation</li>
</ol>
<p>As stated above, if you&#8217;re not sure what these terms mean, see my <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/">Convolutional Neural Network tutorial</a>. Note that the <em>call </em>method is a mandatory method for the tf.keras.Model superclass &#8211; it is where the forward pass through the model is defined.</p>
<p>The next function is the loss function for the optimization:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def loss_fn(model, x, y):
    return tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits_v2(
          logits=model(x), labels=y))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that this function calls the forward pass through the model (which is an instance of our MNISTModel) and calculates the &#8220;raw&#8221; output. This raw output, along with the labels, passes through to the TensorFlow function <em>softmax_cross_entropy_with_logits_v2</em>. This applies the softmax activation to the &#8220;raw&#8221; output from the model, then creates a cross entropy loss.</p>
<p>Next, I define an accuracy function below, to keep track of how the training is progressing regarding training set accuracy, and also to check test set accuracy:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def get_accuracy(model, x, y_true):
    logits = model(x)
    prediction = tf.argmax(logits, 1)
    equality = tf.equal(prediction, tf.argmax(y_true, 1))
    accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))
    return accuracy</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Finally, the full training code for the model is shown below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = MNISTModel()
optimizer = tf.train.AdamOptimizer()
epochs = 1000
for (batch, (images, labels)) in enumerate(train_ds):
    with tfe.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.variables)
    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())
    if batch % 10 == 0:
        acc = get_accuracy(model, images, labels).numpy()
        print(&quot;Iteration {}, loss: {:.3f}, train accuracy: {:.2f}%&quot;.format(batch, loss_fn(model, images, labels).numpy(), acc*100))
    if batch &gt; epochs:
        break</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the code above, we create the model along with an optimizer. The code then enters the training loop, by iterating over the training dataset <em>train_ds</em>. Then follows the definition of the gradients for the model. Here we are using the TensorFlow Eager object called <em>GradientTape()</em>. This is an efficient way of defining the gradients over all the variables involved in the forward pass. It will track all the operations during the forward pass and will efficiently &#8220;play back&#8221; these operations during back-propagation.</p>
<p>Using the Python <em>with </em>functionality, we can include the <em>loss</em><em>_fn</em> function, and all associated upstream variables and operations, within the tape to be recorded. Then, to extract the gradients of the relevant model variables, we call <em>tape.gradient. </em>The first argument is the &#8220;target&#8221; for the calculation, i.e. the loss, and the second argument is the &#8220;source&#8221; i.e. all the model variables.</p>
<p>We then pass the gradients and the variables zipped together to the Adam optimizer for a training step. Every 10 iterations some results are printed and the training loop exits if the iterations number exceeds the maximum number of epochs.</p>
<p>Running this code for 1000 iterations will give you a loss &lt; 0.05, and training set accuracy approaching 100%. The code below calculates the test set accuracy:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">avg_acc = 0
test_epochs = 20
for (batch, (images, labels)) in enumerate(test_ds):
    avg_acc += get_accuracy(model, images, labels).numpy()
    if batch % 100 == 0 and batch != 0:
        print(&quot;Iteration:{}, Average test accuracy: {:.2f}%&quot;.format(batch, (avg_acc/batch)*100))
print(&quot;Final test accuracy: {:.2f}%&quot;.format(avg_acc/batch * 100))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>You should be able to get a test set accuracy, using the code defined above, on the order of 98% or greater for the trained model.</p>
<p>In this post, I&#8217;ve shown you the basics of using the TensorFlow Eager API for imperative deep learning. I&#8217;ve also shown you how to use the autograd-like functionality to perform a polynomial line fitting task and build a convolutional neural network which achieves relatively high test set accuracy for the MNIST classification task. Hopefully you can now use this new TensorFlow paradigm to reduce development time and enhance debugging for your future TensorFlow projects. All the best!</p>
<hr />
<p><strong>Recommended video course &#8211; </strong>If you&#8217;d like to learn more about TensorFlow, and you&#8217;re more of a video learner, check out this cheap online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/">TensorFlow Eager tutorial</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/tensorflow-eager-tutorial/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Reinforcement learning tutorial with TensorFlow</title>
		<link>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/</link>
		<comments>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/#comments</comments>
		<pubDate>Fri, 06 Jul 2018 01:16:59 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Reinforcement learning]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=959</guid>
		<description><![CDATA[<p>Reinforcement learning has gained significant attention with the relatively recent success of DeepMind&#8217;s AlphaGo system defeating the world champion Go player. The AlphaGo system was <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/" title="Reinforcement learning tutorial with TensorFlow">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/">Reinforcement learning tutorial with TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Reinforcement learning has gained significant attention with the relatively recent success of DeepMind&#8217;s AlphaGo system defeating the world champion Go player. The AlphaGo system was trained in part by reinforcement learning on deep neural networks. This type of learning is a different aspect of machine learning from the classical supervised and unsupervised paradigms. In reinforcement learning using deep neural networks, the network reacts to environmental data (called the <em>state</em>) and controls the <em>actions</em> of an <em>agent</em> to attempt to maximize a <em>reward</em>. This process allows a network to learn to play <em>games, </em>such as Atari or other video games, or any other problem that can be recast as some form of game. In this tutorial, I&#8217;ll introduce the broad concepts of Q learning, a popular reinforcement learning paradigm, and I&#8217;ll show how to implement deep Q learning in TensorFlow. If you need to get up to speed in TensorFlow, check out <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">my introductory tutorial</a>.</p>
<hr />
<p><strong>Recommended online course &#8211; </strong>If you are more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1153742&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-reinforcement-learning-in-python%2F">Advanced AI: Deep Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1153742&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Introduction to reinforcement learning</h1>
<p>As stated above, reinforcement learning comprises of a few fundamental entities or concepts. They are: an <em>environment</em> which produces a <em>state</em> and <em>reward</em>, and an <em>agent </em>which performs <em>actions</em> in the given environment. This interaction can be seen in the diagram below:</p>
<figure id="attachment_768" style="width: 381px" class="wp-caption aligncenter"><img class="size-full wp-image-768" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png" alt="Reinforcement learning with Python and Keras - Reinforcement learning environment" width="381" height="211" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png 381w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment-300x166.png 300w" sizes="(max-width: 381px) 100vw, 381px" /><figcaption class="wp-caption-text">Reinforcement learning environment</figcaption></figure>
<p>The goal of the agent in such an environment is to examine the state and the reward information it receives, and choose an action which maximizes the reward feedback it receives.  The agent learns by repeated interaction with the environment, or, in other words, repeated playing of the game.</p>
<p>To be successful, the agent needs to:</p>
<ol>
<li>Learn the interaction between states, actions and subsequent rewards</li>
<li>Determine which is the best action to choose given (1)</li>
</ol>
<p>The implementation of (1) involves determining some set of <em>values</em> which can be used to inform (2), and (2) is called the action <em>policy</em>. One of the most common ways of implementing (1) and (2) using deep learning is via the Deep Q network and the <em>epsilon-greedy </em>policy. I&#8217;ll cover both of these concepts in the next two sections.</p>
<h2>Q learning</h2>
<p>Q learning is a value based method of supplying information to inform which action an agent should take. An initially intuitive idea of creating values upon which to base actions is to create a table which sums up the rewards of taking action <em>a </em>in state <em>s</em> over multiple game plays. This could keep track of which moves are the most advantageous. For instance, let&#8217;s consider a simple game which has 3 states and two possible actions in each state &#8211; the rewards for this game can be represented in a table:</p>
<figure id="attachment_961" style="width: 246px" class="wp-caption aligncenter"><img class=" wp-image-961" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-value-table-reinforcement-learning.png" alt="Reinforcement learning - simple state-action-reward table" width="246" height="109" /><figcaption class="wp-caption-text">Simple state-action-reward table</figcaption></figure>
<p>In the table above, you can see that for this simple game, when the agent is State 1 and takes Action 2, it will receive a reward of 10 but zero reward if it takes Action 1. In State 2, the situation is reversed and finally State 3 resembles State 1. If an agent randomly explored this game, and summed up which actions received the most reward in each of the three states (storing this information in an array, say), then it would basically learn the functional form of the table above.</p>
<p>In other words, if the agent simply chooses the action which it learnt had yielded the highest reward in the past (effectively learning some form of the table above) it would have learnt how to play the game successfully. Why do we need fancy concepts such as Q learning and neural networks then, when simply creating tables by summation is sufficient?</p>
<h3>Deferred reward</h3>
<p>Well, the first obvious answer is that the game above is clearly very simple, with only 3 states and 2 actions per state. Real games are significantly more complex. The other significant concept that is missing in the example above is the idea of <em>deferred reward</em>. To adequately play most realistic games, an agent needs to learn to be able to take actions which may not immediately lead to a reward, but may result in a large reward further down the track.</p>
<p>Consider another game, defined by the table below:</p>
<figure id="attachment_965" style="width: 273px" class="wp-caption aligncenter"><img class=" wp-image-965" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning.png" alt="Reinforcement learning with TensorFlow - simple delayed reward value table" width="273" height="138" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning.png 313w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Simple-delayed-reward-value-table-reinforcement-learning-300x151.png 300w" sizes="(max-width: 273px) 100vw, 273px" /><figcaption class="wp-caption-text">Simple delayed reward value table</figcaption></figure>
<p>In the game defined above, in all states, if Action 2 is taken, the agent moves back to State 1 i.e. it goes back to the beginning. In States 1 to 3, it also receives a reward of 5 when it does so. However, in all States 1 &#8211; 3, if Action 1 is taken, the agent moves forward to the next state, but doesn&#8217;t receive a reward until it reaches State 4 &#8211; at which point it receives a reward of 20. In other words, an agent is better off if it doesn&#8217;t take Action 2 to get an instantaneous reward of 5, but rather it should choose Action 1 consistently to progress through the states to get the reward of 20. The agent needs to be able to select actions which result in a <em>delayed </em><em>reward, </em>if the delayed reward value is sufficiently large.</p>
<h3>The Q learning rule</h3>
<p>This allows us to define the Q learning rule. In deep Q learning, the neural network needs to take the current state, <em>s</em>, as a variable and return a Q value for each possible action, <em>a</em>, in that state &#8211; i.e. it needs to return $Q(s,a)$ for all <em>s</em> and <em>a</em>. This $Q(s,a)$ needs to be updated in training via the following rule:</p>
<p>$$Q(s,a) \leftarrow  Q(s,a) + \alpha [r + \gamma \max_{a&#8217;} Q(s&#8217;, a&#8217;) &#8211; Q(s,a)]$$</p>
<p>This updating rule needs a bit of unpacking. First, you can see that the new value of $Q(s,a)$ involves updating it&#8217;s current value by adding on some extra bits on the right hand side of the equation above. Moving left to right, ignore the $\alpha$ for a bit. We see inside the square brackets the first term is <em>r</em> which stands for the reward that is received for taking action <em>a</em> in state <em>s</em>. This is the immediate reward, no delayed gratification is involved yet.</p>
<p>The next term is the delayed reward calculation. First, we have the $\gamma$ value which discounts the delayed reward impact &#8211; it is always between 0 and 1. More on that in a second. The next term $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$ is the maximum Q value possible in the next state. Let&#8217;s make that a bit clearer &#8211; the agent starts in state <em>s</em>, takes action <em>a, </em>ends up in state <em>s&#8217;</em> and then the code determines the maximum Q value in state <em>s&#8217;</em>  i.e. $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$.</p>
<p>So why is the value $\max_{a&#8217;} Q(s&#8217;, a&#8217;)$ considered? It is considered because it represents the maximum future reward coming to the agent if it takes action <em>a </em>in state <em>s</em>. However, this value is discounted by $\gamma$ to take into account that it isn&#8217;t ideal for the agent to wait forever for a future reward &#8211; it is best for the agent to aim for the maximum award in the least period of time. Note that the value $Q(s&#8217;,a&#8217;)$ implicitly also holds the maximum discounted reward for the state after that, i.e. $Q(s&#8221;, a&#8221;)$ and likewise, it holds the discounted reward for the state $Q(s&#8221;&#8217;, a&#8221;&#8217;)$ and so on. This is how the agent can choose the action <em>a </em>based on not just the immediate reward <em>r</em>, but also based on possible future discounted rewards.</p>
<p>The final components in the formula above are the $\alpha$ value, which is the learning rate during the updating, and finally the current state, $Q(s,a)$, which is subtracted from the square bracket sum. This is done to normalize the updating. Both $\alpha$ and the $Q(s,a)$ subtraction are not required to be explicitly defined in deep Q learning, as the neural network will take care of that during its optimized learning process. This process will be discussed in the next section.</p>
<h3>Deep Q learning</h3>
<p>Deep Q learning applies the Q learning updating rule during the training process. In other words, a neural network is created which takes the state <em>s </em>as its input, and then the network is trained to output appropriate <em>Q(s,a)</em> values for each action in state <em>s</em>. The action <em>a </em>of the agent can then be chosen by taking the action with the greatest <em>Q(s,a)</em> value (by taking an <em>argmax</em> of the output of the neural network). This can be seen in the first step of the diagram below:</p>
<figure id="attachment_984" style="width: 1252px" class="wp-caption alignnone"><img class="size-full wp-image-984" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment.png" alt="Reinforcement learning TensorFlow - action and training steps" width="1252" height="675" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment.png 1252w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-300x162.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-768x414.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/Deep-Q-learning-environment-1024x552.png 1024w" sizes="(max-width: 1252px) 100vw, 1252px" /><figcaption class="wp-caption-text">Action selecting and training steps &#8211; Deep Q learning</figcaption></figure>
<p>Once this step has been taken and an action has been selected, the agent can perform that action. The agent will then receive feedback on what reward is received by taking that action from that state. Now, the next step that we want to perform is to train the network according to the Q learning rule. This can be seen in the second part of the diagram above. The <em>x</em> input array for training the network is the state vector <em>s</em>, and the <em>y</em> output training sample is the <em>Q(s,a) </em>vector retrieved during the action selection step. However, one of the <em>Q(s,a)</em> values, corresponding to action <em>a</em>, is set to have a target of $r + \gamma Q(s&#8217;, a&#8217;)$ &#8211; this can be observed in the figure above.</p>
<p>By training the network in this way, the <em>Q(s,a)</em> output vector from the network will over time become better at informing the agent what action will be the best to select for its long term gain. There is a bit more to the story about action selection, however, which will be discussed in the next section.</p>
<h2>The epsilon-greedy policy</h2>
<p>In the explanation above, the action selection policy was simply the action which corresponded to the highest Q output from the neural network. However, this policy isn&#8217;t the most effective. Why is that? It is because, when the neural network is randomly initialized, it will be predisposed to select certain sub-optimal actions randomly. This may cause the agent to fall into sub-optimal behavior patterns without thoroughly exploring the game and action / reward space. As such, the agent won&#8217;t find the best strategies to play the game.</p>
<p>It is useful here to introduce two concepts &#8211; <em>exploration </em>and <em>exploitation. </em>At the beginning of an optimization problem, it is best to allow the problem space to be explored extensively in the hope of finding good local (or even global) minima. However, once the problem space has been adequately searched, it is now best for the optimization algorithm to focus on exploiting what it has found by converging on the best minima to arrive at a good solution.</p>
<p>Therefore, in reinforcement learning, it is best to allow some randomness in the action selection at the beginning of the training. This randomness is determined by the <em>epsilon</em> parameter. Essentially, a random number is drawn between 0 and 1, and if it is less than <em>epsilon, </em>then a random action is selection. If not, an action is selected based on the output of the neural network. The <em>epsilon </em>variable usually starts somewhere close to 1, and is slowly decayed to somewhere around 0 during training. This allows a large exploration of the game at the beginning, but then the decay of the <em>epsilon </em>value allows the network to zero in on a good solution.</p>
<p>We&#8217;re almost at the point where we can check out the game that will be used in this example, and begin to build our deep Q network. However, there is just one final important point to consider.</p>
<h2>Batching in reinforcement learning</h2>
<p>If a deep Q network is trained at each step in the game i.e. after each action is performed and the reward collected, there is a strong risk of over-fitting in the network. This is because game play is highly correlated i.e. if the game starts from the same place and the agent performs the same actions, there will likely be similar results each time (not exactly the same though, because of randomness in some games). Therefore, after each action it is a good idea to add all the data about the state, reward, action and the new state into some sort of <em>memory</em>. This memory can then be randomly sampled in batches to avoid the risk of over-fitting.</p>
<p>The network can therefore still be trained after each step if you desire (or less frequently, it&#8217;s up to the developer), but it is extracting the training data not from the agent&#8217;s ordered steps through the game, but rather a randomized memory of previous steps and outcomes that the agent has experienced. You&#8217;ll be able to see how this works in the code below.</p>
<p>We are now ready to examine the game/environment that we will develop our network to learn.</p>
<h1>The Mountain Car Environment and Open AI Gym</h1>
<p>In this reinforcement learning tutorial, the deep Q network that will be created will be trained on the Mountain Car environment/game. This can be accessed through the open source reinforcement learning library called <a href="https://gym.openai.com/" target="_blank" rel="noopener">Open AI Gym</a>. A screen capture from the rendered game can be observed below:</p>
<figure id="attachment_992" style="width: 383px" class="wp-caption aligncenter"><img class=" wp-image-992" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame.png" alt="Reinforcement learning TensorFlow - Mountain Car game" width="383" height="258" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame.png 895w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame-300x202.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCarGame-768x517.png 768w" sizes="(max-width: 383px) 100vw, 383px" /><figcaption class="wp-caption-text">Mountain Car game</figcaption></figure>
<p>The object of this game is to get the car to go up the right-side hill to get to the flag. There&#8217;s one problem however, the car doesn&#8217;t have enough power to motor all the way up the hill. Instead, the car / agent needs to learn that it must motor up one hill for a bit, then accelerate down the hill and back up the other side, and repeat until it builds up enough momentum to make it to the top of the hill.</p>
<p>As stated above, Open AI Gym is an open source reinforcement learning package that allows developers to interact easily with games such as the Mountain Car environment. You can find details about the Mountain Car environment <a href="https://github.com/openai/gym/wiki/MountainCar-v0" target="_blank" rel="noopener">here</a>. Basically, the environment is represented by a two-element state vector, detailed below:</p>
<figure id="attachment_994" style="width: 302px" class="wp-caption aligncenter"><img class=" wp-image-994" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState.png" alt="Reinforcement learning - TensorFlow state vector" width="302" height="115" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState.png 462w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareState-300x114.png 300w" sizes="(max-width: 302px) 100vw, 302px" /><figcaption class="wp-caption-text">Mountain Car state vector</figcaption></figure>
<p>As can be observed, the agent&#8217;s state is represented by the car&#8217;s position and velocity. The goal/flag is sitting at a position = 0.5. The actions available to the agent are shown below:</p>
<figure id="attachment_995" style="width: 188px" class="wp-caption aligncenter"><img class=" wp-image-995" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/MountainCareActions.png" alt="Reinforcement learning TensorFlow - mountain car actions" width="188" height="154" /><figcaption class="wp-caption-text">Mountain Car actions</figcaption></figure>
<p>As can be observed, there are three actions available to the agent &#8211; accelerate to the left, right and no acceleration.</p>
<p>In the game&#8217;s default arrangement, for each time step where the car&#8217;s position is &lt;0.5, it receives a reward of -1, up to a maximum of 200 time steps. So the incentive for the agent is to get the car&#8217;s position to &gt;0.5 as soon as possible, after which the game ends. This will minimize the negative reward, which is the aim of the game.</p>
<p>However, in this default arrangement, it will take a significant period of time of random exploration before the car stumbles across the positive feedback of getting to the flag. As such, to speed things up a bit, in this example we&#8217;ll alter the reward structure to:</p>
<ul>
<li>Position &gt; 0.1, r += 10</li>
<li>Position &gt; 0.25 r += 20</li>
<li>Position &gt; 0.5 r += 100</li>
</ul>
<p>This new reward structure gives the agent better positive feedback when it starts learning how to ascend the hill on the right hand side toward the flag. The position of 0.1 is just over half way up the right-hand hill.</p>
<p>Ok, so now you know the environment, let&#8217;s write some code!</p>
<h1>Reinforcement learning in TensorFlow</h1>
<p>In this reinforcement learning implementation in TensorFlow, I&#8217;m going to split the code up into three main classes, these classes are:</p>
<ul>
<li>Model: This class holds the TensorFlow operations and model definitions</li>
<li>Memory: This class is where the memory of the actions, rewards and states are stored and retrieved from</li>
<li>GameRunner: This class is the main training and agent control class</li>
</ul>
<p>As stated before, I&#8217;ll be assuming some prior knowledge of TensorFlow here. If you&#8217;re not up to speed your welcome to wing it. Otherwise check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a>. All the code for this tutorial can be found on <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github repository</a>.</p>
<p>I&#8217;ll go through each of the classes in turn in the sub-sections below.</p>
<h2>The Model class</h2>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Model:
    def __init__(self, num_states, num_actions, batch_size):
        self._num_states = num_states
        self._num_actions = num_actions
        self._batch_size = batch_size
        # define the placeholders
        self._states = None
        self._actions = None
        # the output operations
        self._logits = None
        self._optimizer = None
        self._var_init = None
        # now setup the model
        self._define_model()

    def _define_model(self):
        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)
        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)
        # create a couple of fully connected hidden layers
        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)
        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)
        self._logits = tf.layers.dense(fc2, self._num_actions)
        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)
        self._optimizer = tf.train.AdamOptimizer().minimize(loss)
        self._var_init = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first function within the class is of course the initialization function. All you need to pass into the Model definition is the number of states of the environment (2 in this game), the number of possible actions (3 in this game) and the batch size. The function simply sets up a few internal variables and operations, some of which are exposed as public properties later in the class definition. At the end of the initialization, the second method displayed above <em>_define_model() </em>is called. This method sets up the model structure and the main operations.</p>
<p>First, two placeholders are created <em>_states</em> and <em>_q_s_a</em> &#8211; these hold the state data and the $Q(s,a)$ training data respectively. The first dimension of these placeholders is set to <em>None, </em>so that it will automatically adapt when a batch of training data is fed into the model and also when single predictions from the model are required. The next lines create two fully connected layers <em>fc1 </em>and <em>fc2 </em>using the handy TensorFlow layers module. These hidden layers have 50 nodes each, and they are activated using the ReLU activation function (if you want to know more about the ReLU, check out my <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">vanishing gradient and ReLU tutorial</a>).</p>
<p>The next layer is the output layer <em>_logits</em> &#8211; this is another fully connected or dense layer, but with no activation supplied. When no activation function is supplied to the dense layer API in TensorFlow, it defaults to a &#8216;linear&#8217; activation i.e. no activation. This is what we want, as we want the network to learn continuous $Q(s,a)$ values across all possible real numbers.</p>
<p>Next comes the <em>loss </em>&#8211; this isn&#8217;t a classification problem, so a good loss to use is simply a mean squared error loss. The next line specifies the optimizer &#8211; in this example, we&#8217;ll just use the generic Adam optimizer. Finally, the TensorFlow boiler plate global variable initializer operation is assigned to <em>_var_init</em>.</p>
<p>So far so good. Next, some methods of the Model class are created to perform prediction and training:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def predict_one(self, state, sess):
        return sess.run(self._logits, feed_dict={self._states:
                                                     state.reshape(1, self.num_states)})

    def predict_batch(self, states, sess):
        return sess.run(self._logits, feed_dict={self._states: states})

    def train_batch(self, sess, x_batch, y_batch):
        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first method <em>predict_one</em> simply returns the output of the network (i.e. by calling the <em>_logits</em> operation) with an input of a single state. Note the reshaping operation that is used to ensure that the data has a size (1, num_states). This is called whenever action selection by the agent is required. The next method, <em>predict_batch</em> predicts a whole batch of outputs when given a whole bunch of input states &#8211; this is used to perform batch evaluation of $Q(s,a)$ and $Q(s&#8217;,a&#8217;)$ values for training. Finally, there is a method called <em>train_batch</em> which takes a batch training step of the network.</p>
<p>That&#8217;s the Model class, now it is time to consider the Memory class.</p>
<h2>The Memory class</h2>
<p>The next class to consider in the code is the Memory class &#8211; this class stores all the results of the action of the agent in the game, and also handles the retrieval. These can be used to batch train the network.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Memory:
    def __init__(self, max_memory):
        self._max_memory = max_memory
        self._samples = []

    def add_sample(self, sample):
        self._samples.append(sample)
        if len(self._samples) &gt; self._max_memory:
            self._samples.pop(0)

    def sample(self, no_samples):
        if no_samples &gt; len(self._samples):
            return random.sample(self._samples, len(self._samples))
        else:
            return random.sample(self._samples, no_samples)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, when the Memory class is initialized, it is necessary to supply a maximum memory argument &#8211; this will control the maximum number of (state, action, reward, next_state) tuples the <em>_samples</em> list can hold. The bigger the better, as it ensures better random mixing of the samples, but you have to make sure you don&#8217;t run into memory errors.</p>
<p>The first method, <em>add_sample</em> takes an individual (state, action, reward, next_state) tuple and appends it to the <em>_samples </em>list. After this, a check is made &#8211; if the number of samples is now larger than the allowable memory size, the first element in <em>_samples</em> is removed using the Python .pop() list functionality.</p>
<p>The final method, <em>sample</em> returns a random selection of <em>no_samples</em> in length. However, if the <em>no_samples</em> argument is larger than the actual memory, whatever is available in the memory is returned.</p>
<p>The final class is called GameRunner.</p>
<h2>The GameRunner class</h2>
<p>The GameRunner class in this example is where all the model dynamics, agent action and training is organised.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class GameRunner:
    def __init__(self, sess, model, env, memory, max_eps, min_eps,
                 decay, render=True):
        self._sess = sess
        self._env = env
        self._model = model
        self._memory = memory
        self._render = render
        self._max_eps = max_eps
        self._min_eps = min_eps
        self._decay = decay
        self._eps = self._max_eps
        self._steps = 0
        self._reward_store = []
        self._max_x_store = []</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the GameRunner initialization, some internal variables are created. Note, it takes as first argument a TensorFlow session object, then a neural network Model, an Open AI gym environment and a Memory class instance. The next arguments <em>max_eps </em>and <em>min_eps</em> dictate the maximum and minimum epsilon values respectively &#8211; during training the actual $\epsilon$ will decay from the maximum to the minimum based on the following argument <em>decay</em>. Finally, <em>render </em>is a boolean which determines whether the game environment is rendered to the screen.</p>
<p>The next method is <em>run()</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def run(self):
        state = self._env.reset()
        tot_reward = 0
        max_x = -100
        while True:
            if self._render:
                self._env.render()

            action = self._choose_action(state)
            next_state, reward, done, info = self._env.step(action)
            if next_state[0] &gt;= 0.1:
                reward += 10
            elif next_state[0] &gt;= 0.25:
                reward += 20
            elif next_state[0] &gt;= 0.5:
                reward += 100

            if next_state[0] &gt; max_x:
                max_x = next_state[0]
            # is the game complete? If so, set the next state to
            # None for storage sake
            if done:
                next_state = None

            self._memory.add_sample((state, action, reward, next_state))
            self._replay()

            # exponentially decay the eps value
            self._steps += 1
            self._eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) \
                                      * math.exp(-LAMBDA * self._steps)

            # move the agent to the next state and accumulate the reward
            state = next_state
            tot_reward += reward

            # if the game is done, break the loop
            if done:
                self._reward_store.append(tot_reward)
                self._max_x_store.append(max_x)
                break

        print(&quot;Step {}, Total reward: {}, Eps: {}&quot;.format(self._steps, tot_reward, self._eps))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We&#8217;ll go through each step in the code above. First, the environment is reset by calling the Open AI Gym command .reset(). Then an infinite loop is entered into &#8211; this will be exited by calling a <em>break</em> command. If the boolean _<em>render</em> is True, then the output of the game will be shown on the screen. The <em>action</em> of the agent is determined by calling the internal method <em>_choose_action(state)</em> &#8211; this will discussed later. Next, the agent takes <em>action </em>by calling the Open AI Gym command <em>step(action)</em>. This command returns a tuple containing the new state of the agent, the reward received by taking <em>action</em>, a <em>done </em>boolean indicating whether the game has finished, and an information object (we won&#8217;t using <em>info </em>in this example).</p>
<p>The next step in the code is where there are some manual adjustments to the Mountain Car reward system. If you recall, earlier I mentioned that in order to speed up the training of the network, it was useful to add some more reward steps the closer the car got to the goal (rather than the default reward which was only received when the car reached the goal/flag). The maximum <em>x</em> value achieved in the given episode is also tracked and this will be stored once the game is complete.</p>
<p>The next step is a check to see if the game has completed i.e. <em>done == True</em> &#8211; this will occur after 200 turns. If it has completed, we want to set the <em>next_state</em> to None. This will be picked up during the training / replay step of the class, and the state will be set to an array of zeros whenever <em>next_state</em> is equal to None.</p>
<p>After this, the data about the agent is stored in the memory class &#8211; i.e.its original <em>state</em>, its chosen <em>action</em>, the <em>reward </em>it received for that action and finally the <em>next_state</em> of the agent. After this takes place, the training / replay step of the deep Q network is run &#8211; this step will be discussed more below. At this point the <em>epsilon </em>value is also exponentially decayed. Finally, the agent&#8217;s state is moved to <em>next_state</em>, the total reward during the game is accumulated, and there is some printing and breaking of the loop and storing of relevant variables if the game is complete.</p>
<p>The next part of the GameRunner class is the agent action selection method:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _choose_action(self, state):
        if random.random() &lt; self._eps:
            return random.randint(0, self._model.num_actions - 1)
        else:
            return np.argmax(self._model.predict_one(state, self._sess))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This method executes our <em>epsilon </em>greedy + Q policy. In the first case, if a random number is less than the <em>_eps</em> value, then the returned action will simply be an action chosen at random from the set of possible actions. Otherwise, the action will be chosen based on an argmax of the output from the neural network. Recall that _<em>predict_one</em> from the model will take a single state as input, then output $Q(s,a)$ values for each of the possible actions available &#8211; the action with the highest $Q(s,a)$ value is that action with the highest expected current + future discounted reward.</p>
<p>The final method within the GameRunner class is the <em>_replay</em> method, where the batching and training takes place:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _replay(self):
        batch = self._memory.sample(self._model.batch_size)
        states = np.array([val[0] for val in batch])
        next_states = np.array([(np.zeros(self._model.num_states)
                                 if val[3] is None else val[3]) for val in batch])
        # predict Q(s,a) given the batch of states
        q_s_a = self._model.predict_batch(states, self._sess)
        # predict Q(s&#039;,a&#039;) - so that we can do gamma * max(Q(s&#039;a&#039;)) below
        q_s_a_d = self._model.predict_batch(next_states, self._sess)
        # setup training arrays
        x = np.zeros((len(batch), self._model.num_states))
        y = np.zeros((len(batch), self._model.num_actions))
        for i, b in enumerate(batch):
            state, action, reward, next_state = b[0], b[1], b[2], b[3]
            # get the current q values for all actions in state
            current_q = q_s_a[i]
            # update the q value for action
            if next_state is None:
                # in this case, the game completed after action, so there is no max Q(s&#039;,a&#039;)
                # prediction possible
                current_q[action] = reward
            else:
                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])
            x[i] = state
            y[i] = current_q
        self._model.train_batch(self._sess, x, y)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step in the <em>_replay</em> method is to retrieve a randomized batch of data from memory. Next, we want to setup our batch state variables so that we can:</p>
<ol>
<li>For each state, produce baseline $Q(s,a)$ values &#8211; one of which will be given a target of $r + \gamma \max_{a&#8217;} Q(s&#8217;, a&#8217;)$</li>
<li> For each <em>next_state</em>, predict $Q(s&#8217;,a&#8217;)$ from the model, as required in (1)</li>
</ol>
<p>Now, if you recall, each sample in memory has the form of a tuple: <em>state, action, reward, next_state </em>which was extracted from the game play. To setup a batch of initial states, then, we simply use Python list comprehension to extract the first tuple value from each sample in the batch. Likewise, we do the same for the fourth value in the tuple to extract the <em>next_state</em> value for each sample in the batch. Note that whenever the <em>next_state</em> corresponds to a case where the game finished (i.e. <em>next_state </em>is None) the next state value is replaced by a vector of zeros corresponding in size to the number of states in the game.</p>
<p>Next, the batch of $Q(s, a)$ and $Q(s&#8217;,a&#8217;)$ values are extracted from the model from <em>states </em>and <em>next_states</em> respectively. The <em>x </em>and <em>y</em> training arrays are then created, but initially filled with zeros. After this, a loop is entered into to accumulate the <em>x </em>and <em>y</em> values on which to train the model. Within this loop, we extract the memory values from the batch, then set a variable designating the Q values for the current state. If the <em>next_state</em> value is actually zero, there is no discounted future rewards to add, so the <em>current_q</em> corresponding to <em>action</em> is set a target of the <em>reward</em> only. Alternatively, if there is a valid <em>next_state, </em>then the <em>current_q</em> corresponding to <em>action </em>is set a target of the <em>reward</em> plus the discounted future reward i.e. $max_{a&#8217;} Q(s&#8217;, a&#8217;)$.</p>
<p>The <em>state</em> and <em>current_q</em> are then loaded into the <em>x </em>and <em>y </em>values for the given batch, until the batch data is completely extracted. Then the network is trained by calling <em>_train_batch()</em> on the model.</p>
<p>That completes the review of the main classes within the TensorFlow reinforcement learning example. All that is left is to setup the classes and enter the training loop.</p>
<h2>The main function</h2>
<p>The code below sets up the environment and the classes, and runs multiple games to perform the learning:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if __name__ == &quot;__main__&quot;:
    env_name = &#039;MountainCar-v0&#039;
    env = gym.make(env_name)

    num_states = env.env.observation_space.shape[0]
    num_actions = env.env.action_space.n

    model = Model(num_states, num_actions, BATCH_SIZE)
    mem = Memory(50000)

    with tf.Session() as sess:
        sess.run(model.var_init)
        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,
                        LAMBDA)
        num_episodes = 300
        cnt = 0
        while cnt &lt; num_episodes:
            if cnt % 10 == 0:
                print(&#039;Episode {} of {}&#039;.format(cnt+1, num_episodes))
            gr.run()
            cnt += 1
        plt.plot(gr.reward_store)
        plt.show()
        plt.close(&quot;all&quot;)
        plt.plot(gr.max_x_store)
        plt.show()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first couple of lines, we create an Open AI Gym Mountain Car environment. Next, the number of states and actions are extracted from the environment object itself.</p>
<p>The network model and memory objects are then created &#8211; in this case, we&#8217;re using a batch size of 50 and a total number of samples in the memory of 50,000.</p>
<p>The TensorFlow session object is created, along with the variable initialization &#8211; then the GameRunner class is created. The number of episodes of the Mountain Car game which will be run in this training example is 300. For each of these episodes, we run the game by using the GameRunner <em>run()</em> method.</p>
<p>After all the episodes are run, some plotting is performed on the total reward for each episode, and the maximum x-axis value the cart reaches in the game (remembering that the goal is at x = 0.5). These plots can be observed below:</p>
<figure id="attachment_1006" style="width: 473px" class="wp-caption aligncenter"><img class=" wp-image-1006" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards.png" alt="Mountain Car rewards - from the TensorFlow reinforcement learning example" width="473" height="355" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarRewards-80x60.png 80w" sizes="(max-width: 473px) 100vw, 473px" /><figcaption class="wp-caption-text">The Mountain Car rewards from the TensorFlow reinforcement learning example</figcaption></figure>
<p>As can be observed, the network starts out controlling the agent rather poorly, while it is exploring the environment and accumulating memory. However once it starts to receive positive rewards by ascending the right-hand hill, the rewards rapidly increase.</p>
<figure id="attachment_1007" style="width: 485px" class="wp-caption aligncenter"><img class=" wp-image-1007" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX.png" alt="Mountain Car maximum X - from the TensorFlow reinforcement learning example" width="485" height="364" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/07/MountainCarMaxX-80x60.png 80w" sizes="(max-width: 485px) 100vw, 485px" /><figcaption class="wp-caption-text">The Mountain Car maximum x values from the TensorFlow reinforcement learning example</figcaption></figure>
<p>As can be observed above, while there is some volatility, the network learns that the best rewards are achieved by reaching the top of the right-hand hill and, towards the end of the training, consistently controls the car/agent to reach there.</p>
<p>This reinforcement learning tutorial in TensorFlow has shown you:</p>
<ol>
<li>The basics of Q learning</li>
<li>The epsilon greed action selection policy</li>
<li> The importance of batching in training deep Q reinforcement learning networks, and</li>
<li>How to implement a deep Q reinforcement learning network in TensorFlow</li>
</ol>
<p>I hope it has been instructive &#8211; keep an eye out for future tutorials in reinforcement learning where more complicated games and techniques will be reviewed.</p>
<hr />
<p><strong>Recommended online course &#8211; </strong>If you are more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1153742&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdeep-reinforcement-learning-in-python%2F">Advanced AI: Deep Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1153742&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/">Reinforcement learning tutorial with TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/reinforcement-learning-tensorflow/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Convolutional Neural Networks Tutorial in PyTorch</title>
		<link>http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/</link>
		<comments>http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/#comments</comments>
		<pubDate>Sat, 16 Jun 2018 06:29:54 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Convolutional Neural Networks]]></category>
		<category><![CDATA[PyTorch]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=922</guid>
		<description><![CDATA[<p>In a previous introductory tutorial on neural networks, a three layer neural network was developed to classify the hand-written digits of the MNIST dataset. In <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/" title="Convolutional Neural Networks Tutorial in PyTorch">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/">Convolutional Neural Networks Tutorial in PyTorch</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In a <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">previous introductory tutorial on neural networks</a>, a three layer neural network was developed to classify the hand-written digits of the <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="noopener">MNIST</a> dataset. In the end, it was able to achieve a classification accuracy around 86%. For a simple data set such as MNIST, this is actually quite poor. Further optimizations can bring densely connected networks of a modest size up to 97-98% accuracy. This is significantly better, but still not that great for MNIST. We need something more state-of-the-art, some method which can truly be called <em>deep learning</em>. This tutorial will present just such a <em>deep learning </em>method that can achieve very high accuracy in image classification tasks &#8211;  the Convolutional Neural Network. In particular, this tutorial will show you both the theory and practical application of Convolutional Neural Networks in PyTorch.</p>
<p>PyTorch is a powerful deep learning framework which is rising in popularity, and it is thoroughly at home in Python which makes rapid prototyping very easy. This tutorial won&#8217;t assume much in regards to prior knowledge of PyTorch, but it might be helpful to checkout my <a href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">previous introductory tutorial to PyTorch</a>. All the code for this Convolutional Neural Networks tutorial can be found on this site&#8217;s Github repository &#8211; found <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">here</a>. Let&#8217;s get to it.</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Why Convolutional Neural Networks?</h1>
<p>Fully connected networks with a few layers can only do so much &#8211; to get close to state-of-the-art results in image classification it is necessary to go <em>deeper.</em> In other words, lots more layers are required in the network. However, by adding a lot of additional layers, we come across some problems. First, we can run into the <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">vanishing gradient problem</a>. &#8211; however, this can be solved to an extent by using sensible activation functions, such as the ReLU family of activations. Another issue for deep fully connected networks is that the number of trainable parameters in the model (i.e. the weights) can grow rapidly. This means that the training slows down or becomes practically impossible, and also exposes the model to overfitting. So what&#8217;s a solution?</p>
<p>Convolutional Neural Networks try to solve this second problem by exploiting correlations between adjacent inputs in images (or time series). For instance, in an image of a cat and a dog, the pixels close to the cat&#8217;s eyes are more likely to be correlated with the nearby pixels which show the cat&#8217;s nose &#8211; rather than the pixels on the other side of the image that represent the dog&#8217;s nose. This means that not every node in the network needs to be connected to every other node in the next layer &#8211; and this cuts down the number of weight parameters required to be trained in the model. Convolution Neural Networks also have some other tricks which improve training, but we&#8217;ll get to these in the next section.</p>
<h1>How does a Convolutional Neural Network work?</h1>
<p>The first thing to understand in a Convolutional Neural Network is the actual <em>convolution</em> part. This is a fancy mathematical word for what is essentially a moving window or filter across the image being studied. This moving window applies to a certain neighborhood of nodes as shown below &#8211; here, the filter applied is (0.5 $\times$ the node value):</p>
<figure id="attachment_262" style="width: 632px" class="wp-caption aligncenter"><img class=" wp-image-262" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter.jpg" alt="Convolutional neural network tutorial - moving filter" width="632" height="269" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter.jpg 733w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-300x128.jpg 300w" sizes="(max-width: 632px) 100vw, 632px" /><figcaption class="wp-caption-text">Moving 2&#215;2 filter (all weights = 0.5)</figcaption></figure>
<p>Only two outputs have been shown in the diagram above, where each output node is a map from a 2 x 2 input square. The weight of the mapping of each input square, as previously mentioned, is 0.5 across all four inputs. So the output can be calculated as:</p>
<p>$$\begin{align}<br />
out_1 &amp;= 0.5 in_1 + 0.5 in_2 + 0.5 in_6 + 0.5 in_7 \\<br />
&amp;= 0.5 \times 2.0 + 0.5 \times 3.0 + 0.5 \times 2.0 + 0.5 \times 1.5  \\<br />
&amp;= 4.25 \\<br />
out_2 &amp;= 0.5 in_2 + 0.5 in_3 + 0.5 in_7 + 0.5 in_8 \\<br />
&amp;= 0.5 \times 3.0 + 0.5 \times 0.0 + 0.5 \times 1.5 + 0.5 \times 0.5  \\<br />
&amp;= 2.5 \\<br />
\end{align}$$</p>
<p>In the convolutional part of the neural network, we can imagine this 2 x 2 moving filter sliding across all the available nodes / pixels in the input image. This operation can also be illustrated using standard neural network node diagrams:</p>
<figure id="attachment_269" style="width: 349px" class="wp-caption aligncenter"><img class=" wp-image-269" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-node-diagram.jpg" alt="Convolutional neural network tutorial - moving filter node diagram" width="349" height="379" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-node-diagram.jpg 445w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-node-diagram-276x300.jpg 276w" sizes="(max-width: 349px) 100vw, 349px" /><figcaption class="wp-caption-text">Moving 2&#215;2 filter &#8211; node diagram</figcaption></figure>
<p>The first position of the moving filter connections is illustrated by the blue connections, and the second is shown with the green lines. The weights of each of these connections, as stated previously, is 0.5.</p>
<p>There are a few things in this convolutional step which improve training by reducing parameters/weights:</p>
<ul>
<li><em>Sparse </em>connections &#8211; not every node in the first / input layer is connected to every node in the second layer. This is contrary to fully connected neural networks, where every node is connected to every other in the following layer.</li>
<li>Constant filter parameters &#8211; each filter has constant parameters. In other words, as the filter moves around the image, the same weights are applied to each 2 x 2 set of nodes. Each filter, as such, can be trained to perform a certain specific transformation of the input space. Therefore, each filter has a certain set of weights that are applied for each convolution operation &#8211; this reduces the number of parameters.
<ul>
<li>Note &#8211; this is not to say that each weight is constant <em>within </em>the filter. In the example above, the weights were [0.5, 0.5, 0.5, 0.5] but could have just as easily been something like [0.25, 0.1, 0.8, 0.001]. It all depends on how each filter is trained</li>
</ul>
</li>
</ul>
<p>These two properties of Convolutional Neural Networks can drastically reduce the number of parameters which need to be trained compared to fully connected neural networks.</p>
<p>The next step in the Convolutional Neural Network structure is to pass the output of the convolution operation through a non-linear activation function &#8211; generally some version of the <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/">ReLU activation function</a>. This provides the standard non-linear behavior that neural networks are known for.</p>
<p>The process involved in this convolutional block is often called <em>feature mapping &#8211; </em>this refers to the idea that each convolutional filter can be trained to &#8220;search&#8221; for different features in an image, which can then be used in classification. Before we move onto the next main feature of Convolutional Neural Networks, called <em>pooling</em>, we will examine this idea of feature mapping and <em>channels</em> in the next section.</p>
<h2>Feature mapping and multiple channels</h2>
<p>As mentioned previously, because the weights of individual filters are held constant as they are applied over the input nodes, they can be trained to select certain features from the input data. In the case of images, it may learn to recognize common geometrical objects such as lines, edges and other shapes which make up objects. This is where the name <em>feature mapping</em> comes from. Because of this, any convolution layer needs multiple filters which are trained to detect different features. So therefore, the previous moving filter diagram needs to be updated to look something like this:</p>
<figure id="attachment_273" style="width: 581px" class="wp-caption aligncenter"><img class=" wp-image-273" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-with-multiple-filters.jpg" alt="Convolutional neural networks tutorial - multiple filters" width="581" height="235" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-with-multiple-filters.jpg 771w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-with-multiple-filters-300x121.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Moving-filter-with-multiple-filters-768x311.jpg 768w" sizes="(max-width: 581px) 100vw, 581px" /><figcaption class="wp-caption-text">Multiple convolutional filters</figcaption></figure>
<p>Now you can see on the right hand side of the diagram above that there are multiple, stacked outputs from the convolution operation. This is because there are multiple trained filters which produce their own 2D output (for a 2D image). These multiple filters are commonly called <em>channels</em> in deep learning. Each of these channels will end up being trained to detect certain key features in the image. The output of a convolution layer, for a gray-scale image like the MNIST dataset, will therefore actually have 3 dimensions &#8211; 2D for each of the channels, then another dimension for the number of different channels.</p>
<p>If the input is itself multi-channelled, as in the case of a color RGB image (one channel for each R-G-B), the output will actually be <strong>4D</strong>. Thankfully, any deep learning library worth its salt, PyTorch included, will be able to handle all this mapping easily for you. Finally, don&#8217;t forget that the output of the convolution operation will be passed through an activation for each node.</p>
<p>Now, the next vitally important part of Convolutional Neural Networks is a concept called <em>pooling</em>.</p>
<h2>Pooling</h2>
<p>There are two main benefits to pooling in Convolutional Neural Networks. These are:</p>
<ul>
<li>It reduces the number of parameters in your model by a process called <em>down-sampling</em></li>
<li>It makes feature detection more robust to object orientation and scale changes</li>
</ul>
<p>So what is pooling? It is another sliding window type technique, but instead of applying weights, which can be trained, it applies a statistical function of some type over the contents of its window. The most common type of pooling is called <em>max pooling</em>, and it applies the <em>max()</em> function over the contents of the window. There are other variants such as <em>mean pooling</em> (which takes the statistical mean of the contents) which are also used in some cases. In this tutorial, we will be concentrating on max pooling. The diagram below shows an example of the max pooling operation:</p>
<figure id="attachment_278" style="width: 588px" class="wp-caption aligncenter"><img class=" wp-image-278" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Max-pooling.jpg" alt="Convolutional neural network tutorial - max pooling example" width="588" height="265" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Max-pooling.jpg 807w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Max-pooling-300x135.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Max-pooling-768x346.jpg 768w" sizes="(max-width: 588px) 100vw, 588px" /><figcaption class="wp-caption-text">Max pooling example (with padding)</figcaption></figure>
<p>We&#8217;ll go through a number of points relating to the diagram above:</p>
<h3>The basics</h3>
<p>In the diagram above, you can observe the max pooling taking effect. For the first window, the blue one, you can see that the max pooling outputs a 3.0 which is the maximum node value in the 2&#215;2 window. Likewise for the green 2&#215;2 window it outputs the maximum of 5.0 and a maximum of 7.0 for the red window. This is pretty straight-forward.</p>
<h3>Strides and down-sampling</h3>
<p>In the pooling diagram above, you will notice that the pooling window shifts to the right each time by 2 places. This is called a <em>stride</em> of 2. In the diagram above, the stride is only shown in the <em>x</em> direction, but, if the goal was to prevent pooling window overlap, the stride would also have to be 2 in the <em>y</em> direction as well. In other words, the stride is actually specified as [2, 2]. One important thing to notice is that, if during pooling the stride is greater than 1, then the output size will be reduced. As can be observed above, the 5 x 5 input is reduced to a 3 x 3 output. This is a good thing &#8211; it is called down-sampling, and it reduces the number of trainable parameters in the model.</p>
<h3>Padding</h3>
<p>Another thing to notice in the pooling diagram above is that there is an extra column and row added to the 5 x 5 input &#8211; this makes the effective size of the pooling space equal to 6 x 6. This is to ensure that the 2 x 2 pooling window can operate correctly with a stride of [2, 2] and is called <em>padding</em>. These nodes are basically dummy nodes &#8211; because the values of these dummy nodes is 0, they are basically invisible to the max pooling operation. Padding will need to be considered when constructing our Convolutional Neural Network in PyTorch.</p>
<p>Ok, so now we understand how pooling works in Convolutional Neural Networks, and how it is useful in performing down-sampling, but what else does it do? Why is max pooling used so frequently?</p>
<h3>Why is pooling used in convolutional neural networks?</h3>
<p>In addition to the function of down-sampling, pooling is used in Convolutional Neural Networks to make the detection of certain features somewhat invariant to scale and orientation changes. Another way of thinking about what pooling does is that it generalizes over lower level, more complex information. Let&#8217;s imagine the case where we have convolutional filters that, during training, learn to detect the digit &#8220;9&#8221; in various orientations within the input images. In order for the Convolutional Neural Network to learn to classify the appearance of &#8220;9&#8221; in the image correctly, it needs to in some way &#8220;activate&#8221; whenever a &#8220;9&#8221; is found anywhere in the image, no matter what the size or orientation the digit is (except for when it looks like &#8220;6&#8221;, that is). Pooling can assist with this higher level, generalized feature selection, as the diagram below shows:</p>
<figure id="attachment_289" style="width: 521px" class="wp-caption aligncenter"><img class=" wp-image-289" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Pooling-demonstration-v2.jpg" alt="Convolutional neural networks tutorial - stylised representation of pooling" width="521" height="198" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Pooling-demonstration-v2.jpg 793w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Pooling-demonstration-v2-300x114.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Pooling-demonstration-v2-768x292.jpg 768w" sizes="(max-width: 521px) 100vw, 521px" /><figcaption class="wp-caption-text">Stylized representation of pooling</figcaption></figure>
<p>The diagram is a stylized representation of the pooling operation. If we consider that a small region of the input image has a digit &#8220;9&#8221; in it (green box) and assume we are trying to detect such a digit in the image, what will happen is that, if we have a few convolutional filters, they will learn to activate (via the ReLU) when they &#8220;see&#8221; a &#8220;9&#8221; in the image (i.e. return a large output). However, they will activate more or less strongly depending on what orientation the &#8220;9&#8221; is. We want the network to detect a &#8220;9&#8221; in the image regardless of what the orientation is and this is where the pooling comes it. It &#8220;looks&#8221; over the output of these three filters and gives a high output so long as <em>any one</em> of these filters has a high activation.</p>
<p>Therefore, pooling acts as a <em>generalizer</em> of the lower level data, and so, in a way, enables the network to move from high resolution <em>data </em>to lower resolution <em>information. </em>In other words, pooling coupled with convolutional filters attempts to detect <em>objects</em> within an image.</p>
<h2>The final picture</h2>
<p>The image below from Wikipedia shows the structure of a fully developed Convolutional Neural Network:</p>
<figure id="attachment_290" style="width: 576px" class="wp-caption aligncenter"><img class=" wp-image-290" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn.png" alt="Convolutional neural networks tutorial - full diagram" width="576" height="177" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn.png 1040w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn-300x92.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn-768x236.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/Typical_cnn-1024x315.png 1024w" sizes="(max-width: 576px) 100vw, 576px" /><figcaption class="wp-caption-text">Full convolutional neural network &#8211; By Aphex34 (Own work) [<a href="http://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>], <a href="https://commons.wikimedia.org/wiki/File%3ATypical_cnn.png">via Wikimedia Commons</a></figcaption></figure>If you work the image above from left to right, we first see that there is an image of a robot. Then &#8220;scanning&#8221; over this image are a series of convolutional filters or feature maps. The output of these filters is then sub-sampled by pooling operations. After this, there is another set of convolutions and pooling on the output of the first convolution-pooling operation. Finally, at the output there is &#8220;attached&#8221; a fully connected layer. The purpose of this fully connected layer at the output of the network requires some explanation.</p>
<h3>The fully connected layer</h3>
<p>As previously discussed, a Convolutional Neural Network takes high resolution data and effectively resolves that into representations of objects. The fully connected layer can therefore be thought of as attaching a standard classifier onto the information-rich output of the network, to &#8220;interpret&#8221; the results and finally produce a classification result. In order to attach this fully connected layer to the network, the dimensions of the output of the Convolutional Neural Network need to be flattened.</p>
<p>Consider the previous diagram &#8211; at the output, we have multiple channels of <em>x </em>x <em>y</em> matrices/tensors. These channels need to be flattened to a single (N X 1) tensor. Consider an example &#8211; let&#8217;s say we have 100 channels of 2 x 2 matrices, representing the output of the final pooling operation of the network. Therefore, this needs to be flattened to 2 x 2 x 100 = 400 rows. This can be easily performed in PyTorch, as will be demonstrated below.</p>
<p>Now the basics of Convolutional Neural Networks has been covered, it is time to show how they can be implemented in PyTorch.</p>
<h1>Implementing Convolutional Neural Networks in PyTorch</h1>
<p>Any deep learning framework worth its salt will be able to easily handle Convolutional Neural Network operations. PyTorch is such a framework. In this section, I&#8217;ll show you how to create Convolutional Neural Networks in PyTorch, going step by step. Ideally, you will already have some notion of the basics of PyTorch (if not, you can check out my <a href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">introductory PyTorch tutorial</a>) &#8211; otherwise, you&#8217;re welcome to wing it. The network we&#8217;re going to build will perform MNIST digit classification. The full code for the tutorial can be found at <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github repository</a>.</p>
<p>The Convolutional Neural Network architecture that we are going to build can be seen in the diagram below:</p>
<figure id="attachment_293" style="width: 1725px" class="wp-caption alignnone"><img class="size-full wp-image-293" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/CNN-example-block-diagram.jpg" alt="PyTorch CNN tutorial - network" width="1725" height="572" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/CNN-example-block-diagram.jpg 1725w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/CNN-example-block-diagram-300x99.jpg 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/CNN-example-block-diagram-768x255.jpg 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/04/CNN-example-block-diagram-1024x340.jpg 1024w" sizes="(max-width: 1725px) 100vw, 1725px" /><figcaption class="wp-caption-text">Convolutional neural network that will be built</figcaption></figure>
<p>First up, we can see that the input images will be 28 x 28 pixel greyscale representations of digits. The first layer will consist of 32 channels of 5 x 5 convolutional filters + a ReLU activation, followed by 2 x 2 max pooling down-sampling with a stride of 2 (this gives a 14 x 14 output). In the next layer, we have the 14 x 14 output of layer 1 being scanned again with 64 channels of 5 x 5 convolutional filters and a final 2 x 2 max pooling (stride = 2) down-sampling to produce a 7 x 7 output of layer 2.</p>
<p>After the convolutional part of the network, there will be a flatten operation which creates 7 x 7 x 64 = 3164 nodes, an intermediate layer of 1000 fully connected nodes and a softmax operation over the 10 output nodes to produce class probabilities. These layers represent the output classifier.</p>
<h2>Loading the dataset</h2>
<p>PyTorch has an integrated MNIST dataset (in the torchvision package) which we can use via the DataLoader functionality. In this sub-section, I&#8217;ll go through how to setup the data loader for the MNIST data set. But first, some preliminary variables need to be defined:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Hyperparameters
num_epochs = 5
num_classes = 10
batch_size = 100
learning_rate = 0.001

DATA_PATH = &#039;C:\\Users\Andy\PycharmProjects\MNISTData&#039;
MODEL_STORE_PATH = &#039;C:\\Users\Andy\PycharmProjects\pytorch_models\\&#039;</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First off, we set up some training hyperparameters. Next &#8211; there is a specification of some local drive folders to use to store the MNIST dataset (PyTorch will download the dataset into this folder for you automatically) and also a location for the trained model parameters once training is complete.</p>
<p>Next, we setup a transform to apply to the MNIST data, and also the data set variables:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># transforms to apply to the data
trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

# MNIST dataset
train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)
test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first thing to note above is the transforms.Compose() function. This function comes from the torchvision package. It allows the developer to setup various manipulations on the specified dataset. Numerous transforms can be chained together in a list using the Compose() function. In this case, first we specify a transform which converts the input data set to a PyTorch <em>tensor</em>. A PyTorch tensor is a specific data type used in PyTorch for all of the various data and weight operations within the network. In its essence though, it is simply a multi-dimensional matrix. In any case, PyTorch requires the data set to be transformed into a tensor so it can be consumed in the training and testing of the network.</p>
<p>The next argument in the Compose() list is a normalization transformation. Neural networks train better when the input data is normalized so that the data ranges from -1 to 1 or 0 to 1. To do this via the PyTorch Normalize transform, we need to supply the mean and standard deviation of the MNIST dataset, which in this case is 0.1307 and 0.3081 respectively. Note, that for each input channel a mean and standard deviation must be supplied &#8211; in the MNIST case, the input data is only single channeled, but for something like the CIFAR data set, which has 3 channels (one for each color in the RGB spectrum) you would need to provide a mean and standard deviation for each channel.</p>
<p>Next, the <em>train_dataset</em> and <em>test_dataset</em> objects need to be created. These will subsequently be passed to the data loader. In order to create these data sets from the MNIST data, we need to provide a few arguments. First, the <em>root</em> argument specifies the folder where the train.pt and test.pt data files exist. The <em>train</em> argument is a boolean which informs the data set to pickup either the train.pt data file or the test.pt data file. The next argument, <em>transform</em>, is where we supply any transform object that we&#8217;ve created to apply to the data set &#8211; here we supply the <em>trans</em> object which was created earlier. Finally, the download argument tells the MNIST data set function to download the data (if required) from an online source.</p>
<p>Now both the train and test datasets have been created, it is time to load them into the data loader:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The data loader object in PyTorch provides a number of features which are useful in consuming training data &#8211; the ability to shuffle the data easily, the ability to easily batch the data and finally, to make data consumption more efficient via the ability to load the data in parallel using multiprocessing. As can be observed, there are three simple arguments to supply &#8211; first the data set you wish to load, second the batch size you desire and finally whether you wish to randomly shuffle the data. A data loader can be used as an iterator &#8211; so to extract the data we can just use the standard Python iterators such as enumerate. This will be shown in practice later in this tutorial.</p>
<h2>Creating the model</h2>
<p>Next, we need to setup our nn.Module class, which will define the Convolutional Neural Network which we are going to train:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.drop_out = nn.Dropout()
        self.fc1 = nn.Linear(7 * 7 * 64, 1000)
        self.fc2 = nn.Linear(1000, 10)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Ok &#8211; so this is where the model definition takes place. The most straight-forward way of creating a neural network structure in PyTorch is by creating a class which inherits from the nn.Module super class within PyTorch. The nn.Module is a very useful PyTorch class which contains all you need to construct your typical deep learning networks. It also has handy functions such as ways to move variables and operations onto a GPU or back to a CPU, apply recursive functions across all the properties in the class (i.e. resetting all the weight variables), creates streamlined interfaces for training and so on. It is worth checking out all the methods available <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">here</a>.</p>
<p>The first step is to create some sequential layer objects within the class _init_ function. First, we create layer 1 (<em>self.layer1</em>) by creating a nn.Sequential object. This method allows us to create sequentially ordered layers in our network and is a handy way of creating a convolution + ReLU + pooling sequence. As can be observed, the first element in the sequential definition is the Conv2d nn.Module method &#8211; this method creates a set of convolutional filters. The first argument is the number of input channels &#8211; in this case, it is our single channel grayscale MNIST images, so the argument is 1. The second argument to Conv2d is the number of output channels &#8211; as shown in the model architecture diagram above, the first convolutional filter layer comprises of 32 channels, so this is the value of our second argument.</p>
<p>The <em>kernel_size</em> argument is the size of the convolutional filter &#8211; in this case we want 5 x 5 sized convolutional filters &#8211; so the argument is 5. If you wanted filters with different sized shapes in the <em>x </em>and <em>y </em>directions, you&#8217;d supply a tuple (<em>x-size, y-size</em>). Finally, we want to specify the padding argument. This takes a little bit more thought. The output size of any dimension from either a convolutional filtering or pooling operation can be calculated by the following equation:</p>
<p>$$W_{out} = \frac{(W_{in} &#8211; F + 2P)}{S} + 1$$</p>
<p>Where $W_{in}$ is the width of the input, <em>F</em> is the filter size, <em>P </em>is the padding and <em>S</em> is the stride. The same formula applies to the height calculation, but seeing as our image and filtering are symmetrical the same formula applies to both. If we wish to keep our input and output dimensions the same, with a filter size of 5 and a stride of 1, it turns out from the above formula that we need a padding of 2. Therefore, the argument for padding in Conv2d is 2.</p>
<p>The next element in the sequence is a simple ReLU activation. The last element that is added in the sequential definition for <em>self.layer1</em> is the max pooling operation. The first argument is the pooling size, which is 2 x 2 and hence the argument is 2. Second &#8211; we want to down-sample our data by reducing the effective image size by a factor of 2. To do this, using the formula above, we set the stride to 2 and the padding to zero. Therefore, the stride argument is equal to 2. The padding argument defaults to 0 if we don&#8217;t specify it &#8211; so that&#8217;s what is done in the code above. From these calculations, we now know that the output from <em>self.layer1</em> will be 32 channels of 14 x 14 &#8220;images&#8221;.</p>
<p>Next, the second layer, <em>self.layer2,</em> is defined in the same way as the first layer. The only difference is that the input into the Conv2d function is now 32 channels, with an output of 64 channels. Using the same logic, and given the pooling down-sampling, the output from <em>self.layer2 </em>is 64 channels of 7 x 7 images.</p>
<p>Next, we specify a drop-out layer to avoid over-fitting in the model. Finally, two two fully connected layers are created. The first layer will be of size 7 x 7 x 64 nodes and will connect to the second layer of 1000 nodes. To create a fully connected layer in PyTorch, we use the nn.Linear method. The first argument to this method is the number of nodes in the layer, and the second argument is the number of nodes in the following layer.</p>
<p>With this _init_ definition, the layer definitions have now been created. The next step is to define how the data flows through these layers when performing the forward pass through the network:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.drop_out(out)
        out = self.fc1(out)
        out = self.fc2(out)
        return out</code></pre> <div class="code-embed-infos"> </div> </div>
<p>It is important to call this function &#8220;forward&#8221; as this will override the base forward function in nn.Module and allow all the nn.Module functionality to work correctly. As can be observed, it takes an input argument <em>x,</em> which is the data that is to be passed through the model (i.e. a batch of data). We pass this data into the first layer (<em>self.layer1</em>) and return the output as &#8220;out&#8221;. This output is then fed into the following layer and so on. Note, after <em>self.layer2</em>, we apply a reshaping function to <em>out</em>, which flattens the data dimensions from 7 x 7 x 64 into 3164 x 1. Next, the dropout is applied followed by the two fully connected layers, with the final output being returned from the function.</p>
<p>Ok &#8211; so now we have defined what our Convolutional Neural Network is, and how it operates. It&#8217;s time to train the model.</p>
<h2>Training the model</h2>
<p>Before we train the model, we have to first create an instance of our ConvNet class, and define our loss function and optimizer:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = ConvNet()

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, an instance of ConvNet() is created called &#8220;model&#8221;. Next, we define the loss operation that will be used to calculate the loss. In this case, we use PyTorch&#8217;s CrossEntropyLoss() function. You may have noticed that we haven&#8217;t yet defined a SoftMax activation for the final classification layer. This is because the CrossEntropyLoss function combines both a SoftMax activation and a cross entropy loss function in the same function &#8211; winning. Next, we define an Adam optimizer. The first argument passed to this function are the parameters we want the optimizer to train. This is made easy via the nn.Module class which ConvNet derives from &#8211; all we have to do is pass model.parameters() to the function and PyTorch keeps track of all the parameters within our model which are required to be trained. Finally, the learning rate is supplied.</p>
<p>Next &#8211; the training loop is created:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Train the model
total_step = len(train_loader)
loss_list = []
acc_list = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Run the forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss_list.append(loss.item())

        # Backprop and perform Adam optimisation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Track the accuracy
        total = labels.size(0)
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        acc_list.append(correct / total)

        if (i + 1) % 100 == 0:
            print(&#039;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%&#039;
                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),
                          (correct / total) * 100))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The most important parts to start with are the two loops &#8211; first, the number of epochs is looped over, and within this loop, we iterate over train_loader using enumerate. Within this inner loop, first the outputs of the forward pass through the model are calculated by passing <em>images </em>(which is a batch of normalized MNIST images from <em>train_loader</em>) to it. Note, we don&#8217;t have to call <em>model.forward(images)</em> as nn.Module knows that <em>forward</em> needs to be called when it executes <em>model(images)</em>.</p>
<p>The next step is to pass the model outputs and the true image labels to our CrossEntropyLoss function, defined as <em>criterion</em>. The loss is appended to a list that will be used later to plot the progress of the training. The next step is to perform back-propagation and an optimized training step. First, the gradients have to be zeroed, which can be done easily by calling zero_grad() on the optimizer. Next, we call .backward() on the <em>loss</em> variable to perform the back-propagation. Finally, now that the gradients have been calculated in the back-propagation, we simply call optimizer.step() to perform the Adam optimizer training step. PyTorch makes training the model very easy and intuitive.</p>
<p>The next set of steps involves keeping track of the accuracy on the training set. The predictions of the model can be determined by using the torch.max() function, which returns the index of the maximum value in a tensor. The first argument to this function is the tensor to be examined, and the second argument is the axis over which to determine the index of the maximum. The output tensor from the model will be of size (batch_size, 10). To determine the model prediction, for each sample in the batch we need to find the maximum value over the 10 output nodes. Each of these will correspond to one of the hand written digits (i.e. output 2 will correspond to digit &#8220;2&#8221; and so on). The output node with the highest value will be the prediction of the model. Therefore, we need to set the second argument of the torch.max() function to 1 &#8211; this points the max function to examine the output node axis (axis=0 corresponds to the batch_size dimension).</p>
<p>This returns a list of prediction integers from the model &#8211; the next line compares the predictions with the true labels (predicted == labels) and sums them to determine how many correct predictions there are. Note the output of sum() is still a tensor, so to access it&#8217;s value you need to call .item(). We divide the number of correct predictions by the batch_size (equivalent to labels.size(0)) to obtain the accuracy. Finally, during training, after every 100 iterations of the inner loop the progress is printed.</p>
<p>The training output will look something like this:</p>
<p>Epoch [1/6], Step [100/600], Loss: 0.2183, Accuracy: 95.00%<br />
Epoch [1/6], Step [200/600], Loss: 0.1637, Accuracy: 95.00%<br />
Epoch [1/6], Step [300/600], Loss: 0.0848, Accuracy: 98.00%<br />
Epoch [1/6], Step [400/600], Loss: 0.1241, Accuracy: 97.00%<br />
Epoch [1/6], Step [500/600], Loss: 0.2433, Accuracy: 95.00%<br />
Epoch [1/6], Step [600/600], Loss: 0.0473, Accuracy: 98.00%<br />
Epoch [2/6], Step [100/600], Loss: 0.1195, Accuracy: 97.00%</p>
<p>Next, let&#8217;s create some code to determine the model accuracy on the test set.</p>
<h2>Testing the model</h2>
<p>To test the model, we use the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># Test the model
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print(&#039;Test Accuracy of the model on the 10000 test images: {} %&#039;.format((correct / total) * 100))

# Save the model and plot
torch.save(model.state_dict(), MODEL_STORE_PATH + &#039;conv_net_model.ckpt&#039;)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As a first step, we set the model to evaluation mode by running model.eval(). This is a handy function which disables any drop-out or batch normalization layers in your model, which will befuddle your model evaluation / testing. The torch.no_grad() statement disables the autograd functionality in the model (see <a href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/">here</a> for more details) as it is not needing in model testing / evaluation, and this will act to speed up the computations. The rest is the same as the accuracy calculations during training, except that in this case, the code iterates through the <em>test_loader</em>.</p>
<p>Finally, the result is output to the console, and the model is saved using the torch.save() function.</p>
<p>In the the last part of the code on <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">the Github repo</a>, I perform some plotting of the loss and accuracy tracking using the Bokeh plotting library. The final results look like this:</p>
<p>Test Accuracy of the model on the 10000 test images: 99.03 %</p>
<figure id="attachment_951" style="width: 546px" class="wp-caption aligncenter"><img class=" wp-image-951" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/PyTorch-convnet-results.png" alt="PyTorch Convolutional Neural Network results" width="546" height="386" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/PyTorch-convnet-results.png 1147w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/PyTorch-convnet-results-300x212.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/PyTorch-convnet-results-768x542.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/06/PyTorch-convnet-results-1024x723.png 1024w" sizes="(max-width: 546px) 100vw, 546px" /><figcaption class="wp-caption-text">PyTorch Convolutional Neural Network results</figcaption></figure>
<p>As can be observed, the network quite rapidly achieves a high degree of accuracy on the training set, and the test set accuracy, after 6 epochs, arrives at 99% &#8211; not bad! Certainly better than the accuracy achieved in basic fully connected neural networks.</p>
<p>In summary: in this tutorial you have learnt all about the benefits and structure of Convolutional Neural Networks and how they work. You have also learnt how to implement them in the awesome PyTorch deep learning framework &#8211; a framework which, in my view, has a big future. I hope it was useful &#8211; have fun in your deep learning journey!</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video learner, check out this inexpensive online course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/">Convolutional Neural Networks Tutorial in PyTorch</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Weight initialization tutorial in TensorFlow</title>
		<link>http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/</link>
		<comments>http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/#respond</comments>
		<pubDate>Thu, 17 May 2018 10:28:56 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[TensorFlow]]></category>
		<category><![CDATA[Weight initialization]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=879</guid>
		<description><![CDATA[<p>In the late 80&#8217;s and 90&#8217;s, neural network research stalled due to a lack of good performance. There were a number of reasons for this, <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/" title="Weight initialization tutorial in TensorFlow">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/">Weight initialization tutorial in TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In the late 80&#8217;s and 90&#8217;s, neural network research stalled due to a lack of good performance. There were a number of reasons for this, outlined by the prominent AI researcher <a href="https://www.youtube.com/watch?v=MpLds0oohC8" target="_blank" rel="noopener">Geoffrey Hinton</a> &#8211; these reasons included poor computing speeds, lack of data, using the wrong type of non-linear activation functions and poor initialization of the weights in neural networks. <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">My post on the vanishing gradient problem and ReLUs</a> addresses the problem of the wrong kind of non-linear activation functions, and this post will deal with proper weight initialization. In particular, in this post we&#8217;ll be examining the problem with a naive normal distribution when initializing weights, and examine Xavier and He initialization as a remedy to this problem. This will be empirically studied using TensorFlow and some associated TensorBoard visualizations. Note: to run the code in this tutorial, you&#8217;ll need TensorFlow 1.8 or greater installed.</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h2>The problem with a naive initialization of weights</h2>
<p>The random initialization of weights is critical to learning good mappings from input to output in neural networks. Because the search space involving many weights during training is very large, there are multiple local minimums within which the back-propagation may be trapped. Effective randomization of weights ensures that the search space is adequately explored during training, resulting in the best chances of a good minimum being found during back-propagation (for more on back-propagation, see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">my neural networks tutorial</a>). However, the weight initialization randomization function needs to be carefully chosen and specified otherwise there is a large risk that the training progress will be slowed to the point of impracticality.</p>
<p>This is especially the case when using the historical &#8220;squashing&#8221; non-linear activation functions such as the sigmoid function and the tanh function, though it is still an issue with ReLU function, as will be seen later. The reason for this problem is that, if the weights are such that the activation functions of nodes are pushed into the &#8220;flat&#8221; regions of their curves, they are &#8220;saturated&#8221; and impede learning. Consider the plot below showing the tanh function and its first derivative:</p>
<figure id="attachment_881" style="width: 404px" class="wp-caption aligncenter"><img class="size-full wp-image-881" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Tanh-gradient.png" alt="Tanh function - weight initialization TensorFlow" width="404" height="266" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Tanh-gradient.png 404w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Tanh-gradient-300x198.png 300w" sizes="(max-width: 404px) 100vw, 404px" /><figcaption class="wp-caption-text">Tanh function and its first derivative</figcaption></figure>
<p>It can observed that when abs(x) &gt; 2, the derivative of the tanh function approaches zero. Now because the back-propagation method of updating the weight values in a neural network depends on the derivative of the activation functions, this means that when nodes are pushed into such &#8220;saturation&#8221; regions, slow or no learning will take place. Therefore, we don&#8217;t want to start with weight values that push some or all of the nodes into those saturation regions, as that network just won&#8217;t work very well. The sigmoid function operates similarly, as can be observed in <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">my vanishing gradient</a> post.</p>
<p>A naive initialization of weights might be to simply use a normal distribution of mean zero and unit standard deviation (i.e. 1.0). Let&#8217;s consider how this might play out using a bit of simple statistical theory. Recall that the input to a neuron in the first layer of a neural network looks like:</p>
<p>$$in = X_1 W_1 + X_2 W_2 + X_3 W_3+ X_4 W_4 + \dots$$</p>
<p>The input, in other words, is a summation of the respective weights and their inputs. The variance (the square of the standard deviation) of each element in this sum can be explained by the <a href="https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables" target="_blank" rel="noopener">product of independent variables</a> law:</p>
<p>$$Var(X_i W_i) = [E(X_i)]^2 Var(W_i) + [E(W_i)]^2 Var(X_i) + Var(X_i)Var(W_i)$$</p>
<p>If we assume that the input has been appropriately scaled with a mean of 0 and a unit variance, and likewise we initialize the weights for a mean 0 and unit variance, then this results in:</p>
<p>$$Var(X_i W_i) = 0 \times 1 + 0 \times 1 + 1 \times 1 = 1$$</p>
<p>So each product within the total sum of <em>in</em> has a variance of 1. What is the total variance of the node input variable <em>in</em>? We can make the assumption that each product (i.e. each $X_i W_i$) is statistically independent (not quite correct for things like images, but close enough for our purposes) and then apply the <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)" target="_blank" rel="noopener">sum of uncorrelated independent variables</a> law:</p>
<p>$$Var(in) = \sum_{i=0}^n  Var(X_i W_i) = n \times 1 = n$$</p>
<p>Where <em>n</em> is the number of inputs. So here, we can observe that if there are, say, 784 inputs (equal to the input size of the MNIST problem), the variance will be large and the standard deviation will be $\sqrt{Var(in)} = \sqrt{784} = 28$. This will result in the vast majority of neurons in the first layer being saturated, as most values will be &gt;&gt; |2| (i.e. the saturation regions of the functions).</p>
<p>Clearly this is not ideal, and so another way of initializing our weight variables is desirable.</p>
<h2>Xavier or variance scaling for weight initialization</h2>
<p>The <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Xavier method of weight initialization</a> is a big improvement on the naive way of weight scaling shown in the section above. This method has helped accelerate the field of deep learning in a big way. It takes into account the problems shown above and bases the standard deviation or the variance of the weight initialization on the number of variables involved. It thereby adjusts itself based on the number of weight values. It works on the idea that if you can keep the variance constant from layer to layer in both the feed forward direction and back-propagation direction, your network will learn optimally. This makes sense, as if the variance increases or decreases as you go through the layers, your weights will eventually saturate your non-linear neurons in either the positive or negative direction.</p>
<p>So, how do we use this idea to work out what variance should be used to best initialize the weights? First, because the network will be learning effectively when it is operating in the linear regions of the <em>tanh</em> and <em>sigmoid</em> functions, the activation function can be approximated by a linear activation, i.e.:</p>
<p>$$ Y = W_{1} X_{1} + W_{2} X_{2} + W_{3} X_{3} + \dots $$</p>
<p>Therefore, with this linear activation function, we can use the same result that was arrived at above using the product of independent variables and sum of uncorrelated independent variables, namely:</p>
<p>$$ Var(Y) = n_{in} Var(W_i)Var(X_i)$$</p>
<p>Where $n_{in}$ is the number of inputs to each node. If we want the variance of the input ($Var(X_i)$) to be equal to the variance of the output ($Var(Y)$) this reduces to:</p>
<p>$$ Var(W_i) = \frac{1}{n_{in}} $$</p>
<p>Which is a preliminary result for a good initialization variance for the weights in your network. However, this is really just keeping the variance constant during the forward pass. What about trying to keep the variance constant also during back-propagation? It turns out that during back-propagation, to try to do this you need:</p>
<p>$$ n_{i+1} Var(W_i) = 1 $$</p>
<p>Or:</p>
<p>$$ Var(W_i) = \frac{1}{n_{out}} $$</p>
<p>Now there are two different ways of calculating the variance, one depending on the value of the number of inputs and the other on the number of outputs. The authors of the <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">original paper on Xavier initialization</a> take the average of the two:</p>
<p>$$ n_{avg} = \frac{n_{in}  + n_{out}}{2} $$</p>
<p>$$ Var(W_i) = \frac{1}{n_{avg}} = \frac {2}{n_{in} + n_{out}} $$</p>
<p>That is the final result in the Xavier initialization of weights for <em>squashing </em>activation functions i.e. <em>tanh </em>and <em>sigmoid</em>. However, it turns out this isn&#8217;t quite as optimal for <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">ReLU functions</a>.</p>
<h2>ReLU activations and the He initialization</h2>
<p>Consider the ReLU function &#8211; for all values less than zero, the output of the activation function is also zero. For values greater than zero, the ReLU function simply returns it&#8217;s input. In other words, half of the output is linear, like the assumption made in the analysis above &#8211; so that&#8217;s easy. However, for the other half of the inputs, for input values &lt; 0, the output is zero. If we assume that the inputs to the ReLU neurons are approximately centered about 0, then, roughly speaking, half the variance will be in line with the Xavier initialization result, and the other half will be 0.</p>
<p>This is basically equivalent to halving the number of input nodes. So if we return to our Xavier calculations, but with half the number of input nodes, we have:</p>
<p>$$ Var(Y) = \frac{n_{in}}{2} Var(W_i)Var(X_i) $$</p>
<p>Again, if we want the variance of the input ($Var(X_i)$) to be equal to the variance of the output ($Var(Y)$) this reduces to:</p>
<p>$$ Var(W_i) = \frac{2}{n_{in}} $$</p>
<p>This is He initialization, and this initialization has been found to generally work better with ReLU activation functions.</p>
<p>Now that we&#8217;ve reviewed the theory, let&#8217;s get to the code.</p>
<h2>Weight initialization in TensorFlow</h2>
<p>This section will show you how to initialize weights easily in TensorFlow. The full code can be found on <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github page</a>. Performing Xavier and He initialization in TensorFlow is now really straight-forward using the <em>tf.contrib.layers.variance_scaling_initializer</em>. By adjusting the available parameters, we can create either Xavier, He or other types of modern weight initializations. In this TensorFlow example, I&#8217;ll be creating a simple MNIST classifier using TensorFlow&#8217;s packaged MNIST dataset, with a simple three layer fully connected neural network architecture. I&#8217;ll also be logging various quantities so that we can visualize the variance, activations and so on in TensorBoard.</p>
<p>First, we define a Model class to hold the neural network model:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class Model(object):
    def __init__(self, input_size, label_size, initialization, activation, num_layers=3,
                 hidden_size=100):
        self._input_size = input_size
        self._label_size = label_size
        self._init = initialization
        self._activation = activation
        # num layers does not include the input layer
        self._num_layers = num_layers
        self._hidden_size = hidden_size
        self._model_def()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The above code is the class initialization function &#8211; notice that various initialization and activation functions can be passed to the model. Later on, we&#8217;ll cycle through different weight initialization and activation functions and see how they perform.</p>
<p>In the next section, I define the model creation function inside the Model class:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _model_def(self):
        # create placeholder variables
        self.input_images = tf.placeholder(tf.float32, shape=[None, self._input_size])
        self.labels = tf.placeholder(tf.float32, shape=[None, self._label_size])
        # create self._num_layers dense layers as the model
        input = self.input_images
        tf.summary.scalar(&quot;input_var&quot;, self._calculate_variance(input))
        for i in range(self._num_layers - 1):
            input = tf.layers.dense(input, self._hidden_size, kernel_initializer=self._init,
                                    activation=self._activation, name=&#039;layer{}&#039;.format(i+1))
            # get the input to the nodes (sans bias)
            mat_mul_in = tf.get_default_graph().get_tensor_by_name(&quot;layer{}/MatMul:0&quot;.format(i + 1))
            # log pre and post activation function histograms
            tf.summary.histogram(&quot;mat_mul_hist_{}&quot;.format(i + 1), mat_mul_in)
            tf.summary.histogram(&quot;fc_out_{}&quot;.format(i + 1), input)
            # also log the variance of mat mul
            tf.summary.scalar(&quot;mat_mul_var_{}&quot;.format(i + 1), self._calculate_variance(mat_mul_in))
        # don&#039;t supply an activation for the final layer - the loss definition will
        # supply softmax activation. This defaults to a linear activation i.e. f(x) = x
        logits = tf.layers.dense(input, 10, name=&#039;layer{}&#039;.format(self._num_layers))
        mat_mul_in = tf.get_default_graph().get_tensor_by_name(&quot;layer{}/MatMul:0&quot;.format(self._num_layers))
        tf.summary.histogram(&quot;mat_mul_hist_{}&quot;.format(self._num_layers), mat_mul_in)
        tf.summary.histogram(&quot;fc_out_{}&quot;.format(self._num_layers), input)
        # use softmax cross entropy with logits - no need to apply softmax activation to
        # logits
        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                             labels=self.labels))
        # add the loss to the summary
        tf.summary.scalar(&#039;loss&#039;, self.loss)
        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)
        self.accuracy = self._compute_accuracy(logits, self.labels)
        tf.summary.scalar(&#039;acc&#039;, self.accuracy)
        self.merged = tf.summary.merge_all()
        self.init_op = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>I&#8217;ll step through the major points in this function. First, there is the usual placeholders to hold the training input and output data &#8211; if you&#8217;re unfamiliar with the basics of TensorFlow, check out my introductory tutorial <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">here</a>. Then, a scalar variable is logged called &#8220;input_var&#8221; which logs the variance of the input images, calculated via the _calculate_variance function &#8211; this will be presented later. The next step involves a loop through the layers, and here I have used the TensorFlow layers API which allows us to create densely connected layers easily. Notice that the <em>kernel_initializer</em> argument is what will initialize the weights of the layer, and <em>activation</em> is the activation function which the layer neurons will use.</p>
<p>Next, I access the values of the matrix multiplication between the weights and inputs for each layer, and log the values. This way we can observe what the values of the inputs to each neuron is, and the variance of these inputs. We log these values as histograms. Finally, within the layer loop, the variance of the matrix multiplication input is also logged as a scalar.</p>
<p>The remainder of this model construction function is all the standard TensorFlow operations which define the loss, the optimizer and variable initialization, and also some additional logging of variables. The next function to take notice of within the Model class is the _calculate_variance function &#8211; it looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    def _calculate_variance(self, x):
        mean = tf.reduce_mean(x)
        sqr = tf.square(x - mean)
        return tf.reduce_mean(sqr)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The function above is just a simple calculation of the <a href="https://en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener">variance</a> of <em>x</em>.</p>
<p>The main code block creates a list of various scenarios to run through, each with a different folder name in which to store the results, a different weight initialization function and finally a different activation function to supply to the neurons. The main training / analysis loop first runs a single batch of data through the network to examine initial variances. Thereafter it performs a full training run of the network so that performance indicators can be analysed.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if __name__ == &quot;__main__&quot;:
    sub_folders = [&#039;first_pass_normal&#039;, &#039;first_pass_variance&#039;,
                   &#039;full_train_normal&#039;, &#039;full_train_variance&#039;,
                   &#039;full_train_normal_relu&#039;, &#039;full_train_variance_relu&#039;,
                   &#039;full_train_he_relu&#039;]
    initializers = [tf.random_normal_initializer,
                    tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=&#039;FAN_AVG&#039;, uniform=False),
                    tf.random_normal_initializer,
                    tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=&#039;FAN_AVG&#039;, uniform=False),
                    tf.random_normal_initializer,
                    tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=&#039;FAN_AVG&#039;, uniform=False),
                    tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode=&#039;FAN_IN&#039;, uniform=False)]
    activations = [tf.sigmoid, tf.sigmoid, tf.sigmoid, tf.sigmoid, tf.nn.relu, tf.nn.relu, tf.nn.relu]
    assert len(sub_folders) == len(initializers) == len(activations)
    maybe_create_folder_structure(sub_folders)
    for i in range(len(sub_folders)):
        tf.reset_default_graph()
        model = Model(784, 10, initializers[i], activations[i])
        if &quot;first_pass&quot; in sub_folders[i]:
            init_pass_through(model, sub_folders[i])
        else:
            train_model(model, sub_folders[i], 30, 1000)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The most important thing to consider in the code above is the Xavier and He weight initialization<em> </em>definitions. The function used to create these is the <em>tf.contrib.layers.variance_scaling_initializer</em> which allows us to create weight initializers which are based on the number of input and output connections in order to execute the Xavier and He initialization discussed previously.</p>
<p>The three arguments used in this function are:</p>
<ul>
<li>The <em>factor </em>argument, which is a multiplicative factor that is applied to the scaling. This is 1.0 for Xavier weight initialization, and 2.0 for He weight initialization</li>
<li>The <em>mode</em> argument: this defines which is on the denominator of the variance calculation. If &#8216;FAN_IN&#8217;, the variance scaling is based solely on the number of inputs to the node. If &#8216;FAN_OUT&#8217; it is based solely on the number of outputs. If it is &#8216;FAN_AVG&#8217;, it is based on an averaging calculation, i.e. Xavier initialization. For He initialization, use &#8216;FAN_IN&#8217;</li>
<li>The <em>uniform</em> argument: this defines whether to use a uniform distribution or a normal distribution to sample the weights from during initialization. For both Xavier and He weight initialization, you can use a normal distribution, so set this argument to False</li>
</ul>
<p>The other weight initialization function used in the scenarios is the <em>tf.random_normal_initializer</em> with default parameters. The default parameters for this initializer are a mean of zero, and a unit (i.e. 1.0) standard deviation / variance.</p>
<p>After running this code, a number of interesting results are obtained.</p>
<h2>Visualizing the TensorFlow model variables</h2>
<p>The first thing that we want to look at is the &#8220;first pass&#8221; model results, where only one batch is passed through the model. If we look at the distribution of inputs into the first layer in TensorBoard, with our naive normally distributed weight values with a unit variance, we can see the following:</p>
<figure id="attachment_911" style="width: 322px" class="wp-caption aligncenter"><img class=" wp-image-911" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-Mat-Mul-1.png" alt="Weight initialization - First pass distribution of inputs to the first layer" width="322" height="201" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-Mat-Mul-1.png 505w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-Mat-Mul-1-300x187.png 300w" sizes="(max-width: 322px) 100vw, 322px" /><figcaption class="wp-caption-text">First pass distribution of inputs to the first layer</figcaption></figure>
<p>As can be observed the matrix multiplication input into the first layer is approximately normally distributed, with a standard deviation around 10. If you recall, the variance scalar of the matrix multiplication input was also been logged, and it gives a value of approximately 88. Does this make sense? I mentioned earlier that with 784 inputs (i.e. the input size of the MNIST dataset), we should expect a variance of approximately 784. What&#8217;s the explanation of this discrepancy? Well, remember I also logged the variance of the input data &#8211; it turns out that the MNIST TensorFlow dataset has a variance of 0.094. You&#8217;ll recall that we assumed a unit variance in the calculations previously shown. In this case, though, we should expect a variance of (remember that $Var(W_i)$, for the normal distribution initializer we are currently considering, is equal to 1.0):</p>
<p>$$Var(in) = \sum_{i=0}^n Var(X_i)Var(W_i) = n Var(X_i)Var(W_i) = 784 * 0.094 * 1 = 74$$</p>
<p>This is roughly in line with the observed variance &#8211; so we can be happy that we are on the right track. The distribution shown above is the distribution into the first layer neurons. In the first set of scenarios, we&#8217;re using a sigmoid activation function &#8211; so what does the first layer output distribution look like for this type of input distribution?</p>
<figure id="attachment_912" style="width: 319px" class="wp-caption aligncenter"><img class=" wp-image-912" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-distribution.png" alt="Weight initialization - Distribution of outputs from first layer - sigmoid activations and normal weight initialization" width="319" height="210" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-distribution.png 498w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-distribution-300x198.png 300w" sizes="(max-width: 319px) 100vw, 319px" /><figcaption class="wp-caption-text">Distribution of outputs from first layer &#8211; sigmoid activations and normal weight initialization</figcaption></figure>
<p>As can be observed, the input distribution with such a relatively large variance completely saturates the first layer &#8211; with the output distribution being squeezed to the saturated regions of the sigmoid<em> </em>curve i.e. outputs close to 0 and 1 (we&#8217;d observe the same thing with a <em>tanh</em> activation). This confirms our previous analysis of the problems with a naive normally distributed weight initialization.</p>
<p>What happens when we use the Xavier initialization configuration of the variance scaler initializer? The plot below shows the same distribution of outputs:</p>
<figure id="attachment_913" style="width: 302px" class="wp-caption aligncenter"><img class=" wp-image-913" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-Xavier-distribution.png" alt="Weight initialization - Distribution of outputs from first layer - sigmoid activations and Xavier weight initialization" width="302" height="213" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-Xavier-distribution.png 463w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-first-layer-output-Xavier-distribution-300x212.png 300w" sizes="(max-width: 302px) 100vw, 302px" /><figcaption class="wp-caption-text">Distribution of outputs from first layer &#8211; sigmoid activations and Xavier weight initialization</figcaption></figure>
<p>As can be observed, this is a very satisfactory distribution &#8211; with the output values centered around the linear region of the sigmoid function (i.e. 0.5), with no saturation occurring. This more optimal initialization results in better training outcomes also. The figure below shows the accuracy comparison between the normally initialized weight distribution and the Xavier initialized weight distribution, for the full training run scenario:</p>
<figure id="attachment_914" style="width: 379px" class="wp-caption aligncenter"><img class=" wp-image-914" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-accuracy-comparison.png" alt="Weight initialization - Accuracy comparison between normal and Xavier initialization - sigmoid activation" width="379" height="248" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-accuracy-comparison.png 527w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-accuracy-comparison-300x196.png 300w" sizes="(max-width: 379px) 100vw, 379px" /><figcaption class="wp-caption-text">Accuracy comparison between normal (red) and Xavier initialization (light blue) &#8211; sigmoid activation</figcaption></figure>
<p>As can be observed, Xavier initialization results in better training performance, as we should expect.</p>
<p>The next thing to compare is the performance of normal weight initialization, Xavier initialization and He initialization for a ReLU activation function. The plot below shows the accuracy comparison during training between the three initialization techniques:</p>
<figure id="attachment_917" style="width: 380px" class="wp-caption aligncenter"><img class=" wp-image-917" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-ReLU-accuracy-comparison.png" alt="Weight initialization - He, Xavier and normal comparison with ReLU activations" width="380" height="253" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-ReLU-accuracy-comparison.png 525w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/05/Weight-initialization-ReLU-accuracy-comparison-300x200.png 300w" sizes="(max-width: 380px) 100vw, 380px" /><figcaption class="wp-caption-text">Accuracy comparison for ReLU activation functions and normal (red), Xavier (green) and He (grey) weight initialization</figcaption></figure>
<p>As can be observed, the model performance is significantly greater for Xavier and He weight initialization than for the normal initialization on a ReLU network. There is little clear difference between the Xavier and He initialization, but a better average performance should be expected from He initialization for more complicated networks and problems that use a ReLU activation function.</p>
<p>There you have it &#8211; you should now hopefully understand the drawbacks of naive, normally distributed weight initialization, and you should also understand the basics of how Xavier and He initialization work, and their performance benefits. You should also understand how to easily use such initialization methods in TensorFlow. I hope this helps you build better performing models for both sigmoid/tanh and ReLU networks.</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1326292&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1326292&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/">Weight initialization tutorial in TensorFlow</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>The vanishing gradient problem and ReLUs &#8211; a TensorFlow investigation</title>
		<link>http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/</link>
		<comments>http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments</comments>
		<pubDate>Tue, 03 Apr 2018 01:50:19 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=839</guid>
		<description><![CDATA[<p>Deep learning is huge in machine learning at the moment, and no wonder &#8211; it is making large and important strides in solving problems in computer <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" title="The vanishing gradient problem and ReLUs &#8211; a TensorFlow investigation">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/">The vanishing gradient problem and ReLUs &#8211; a TensorFlow investigation</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Deep learning is huge in machine learning at the moment, and no wonder &#8211; it is making large and important strides in solving problems in <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/">computer vision</a>, <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/">natural language</a> and <a href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/">reinforcement learning</a> and problems in many other areas. Deep learning neural networks are neural networks which are characterized by <em>many </em><em>layers</em> &#8211; making them <em>deep </em>instead of <em>wide</em>. Deep networks <a href="https://arxiv.org/pdf/1512.03965v4.pdf" target="_blank" rel="noopener">have been demonstrated</a> to be more practically capable of solving problems than simple, wide two layer networks. Neural networks have been around for a long time, but initial success using these networks was elusive. One of the issues that had to be overcome in making them more useful and transitioning to modern deep learning networks was the <em>vanishing gradient</em> problem. This problem manifests in the early layers of deep neural networks not learning (or learning very slowly), resulting in difficulties in solving practical problems.</p>
<p>This post will examine the vanishing gradient problem, and demonstrate an improvement to the problem through the use of the rectified linear unit activation function, or ReLUs. The examination will take place using TensorFlow and visualizing with the TensorBoard utility. The TensorFlow code used in this tutorial can be found on <a href="https://github.com/adventuresinML/adventures-in-ml-code">this site&#8217;s Github repository</a>.</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&#038;offerid=323058.1326292&#038;type=2&#038;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><IMG border=0 width=1 height=1 src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.1326292&#038;type=2&#038;subid=0" ></p>
<hr />
<h2>The vanishing gradient problem</h2>
<p>The vanishing gradient problem arises due to the nature of the back-propagation optimization which occurs in neural network training (for a comprehensive introduction to back-propagation, see <a href="http://adventuresinmachinelearning.com/ebook-newsletter-sign/">my free ebook</a>). The weight and bias values in the various layers within a neural network are updated each optimization iteration by stepping in the direction of the <em>gradient </em>of the weight/bias values with respect to the loss function. In other words, the weight values change in proportion to the following gradient:</p>
<p>$$ \partial C/ \partial W_l $$</p>
<p>Where <i>$W_l$</i> represents the weights of layer <em>l</em> and <em>C  </em>is the cost or loss function at the output layer (again, if these terms are gibberish to you, check out <a href="http://adventuresinmachinelearning.com/ebook-newsletter-sign/">my free ebook</a> which will get you up to speed). In the final layer, this calculation is straight-forward, however in earlier layers, the back-propagation of errors method needs to be utilized. At the final layer, the error term $\delta$ looks like:</p>
<p>$$\delta_i^{(n_l)} = -(y_i &#8211; h_i^{(n_l)})\cdot f^\prime(z_i^{(n_l)})$$</p>
<p>Don&#8217;t worry too much about the notation, but basically the equation above shows first that the error is related to the difference between the output of the network $h_i^{(n_l)}$ and the training labels $y_i$ (i.e. $(y_i &#8211; h_i^{(n_l)})$). It is also, more importantly for the vanishing gradient problem, proportional to the derivative of the activation function $f^\prime(z_i^{(n_l)})$. The weights in the final layer change in direct proportion to this $\delta$ value. For earlier layers, the error from the latter layers is back-propagated via the following rule:</p>
<p>$$\delta^{(l)} = \left((W^{(l)})^T \delta^{(l+1)}\right) \bullet f'(z^{(l)})$$</p>
<p>Again, in the second part of this equation, there is the derivative of the activation function f'(z^{(l)}). Notice that $\delta^{(l)}$ is also proportional to the error propagated from the downstream layer $\delta^{(l+1)}$. These downstream $\delta$ values also include their own f'(z^{(l)}) values. So, basically, the gradient of the weights of a given layer with respect to the loss function, which controls how these weight values are updated, is proportional to chained multiplications of the derivative of the activation function i.e.:</p>
<p>$$ \frac{\partial C} {\partial W_l} \propto  f'(z^{(l)}) f'(z^{(l+1)}) f'(z^{(l+2)}) \dots$$</p>
<p>The vanishing gradient problem comes about in deep neural networks when the <em>f&#8217;</em> terms are all outputting values &lt;&lt; 1. When we multiply lots of numbers &lt;&lt; 1 together, we end up with a vanishing product, which leads to a very small $\frac{\partial C} {\partial W_l}$ value and hence practically no learning of the weight values &#8211; the predictive power of the neural network then platueus.</p>
<h3>The sigmoid activation function</h3>
<p>The vanishing gradient problem is particularly problematic with sigmoid activation functions. The plot below shows the sigmoid activation function and its first derivative:</p>
<figure id="attachment_559" style="width: 389px" class="wp-caption aligncenter"><img class="size-full wp-image-559" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png" alt="Recurrent neural network and LSTM tutorial - sigmoid gradient" width="389" height="266" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient.png 389w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Sigmoid-gradient-300x205.png 300w" sizes="(max-width: 389px) 100vw, 389px" /><figcaption class="wp-caption-text">Sigmoid gradient</figcaption></figure>
<p>As can be observed, when the sigmoid function value is either too high or too low, the derivative (orange line) becomes very small i.e. &lt;&lt; 1. This causes vanishing gradients and poor learning for deep networks. This can occur when the weights of our networks are initialized poorly &#8211; with too-large negative and positive values. These too-large values <em>saturate</em> the input to the sigmoid and pushes the derivatives into the small valued regions. However, even if the weights are initialized nicely, and the derivatives are sitting around the maximum i.e. ~0.2, with many layers there will still be a vanishing gradient problem. With only 4 layers of 0.2 valued derivatives we have a product of $0.2^{4} = 0.0016$ &#8211; not very large! Consider how the ResNet architecture, generally with 10&#8217;s or 100&#8217;s of layers, would train using sigmoid activation functions with even the best initialized weights. Most of the layers would be static or dead and impervious to training.</p>
<p>So what&#8217;s the solution to this problem? It&#8217;s called a rectified linear unit activation function, or ReLU.</p>
<h3>The ReLU activation function</h3>
<p>The ReLU activation function is defined as:</p>
<p>$$f(x) = \max(0, x)$$</p>
<p>This function and it&#8217;s first derivative look like:</p>
<figure id="attachment_849" style="width: 380px" class="wp-caption aligncenter"><img class="size-full wp-image-849" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/ReLU-activation-and-derivative.png" alt="ReLU activation - vanishing gradient problem and TensorFlow" width="380" height="266" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/ReLU-activation-and-derivative.png 380w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/ReLU-activation-and-derivative-300x210.png 300w" sizes="(max-width: 380px) 100vw, 380px" /><figcaption class="wp-caption-text">ReLU activation and first derivative</figcaption></figure>
<p>As can be observed, the ReLU activation simply returns its argument <em>x</em> whenever it is greater than zero, and returns 0 otherwise. The first derivative of ReLU is also very simple &#8211; it is equal to 1 when <em>x </em>is greater than zero, but otherwise it is 0. You can probably see the advantages of ReLU at this point &#8211; when it&#8217;s derivative is back-propagated there will be no degradation of the error signal as 1 x 1 x 1 x 1&#8230; = 1. However, the ReLU activation still maintains a non-linearity or &#8220;switch on&#8221; characteristic which enables it to behave analogously to a biological neuron.</p>
<p>There is only one problem with the ReLU activation &#8211; sometimes, because the derivative is zero when <em>x </em>&lt; 0, certain weights can be &#8220;killed off&#8221; or become &#8220;dead&#8221;. This is because the back-propagated error can be cancelled out whenever there is a negative input into a given neuron and therefore the gradient $\frac{\partial C} {\partial W_l}$ will also fall to zero. This means there is no way for the associated weights to update in the right direction. This can obviously impact learning.</p>
<p>What&#8217;s the solution? A variant of ReLU which is called a Leaky ReLU activation.</p>
<h3>The Leaky ReLU activation</h3>
<p>The Leaky ReLU activation is defined as:</p>
<p>$$f(x) = \max(0.01x, x)$$</p>
<p>As you can observe, when <em>x </em>is below zero, the output will switch from <em>x </em>to 0.01<em>x</em>. I won&#8217;t plot the activation for this function, as it is too difficult to see the difference between 0.01<em>x</em> and 0 and therefore in plots it looks just like a normal ReLU. However, the good thing about the Leaky ReLU activation function is that the derivative when <em>x</em> is below zero is 0.01 &#8211; i.e. it is a small but no longer 0. This gives the neuron and associated weights the <em>chance </em>to reactivate, and therefore this should improve the overall learning performance.</p>
<p>Now it&#8217;s time to test out these ideas in a real example using TensorFlow.</p>
<h2>Demonstrating the vanishing gradient problem in TensorFlow</h2>
<h3>Creating the model</h3>
<p>In the TensorFlow code I am about to show you, we&#8217;ll be creating a 7 layer densely connected network (including the input and output layers) and using the TensorFlow summary operations and TensorBoard visualization to see what is going on with the gradients. The code uses the TensorFlow layers (tf.layers) framework which allows quick and easy building of networks. The data we will be training the network on is the MNIST hand-written digit recognition dataset that comes packaged up with the TensorFlow installation.</p>
<p>To create the dataset, we can run the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The MNIST data can be extracted from this mnist data set by calling mnist.train.next_batch(batch_size). In this case, we&#8217;ll just be looking at the training data, but you can also extract a test dataset from the same data. In this example, I&#8217;ll be using the feed_dict methodology and placeholder variables to feed in the training data, which isn&#8217;t the optimal method (see my <a href="http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/">Dataset tutorial</a> for the most efficient data consumption methodology) but it will do for these purposes. First, I&#8217;ll setup the data placeholders:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">self.input_images = tf.placeholder(tf.float32, shape=[None, self._input_size])
self.labels = tf.placeholder(tf.float32, shape=[None, self._label_size])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note, I have created these variables in an overarching class called Model, hence all the <em>self</em> references. The MNIST data input size (<em>self._input_size</em>) is equal to the 28 x 28 image pixels i.e. 784 pixels. The number of associated labels, <em>self._label_size</em> is equal to the 10 possible hand-written digit classes in the MNIST dataset.</p>
<p>In this tutorial, we&#8217;ll be creating a slightly deep fully connected network &#8211; a network with 7 total layers including input and output layers. To create these densely connected layers easily, we&#8217;ll be using TensorFlow&#8217;s handy tf.layers API and a simple Python loop like follows:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create self._num_layers dense layers as the model
input = self.input_images
for i in range(self._num_layers - 1):
    input = tf.layers.dense(input, self._hidden_size, activation=self._activation,
                                    name=&#039;layer{}&#039;.format(i+1))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, the generic <em>input </em>variable is initialized to be equal to the input images (fed via the placeholder). Next, the code runs through a loop where multiple dense layers are created, each named &#8216;layerX&#8217; where X is the layer number. The number of nodes in the layer is set equal to the class property <em>self._hidden_size</em> and the activation function is also supplied via the property <em>self._activation</em>.</p>
<p>Next we create the final, output layer (you&#8217;ll note that the loop above terminates before it gets to creating the final layer), and we don&#8217;t supply an activation to this layer. In the tf.layers API, a linear activation (i.e. <em>f(x) = x</em>) is applied by default if no activation argument is supplied.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># don&#039;t supply an activation for the final layer - the loss definition will
# supply softmax activation. This defaults to a linear activation i.e. f(x) = x
logits = tf.layers.dense(input, 10, name=&#039;layer{}&#039;.format(self._num_layers))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Next, the loss operation is setup and logged:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># use softmax cross entropy with logits - no need to apply softmax activation to
# logits
self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                             labels=self.labels))
# add the loss to the summary
tf.summary.scalar(&#039;loss&#039;, self.loss)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The loss used in this instance is the handy TensorFlow softmax_cross_entropy_with_logits_v2 (the original version is soon to be deprecated). This loss function will apply the <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax</a> operation to the un-activated output of the network, then apply the <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">cross entropy loss</a> to this outcome. After this loss operation is created, it&#8217;s output value is added to the tf.summary framework. This framework allows scalar values to be logged and subsequently visualized in the TensorBoard web-based visualization page. It can also log histogram information, along with audio and images &#8211; all of these can be observed through the aforementioned TensorBoard visualization.</p>
<p>Next, the program calls a method to log the gradients, which we will visualize to examine the vanishing gradient problem:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">self._log_gradients(self._num_layers)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This method looks like the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def _log_gradients(self, num_layers):
    gr = tf.get_default_graph()
    for i in range(num_layers):
        weight = gr.get_tensor_by_name(&#039;layer{}/kernel:0&#039;.format(i + 1))
        grad = tf.gradients(self.loss, weight)[0]
        mean = tf.reduce_mean(tf.abs(grad))
        tf.summary.scalar(&#039;mean_{}&#039;.format(i + 1), mean)
        tf.summary.histogram(&#039;histogram_{}&#039;.format(i + 1), grad)
        tf.summary.histogram(&#039;hist_weights_{}&#039;.format(i + 1), grad)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this method, first the TensorFlow computational graph is extracted so that weight variables can be called out of it. Then a loop is entered into, to cycle through all the layers. For each layer, first the weight tensor for the given layer is grabbed by the handy function get_tensor_by_name. You will recall that each layer was named &#8220;layerX&#8221; where X is the layer number. This is supplied to the function, along with &#8220;/kernel:0&#8221; &#8211; this tells the function that we are trying to access the weight variable (also called a kernel) as opposed to the bias value, which would be &#8220;/bias:0&#8221;.</p>
<p>On the next line, the tf.gradients() function is used. This will calculate gradients of the form $\partial y / \partial x$ where the first argument supplied to the function is <em>y</em> and the second is <em>x</em>. In the gradient descent step, the weight update is made in proportion to $\partial loss / \partial W$, so in this case the first argument supplied to tf.gradients() is the loss, and the second is the weight tensor.</p>
<p>Next, the mean absolute value of the gradient is calculated, and then this is logged as a scalar in the summary. Next, histograms of the gradients and the weight values are also logged in the summary. The flow now returns back to the main method in the class.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)
self.accuracy = self._compute_accuracy(logits, self.labels)
tf.summary.scalar(&#039;acc&#039;, self.accuracy)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The code above is fairly standard TensorFlow usage &#8211; defining an optimizer, in this case the flexible and powerful AdamOptimizer(), and also a generic accuracy operation, the outcome of which is also added to the summary (see the<a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener"> Github code</a> for the accuracy method called).</p>
<p>Finally a summary merge operation is created, which will gather up all the summary data ready for export to the TensorBoard file whenever it is executed:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">self.merged = tf.summary.merge_all()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>An initialization operation is also created. Now all that is left is to run the main training loop.</p>
<h3>Training the model</h3>
<p>The main training loop of this experimental model is shown in the code below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def run_training(model, mnist, sub_folder, iterations=2500, batch_size=30):
    with tf.Session() as sess:
        sess.run(model.init_op)
        train_writer = tf.summary.FileWriter(base_path + sub_folder,
                                             sess.graph)
        for i in range(iterations):
            image_batch, label_batch = mnist.train.next_batch(batch_size)
            l, _, acc = sess.run([model.loss, model.optimizer, model.accuracy],
                                 feed_dict={model.input_images: image_batch, model.labels: label_batch})
            if i % 200 == 0:
                summary = sess.run(model.merged, feed_dict={model.input_images: image_batch,
                                                            model.labels: label_batch})
                train_writer.add_summary(summary, i)
                print(&quot;Iteration {} of {}, loss: {:.3f}, train accuracy: &quot;
                      &quot;{:.2f}%&quot;.format(i, iterations, l, acc * 100))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This is a pretty standard TensorFlow training loop (if you&#8217;re unfamiliar with this, see my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow tutorial</a>) &#8211; however, one non-standard addition is the tf.summary.FileWriter() operation and its associated uses. This operation generally takes two arguments &#8211; the location to store the files and the session graph. Note that it is a good idea to setup a different sub folder for each of your TensorFlow runs when using summaries, as this allows for better visualization and comparison of the various runs within TensorBoard.</p>
<p>Every 200 iterations, we run the <em>merged</em> operation, which is defined in the class instance model &#8211; as mentioned previously, this gathers up all the logged summary data ready for writing. The train_writer.add_summary() operation is then run on this output, which writes the data into the chosen location (optionally along with the iteration/epoch number).</p>
<p>The summary data can then be visualized using TensorBoard. To run TensorBoard, using command prompt, navigate to the base directory where all the sub folders are stored, and run the following command:</p>
<blockquote><p>tensorboard &#8211;log_dir=whatever_your_folder_path_is</p></blockquote>
<p>Upon running this command, you will see startup information in the prompt which will tell you the address to type into your browser which will bring up the TensorBoard interface. Note that the TensorBoard page will update itself dynamically during training, so you can visually monitor the progress.</p>
<p>Now, to run this whole experiment, we can run the following code which cycles through each of the activation functions:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">scenarios = [&quot;sigmoid&quot;, &quot;relu&quot;, &quot;leaky_relu&quot;]
act_funcs = [tf.sigmoid, tf.nn.relu, tf.nn.leaky_relu]
assert len(scenarios) == len(act_funcs)
# collect the training data
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)
for i in range(len(scenarios)):
    tf.reset_default_graph()
    print(&quot;Running scenario: {}&quot;.format(scenarios[i]))
    model = Model(784, 10, act_funcs[i], 6, 10)
    run_training(model, mnist, scenarios[i])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This should be pretty self-explanatory. Three scenarios are investigated &#8211; a scenario for each type of activation reviewed: sigmoid, ReLU and Leaky ReLU. Note that, in this experiment, I&#8217;ve setup a densely connected model with 6 layers (including the output layer but excluding the input layer), with each having a layer size of 10 nodes.</p>
<h3>Analyzing the results</h3>
<p>The first figure below shows the training accuracy of the network, for each of the activations:</p>
<figure id="attachment_859" style="width: 355px" class="wp-caption aligncenter"><img class=" wp-image-859" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Vanishing-gradient-accuracy-three-scenarios.png" alt="Vanishing gradient TensorFlow - accuracy scenarios" width="355" height="230" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Vanishing-gradient-accuracy-three-scenarios.png 484w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Vanishing-gradient-accuracy-three-scenarios-300x195.png 300w" sizes="(max-width: 355px) 100vw, 355px" /><figcaption class="wp-caption-text">Accuracy of the three activation scenarios &#8211; sigmoid (blue), ReLU (red), Leaky ReLU (green)</figcaption></figure>
<p>As can be observed, the sigmoid (blue) significantly under performs the ReLU and Leaky ReLU activation functions. Is this due to the vanishing gradient problem? The plots below show the mean absolute gradient logs during training, again for the three scenarios:</p>
<figure id="attachment_862" style="width: 314px" class="wp-caption aligncenter"><img class=" wp-image-862" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-output-layer-gradients.png" alt="Vanishing gradients - TensorFlow - output layer mean gradients" width="314" height="210" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-output-layer-gradients.png 484w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-output-layer-gradients-300x201.png 300w" sizes="(max-width: 314px) 100vw, 314px" /><figcaption class="wp-caption-text">Three scenario mean absolute gradients &#8211; output layer (6th layer) &#8211; sigmoid (blue), ReLU (red), Leaky ReLU (green)</figcaption></figure>
<figure id="attachment_863" style="width: 330px" class="wp-caption aligncenter"><img class=" wp-image-863" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-1st-layer-gradients.png" alt="Vanishing gradient TensorFlow - 1st layer mean gradients" width="330" height="227" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-1st-layer-gradients.png 469w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-1st-layer-gradients-300x207.png 300w" sizes="(max-width: 330px) 100vw, 330px" /><figcaption class="wp-caption-text">Three scenario mean absolute gradients &#8211; 1st layer &#8211; sigmoid (blue), ReLU (red), Leaky ReLU (green)</figcaption></figure>
<p>The first graph shows the mean absolute gradients of the loss with respect to the weights for the output layer, and the second graph shows the same gradients for the first layer, for all three activation scenarios. First, it is clear that the overall magnitudes of the gradients for the ReLU activated networks are significantly greater than those in the sigmoid activated network. It can also be observed that there is a significant reduction in the gradient magnitudes between the output layer (layer 6) and the first layer (layer 1). This is the vanishing gradient problem.</p>
<p>You may be wondering why the ReLU activated networks still experience a significant reduction in the gradient values from the output layer to the first layer &#8211; weren&#8217;t these activation functions, with their gradients of 1 for activated regions, supposed to stop vanishing gradients? Yes and no. The gradient of the ReLU functions where <em>x &gt; 0</em> is 1, so there is no degradation in multiplying 1&#8217;s together. However, the &#8220;chaining&#8221; expression I showed previously describing the vanishing gradient problem, i.e.:</p>
<p>$$ \frac{\partial C} {\partial W_l} \propto  f'(z^{(l)}) f'(z^{(l+1)}) f'(z^{(l+2)}) \dots$$</p>
<p>isn&#8217;t quite the full picture. Rather, the back-propagation product is also in some sense proportional to the values of the weights in each layer, so more completely, it looks something like this:</p>
<p>$$ \frac{\partial C} {\partial W_l} \propto  f'(z^{(l)}) \cdot W_{l} \cdot f'(z^{(l+1)}) \cdot W_{l+1} \cdot f'(z^{(l+2)}) \cdot W_{l+2} \dots$$</p>
<p>So if the weight values are consistently &lt; 0, then we will also see a vanishing of gradients, as the chained expression will reduce through the layers as the weight values &lt; 0 are multiplied together. We can confirm that the weight values in this case are &lt; 0 by checking the histogram that was logged for the weight values in each layer:</p>
<figure id="attachment_866" style="width: 308px" class="wp-caption aligncenter"><img class=" wp-image-866" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-leaky-ReLU-weights-layer-4.png" alt="Vanishing gradients TensorFlow - layer 4 weights leaky ReLU" width="308" height="204" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-leaky-ReLU-weights-layer-4.png 435w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/04/Vanishing-gradient-leaky-ReLU-weights-layer-4-300x199.png 300w" sizes="(max-width: 308px) 100vw, 308px" /><figcaption class="wp-caption-text">Distribution of layer 4 weights &#8211; leaky ReLU scenario</figcaption></figure>
<p>The diagram above shows the histogram of layer 4 weights in the leaky ReLU scenario as they evolve through the epochs (y axis) &#8211; this is a handy visualization available in the TensorBoard panel. Note that the weights are consistently &lt; 0, and therefore we should expect the gradients to reduce even under the ReLU scenarios.</p>
<p>In saying all this, we can observe that the degradation of the gradients is <em>significantly worse </em>in the sigmoid scenario than the ReLU scenarios. The mean absolute weight reduces by a factor of 30 between layer 6 and layer 1 for the sigmoid scenario, compared to a factor of 6 for the leaky ReLU scenario (the standard ReLU scenario is pretty much the same). Therefore, while there is still a vanishing gradient problem in the network presented, it is <em>greatly reduced</em> by using the ReLU activation functions. This benefit can be observed in the significantly better performance of the ReLU activation scenarios compared to the sigmoid scenario. Note that, at least in this example, there is not an observable benefit of the leaky ReLU activation function over the standard ReLU activation function.</p>
<p>In summary then, this post has shown you how the vanishing gradient problem comes about, particularly when using the old canonical sigmoid activation function. However, the problem can be greatly reduced using the ReLU family of activation functions. You will also have seen how to log summary information in TensorFlow and plot it in TensorBoard to understand more about your networks. Hope it helps.</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&#038;offerid=323058.1326292&#038;type=2&#038;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><IMG border=0 width=1 height=1 src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.1326292&#038;type=2&#038;subid=0" ></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/">The vanishing gradient problem and ReLUs &#8211; a TensorFlow investigation</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>TensorFlow Dataset API tutorial &#8211; build high performance data pipelines</title>
		<link>http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments</comments>
		<pubDate>Sat, 17 Mar 2018 01:55:31 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=817</guid>
		<description><![CDATA[<p>Consuming data efficiently becomes really paramount to training performance in deep learning. In a previous post I discussed the TensorFlow data queuing framework. However, TensorFlow <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/" title="TensorFlow Dataset API tutorial &#8211; build high performance data pipelines">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/">TensorFlow Dataset API tutorial &#8211; build high performance data pipelines</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Consuming data efficiently becomes really paramount to training performance in deep learning. In a previous post I discussed the <a href="http://adventuresinmachinelearning.com/introduction-tensorflow-queuing/" target="_blank" rel="noopener">TensorFlow data queuing framework</a>. However, TensorFlow development is always on the move and they have now created a more streamlined and efficient way of setting up data input pipelines. This TensorFlow Dataset tutorial will show you how to use this Dataset framework to enable you to produce highly efficient input data pipelines. This is an important topic which isn&#8217;t covered very well in most TensorFlow tutorials &#8211; rather, these tutorials will often use the <em>feed_dict</em> and placeholder method of feeding data into the model. This method of feeding data into your network in TensorFlow is <em>inefficient</em> and will likely slow down your training for large, realistic datasets &#8211; see a discussion about this on the <a href="https://www.tensorflow.org/performance/performance_guide" target="_blank" rel="noopener">TensorFlow website. </a>Why is this framework better than the feed_dict method that is so commonly used? Simply, all of the operations to transform data and feed it into the model which can be performed with the Dataset API i.e. reading the data from arrays and files, transforming it, shuffling it etc. can all be automatically optimized and paralleled to provide efficient consumption of data.</p>
<p>In this TensorFlow Dataset tutorial, I will show you how to use the framework with some simple examples, and finally show you how to consume the scikit-learn MNIST dataset to create an MNIST classifier. As always, the code for this tutorial can be found on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>.</p>
<p>&nbsp;</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&#038;offerid=323058.1326292&#038;type=2&#038;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><IMG border=0 width=1 height=1 src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.1326292&#038;type=2&#038;subid=0" ></p>
<hr />
<h2>The TensorFlow Dataset framework &#8211; main components</h2>
<p>The TensorFlow Dataset framework has two main components:</p>
<ul>
<li>The Dataset</li>
<li>An associated Iterator</li>
</ul>
<p>The Dataset is basically where the data resides. This data can be loaded in from a number of sources &#8211; existing tensors, numpy arrays and numpy files, the TFRecord format and direct from text files. Once you&#8217;ve loaded the data into the Dataset object, you can string together various operations to apply to the data, these include operations such as:</p>
<ul>
<li>batch() &#8211; this allows you to consume the data from your TensorFlow Dataset in batches</li>
<li>map() &#8211; this allows you to transform the data using lambda statements applied to each element</li>
<li>zip() &#8211; this allows you to zip together different Dataset objects into a new Dataset, in a similar way to the Python zip function</li>
<li>filter() &#8211; this allows you to remove problematic data-points in your data-set, again based on some lambda function</li>
<li>repeat() &#8211; this operation restricts the number of times data is consumed from the Dataset before a tf.errors.OutOfRangeError error is thrown</li>
<li>shuffle() &#8211; this operation shuffles the data in the Dataset</li>
</ul>
<p>There are many other methods that the Dataset API includes &#8211; see <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat" target="_blank" rel="noopener">here</a> for more details.  The next component in the TensorFlow Dataset framework is the Iterator. This creates operations which can be called during the training, validation and/or testing of your model in TensorFlow. I&#8217;ll introduce more of both components in some examples below.</p>
<h2>Simple TensorFlow Dataset examples</h2>
<p>In the first simple example, we&#8217;ll create a dataset out of numpy ranges:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = np.arange(0, 10)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can create a TensorFlow Dataset object straight from a numpy array using <em>from_tensor_slices()</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create dataset object from numpy array
dx = tf.data.Dataset.from_tensor_slices(x)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The object <em>dx</em> is now a TensorFlow Dataset object. The next step is to create an Iterator that will extract data from this dataset. In the code below, the iterator is created using the method <em>make_one_shot_iterator()</em>.  The iterator arising from this method can only be initialized and run once &#8211; it can&#8217;t be re-initialized. The importance of being able to re-initialize an iterator will be explained more later.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create a one-shot iterator
iterator = dx.make_one_shot_iterator()
# extract an element
next_element = iterator.get_next()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>After the iterator is created, the next step is to setup a TensorFlow operation which can be called from the training code to extract the next element from the dataset. Finally, the dataset operation can be examined by running the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">with tf.Session() as sess:
    for i in range(11):
        val = sess.run(next_element)
        print(val)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code will print out integers from 0 to 9 but then throw an OutOfRangeError. This is because the code extracted all the data slices from the dataset and it is now out of range or &#8220;empty&#8221;.</p>
<p>If we want to repeatedly extract data from a dataset, one way we can do it is to make the dataset re-initializable. We can do that by first adjusting the <em>make_one_shot_iterator()</em> line to the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">iterator = dx.make_initializable_iterator()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Then, within the TensorFlow session, the code looks like this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">with tf.Session() as sess:
    sess.run(iterator.initializer)
    for i in range(15):
        val = sess.run(next_element)
        print(val)
        if i % 9 == 0 and i &gt; 0:
            sess.run(iterator.initializer)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that the first operation run is the iterator.initializer operation. This is required to get your iterator ready for action and if you don&#8217;t do this before running the next_element operation it will throw an error. The final change is the last two lines: this <em>if </em>statement ensures that when we know that the iterator has run out of data (i.e. i == 9), the iterator is re-initialized by the iterator.initializer operation. Running this new code will produce: 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4. No error this time!</p>
<p>There are also other things that can be done to manipulate the dataset and how it can be used. First, the batch function:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">dx = tf.data.Dataset.from_tensor_slices(x).batch(3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>After this change, when the next_element operation is run, a batch of length 3 will be extracted from the data. Running the code below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">with tf.Session() as sess:
    sess.run(iterator.initializer)
    for i in range(15):
        val = sess.run(next_element)
        print(val)
        if (i + 1) % (10 // 3) == 0 and i &gt; 0:
            sess.run(iterator.initializer)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Will produce an output like:</p>
[0 1 2]
[3 4 5]
[6 7 8]
[0 1 2]
[3 4 5]
[6 7 8]
<p>and so on.</p>
<p>Next, we can zip together datasets. This is useful when pairing up input-output training/validation pairs of data (i.e. input images and matching labels for each image). The code below does this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def simple_zip_example():
    x = np.arange(0, 10)
    y = np.arange(1, 11)
    # create dataset objects from the arrays
    dx = tf.data.Dataset.from_tensor_slices(x)
    dy = tf.data.Dataset.from_tensor_slices(y)
    # zip the two datasets together
    dcomb = tf.data.Dataset.zip((dx, dy)).batch(3)
    iterator = dcomb.make_initializable_iterator()
    # extract an element
    next_element = iterator.get_next()
    with tf.Session() as sess:
        sess.run(iterator.initializer)
        for i in range(15):
            val = sess.run(next_element)
            print(val)
            if (i + 1) % (10 // 3) == 0 and i &gt; 0:
                sess.run(iterator.initializer)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The zip combination of the two datasets (<em>dx, dy</em>) can be seen in the line where <em>dcomb</em> is created. Note the chaining together of multiple operations &#8211; first the zip method, then the batching operation. The rest of the code is the same. This code will produce an output like the following:</p>
<p>(array([0, 1, 2]), array([1, 2, 3]))<br />
(array([3, 4, 5]), array([4, 5, 6]))<br />
(array([6, 7, 8]), array([7, 8, 9]))<br />
(array([0, 1, 2]), array([1, 2, 3]))</p>
<p>and so on. As you can observe, the batching takes place appropriately within the zipped together datasets i.e. 3 items from dx, 3 items from dy. As stated above, this is handy for combining input data and matching labels.</p>
<p>Note, the re-initialization <em>if </em>statement on the last two lines is a bit unwieldy, we can actually get rid of it by replacing the <em>dcomb</em> dataset creation line with the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">dcomb = tf.data.Dataset.zip((dx, dy)).repeat().batch(3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note the addition of the <em>repeat()</em> method to the operation list. When this method is applied to the dataset with no argument, it means that the dataset can be repeated indefinitely without throwing an OutOfRangeError. This will be shown in the next more detailed example &#8211; using the sci-kit learn MNIST dataset to create a hand-written digits classifier.</p>
<h2>TensorFlow Dataset MNIST example</h2>
<p>In this section, I&#8217;ll show how to create an MNIST hand-written digit classifier which will consume the MNIST image and label data from the simplified MNIST dataset supplied from the Python <a href="http://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">scikit-learn</a> package (a must-have package for practical machine learning enthusiasts). I&#8217;ll step through the code slowly below.</p>
<p>First, we have to load the data from the package and split it into train and validation datasets. This can be performed with the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># load the data
digits = load_digits(return_X_y=True)
# split into train and validation sets
train_images = digits[0][:int(len(digits[0]) * 0.8)]
train_labels = digits[1][:int(len(digits[0]) * 0.8)]
valid_images = digits[0][int(len(digits[0]) * 0.8):]
valid_labels = digits[1][int(len(digits[0]) * 0.8):]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The load_digits method will extract the data from the relevant location in the scikit-learn package, and the code above splits the first 80% of the data into the training arrays, and the remaining 20% into the validation arrays.</p>
<p>Next, the TensorFlow Datasets of the training data are created:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the training datasets
dx_train = tf.data.Dataset.from_tensor_slices(train_images)
# apply a one-hot transformation to each label for use in the neural network
dy_train = tf.data.Dataset.from_tensor_slices(train_labels).map(lambda z: tf.one_hot(z, 10))
# zip the x and y training data together and shuffle, batch etc.
train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The <em>dx_train </em>statement is straightforward, however there is an extra element that has been added in the <em>dy_train</em> statement. Note the use of the <em>map()</em> method. The labels in the MNIST dataset are integers between 0 and 9 corresponding to the hand-written digit in the image. This integer data must be transformed into one-hot format, i.e. the integer label 4 transformed into the vector [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. To do this, the lambda statement is used, where every row (expressed as <em>z</em> in the above) in the label dataset is transformed into one-hot data format using the TensorFlow one_hot function. If you&#8217;d like to learn more about one hot data structures and neural networks, see <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">my neural network tutorial</a>.</p>
<p>Finally, the training <em>x</em> and <em>y</em> data is zipped together in the full <em>train_dataset</em>. Chained along together with this zip method is first the <em>shuffle()</em> dataset method. This method randomly shuffles the data, using a buffer of data specified in the argument &#8211; 500 in this case. Next, the <em>repeat() </em>method is used, to allow the iterator to continuously extract data from this dataset, finally the data is batched with a batch size of 30.</p>
<p>The same steps are used to create the validation dataset:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># do the same operations for the validation set
dx_valid = tf.data.Dataset.from_tensor_slices(valid_images)
dy_valid = tf.data.Dataset.from_tensor_slices(valid_labels).map(lambda z: tf.one_hot(z, 10))
valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now, we want to be able to extract data from either the train_dataset or the valid_dataset seamlessly. This is important, as we don&#8217;t want to have to change how data flows through the neural network structure when all we want to do is just change the dataset the model is consuming. To do this, we can use another way of creating the Iterator object &#8211; the <em>from_structure() </em>method. This method creates a <em>generic</em> iterator object &#8211; all it needs is the data types of the data it will be outputting and the output data size/shape in order to be created. The code below uses this methodology:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create general iterator
iterator = tf.data.Iterator.from_structure(train_dataset.output_types,
                                               train_dataset.output_shapes)
next_element = iterator.get_next()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The second line of the above creates a standard <em>get_next() </em>iterator operation which can be called to extract data from this generic iterator structure. Next, we need some operations which can be called during training or validating to initialize this generic iterator and &#8220;point it&#8221; to the desired dataset. These are as follows:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># make datasets that we can initialize separately, but using the same structure via the common iterator
training_init_op = iterator.make_initializer(train_dataset)
validation_init_op = iterator.make_initializer(valid_dataset)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>These operations can be run to &#8220;switch over&#8221; the iterator from one dataset to another. This &#8220;switching over&#8221; will be demonstrated in code below.</p>
<p>Next, the neural network model is created &#8211; this is standard TensorFlow usage and in this case I will be utilizing the TensorFlow layers API to create a simple fully connected or dense neural network, with dropout and a first layer of batch normalization to effectively scale the input data. If you&#8217;d like to learn some of the basics of TensorFlow, check out my <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">Python TensorFlow tutorial</a>. The TensorFlow model is defined as follows:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def nn_model(in_data):
    bn = tf.layers.batch_normalization(in_data)
    fc1 = tf.layers.dense(bn, 50)
    fc2 = tf.layers.dense(fc1, 50)
    fc2 = tf.layers.dropout(fc2)
    fc3 = tf.layers.dense(fc2, 10)
    return fc3</code></pre> <div class="code-embed-infos"> </div> </div>
<p>To call this model creation function, the code below can be used:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create the neural network model
logits = nn_model(next_element[0])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that the next_element operation is handled directly in the model &#8211; in other words, it doesn&#8217;t need to be called explicitly during the training loop as will be seen below. Rather, whenever any of the operations following this point in the graph are called (i.e. the loss operation, the optimization operation etc.) the TensorFlow graph structure will know to run the next_element operation and extract the data from whichever dataset has been initialized into the iterator. The next_element operation, because it is operating on the generic iterator which is defined by the shape of the train_dataset, is a tuple &#8211; the first element ([0]) will contain the MNIST images, while the second element ([1]) will contain the corresponding labels. Therefore, next_element[0] will extract the image data batch and send it into the neural network model (nn_model) as the input data.</p>
<p>Next are some standard TensorFlow operations to calculate the loss function, the optimization step and prediction accuracy (again, for more details see <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">this tutorial</a> or <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">this one</a>):</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># add the optimizer and loss
loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))
optimizer = tf.train.AdamOptimizer().minimize(loss)
# get accuracy
prediction = tf.argmax(logits, 1)
equality = tf.equal(prediction, tf.argmax(next_element[1], 1))
accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))
init_op = tf.global_variables_initializer()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now we can run the training loop:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># run the training
epochs = 600
with tf.Session() as sess:
    sess.run(init_op)
    sess.run(training_init_op)
    for i in range(epochs):
        l, _, acc = sess.run([loss, optimizer, accuracy])
        if i % 50 == 0:
            print(&quot;Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%&quot;.format(i, l, acc * 100))
    # now setup the validation run
    valid_iters = 100
    # re-initialize the iterator, but this time with validation data
    sess.run(validation_init_op)
    avg_acc = 0
    for i in range(valid_iters):
        acc = sess.run([accuracy])
        avg_acc += acc[0]
    print(&quot;Average validation set accuracy over {} iterations is {:.2f}%&quot;.format(valid_iters,                                                                              (avg_acc / valid_iters) * 100))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As can be observed, before the main training loop is entered into, the session executes the training_init_op operation, which initializes the generic iterator to extract data from train_dataset. After running <em>epochs</em> iterations to train the model, we then want to check how the trained model performs on the validation dataset (valid_dataset). To do this, we can simply run the validation_init_op operation in the session to point the generic iterator to valid_dataset. Then we run the accuracy operation as per normal, knowing that the operation will be calculating the model accuracy based on the validation data, rather than the training data. Running this code will produce an output that will look something like:</p>
<figure id="attachment_828" style="width: 490px" class="wp-caption alignnone"><img class=" wp-image-828" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Dataset-tutorial-MNIST-output.png" alt="TensorFlow Dataset tutorial - MNIST example output" width="490" height="235" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Dataset-tutorial-MNIST-output.png 687w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Dataset-tutorial-MNIST-output-300x144.png 300w" sizes="(max-width: 490px) 100vw, 490px" /><figcaption class="wp-caption-text">TensorFlow Dataset tutorial &#8211; MNIST example output</figcaption></figure>
<p>Obviously not a create validation set accuracy for MNIST &#8211; but this is just an example model to demonstrate how to use the TensorFlow Dataset framework. For more accurate ways of performing image classification, check out my <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">Convolutional Neural Network Tutorial in TensorFlow</a>.</p>
<p>So there you have it &#8211; hopefully you are now in a position to use this new, streamlined data input pipeline API in TensorFlow. Enjoy your newly optimized TensorFlow code.</p>
<hr />
<p><strong>Recommended</strong><strong> online course: </strong>If you&#8217;d like to learn more about TensorFlow I&#8217;d recommend the following inexpensive Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&#038;offerid=323058.1326292&#038;type=2&#038;murl=https%3A%2F%2Fwww.udemy.com%2Fcomplete-guide-to-tensorflow-for-deep-learning-with-python%2F">Complete Guide to TensorFlow for Deep Learning with Python</a><IMG border=0 width=1 height=1 src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&#038;bids=323058.1326292&#038;type=2&#038;subid=0" ></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/">TensorFlow Dataset API tutorial &#8211; build high performance data pipelines</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
		<item>
		<title>Reinforcement learning tutorial using Python and Keras</title>
		<link>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/</link>
		<comments>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments</comments>
		<pubDate>Sat, 03 Mar 2018 03:15:48 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Keras]]></category>
		<category><![CDATA[Reinforcement learning]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=763</guid>
		<description><![CDATA[<p>In this post, I&#8217;m going to introduce the concept of reinforcement learning, and show you how to build an autonomous agent that can successfully play <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/" title="Reinforcement learning tutorial using Python and Keras">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/">Reinforcement learning tutorial using Python and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In this post, I&#8217;m going to introduce the concept of reinforcement learning, and show you how to build an autonomous agent that can successfully play a simple game. Reinforcement learning is an active and interesting area of machine learning research, and has been spurred on by recent successes such as the <a href="https://en.wikipedia.org/wiki/AlphaGo" target="_blank" rel="noopener">AlphaGo system</a>, which has convincingly beat the best human players in the world. This occurred in a game that was thought too difficult for machines to learn. In this tutorial, I&#8217;ll first detail some background theory while dealing with a toy game in the Open AI Gym toolkit. We&#8217;ll then create a Q table of this game using simple Python, and then create a Q network using Keras. If you&#8217;d like to scrub up on Keras, check out my <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">introductory Keras tutorial</a>. All code present in this tutorial is available on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github page</a>.</p>
<hr />
<p><strong>Recommended online course </strong>&#8211;<strong> </strong>If you&#8217;re more of a video based learner, I&#8217;d recommend the following inexpensive Udemy online course in reinforcement learning: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1080408&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fartificial-intelligence-reinforcement-learning-in-python%2F" target="new">Artificial Intelligence: Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Reinforcement learning &#8211; the basics</h1>
<p>Reinforcement learning can be considered the third genre of the machine learning triad &#8211; unsupervised learning, supervised learning and reinforcement learning. In supervised learning, we supply the machine learning system with curated (x, y) training pairs, where the intention is for the network to learn to map x to y. In reinforcement learning, we create an <em>agent</em> which performs <em>actions </em>in an <em>environment </em>and the agent receives various <em>rewards</em> depending on what <em>state </em>it is in when it performs the action. In other words, an agent explores a kind of game, and it is trained by trying to maximize rewards in this game. This cycle is illustrated in the figure below:</p>
<figure id="attachment_768" style="width: 381px" class="wp-caption aligncenter"><img class="size-full wp-image-768" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png" alt="Reinforcement learning with Python and Keras - Reinforcement learning environment" width="381" height="211" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment.png 381w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Reinforcement-learning-environment-300x166.png 300w" sizes="(max-width: 381px) 100vw, 381px" /><figcaption class="wp-caption-text">Reinforcement learning environment</figcaption></figure>
<p>As can be observed above, the agent performs some action in the environment. An interpreter views this action in the environment, and feeds back an updated state that the agent now resides in, and also the reward for taking this action. The environment is not known by the agent beforehand, but rather it is discovered by the agent taking incremental steps in time. So, for instance, at time <em>t </em>the agent, in state <em>$s_{t}$</em>,  may take action <em>a</em>. This results in a new state <em>$s_{t+1}$ </em>and a reward <em>r.</em> This reward can be a positive real number, zero, or a negative real number. It is the goal of the agent to learn which <em>state dependent action</em> to take which maximizes its rewards. The way which the agent optimally learns is the subject of reinforcement learning theory and methodologies.</p>
<p>To more meaningfully examine the theory and possible approaches behind reinforcement learning, it is useful to have a simple example in which to work through. This simple example will come from an environment available on <a href="https://gym.openai.com/envs/NChain-v0/" target="_blank" rel="noopener">Open AI Gym called NChain</a>.</p>
<h2>Open AI Gym example</h2>
<p>The NChain example on Open AI Gym is a simple 5 state environment. There are two possible actions in each state, move forward (action 0) and move backwards (action 1). When action 1 is taken, i.e. move backwards, there is an immediate reward of 2 given to the agent &#8211; and the agent is returned to state 0 (back to the beginning of the chain). However, when a move forward action is taken (action 0), there is no immediate reward until state 4. When the agent moves forward while in state 4, a reward of 10 is received by the agent. The agent stays in state 4 at this point also, so the reward can be repeated. There is also a random chance that the agent&#8217;s action is &#8220;flipped&#8221; by the environment (i.e. an action 0 is flipped to an action 1 and vice versa). The diagram below demonstrates this environment:</p>
<figure id="attachment_769" style="width: 648px" class="wp-caption aligncenter"><img class=" wp-image-769" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png" alt="Reinforcement learning - Python and Keras - NChain environment" width="648" height="207" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration.png 906w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration-300x96.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-illustration-768x245.png 768w" sizes="(max-width: 648px) 100vw, 648px" /><figcaption class="wp-caption-text">Open AI Gym&#8217;s NChain environment</figcaption></figure>
<p>You can play around with this environment by first installing the Open AI Gym Python package &#8211; see instructions <a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">here</a>. Then simply open up your Python command prompt and have a play &#8211; see the figure below for an example of some of the commands available:</p>
<figure id="attachment_771" style="width: 449px" class="wp-caption aligncenter"><img class=" wp-image-771" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround.png" alt="Reinforcement learning - Python and Keras - NChain Python playaround" width="449" height="329" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround.png 795w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-300x220.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-768x562.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/NChain-Python-playaround-80x60.png 80w" sizes="(max-width: 449px) 100vw, 449px" /><figcaption class="wp-caption-text">NChain Python playaround</figcaption></figure>
<p>If you examine the code above, you can observe that first the Python module is imported, and then the environment is loaded via the gym.make() command. The first step is to initalize / reset the environment by running env.reset() &#8211; this command returns the initial state of the environment &#8211; in this case 0. The first command I then run is env.step(1) &#8211; the value in the bracket is the action ID. As explained previously, action 1 represents a step back to the beginning of the chain (state 0). The step() command returns 4 variables in a tuple, these are (in order):</p>
<ul>
<li>The new state after the action</li>
<li>The reward due to the action</li>
<li>Whether the game is &#8220;done&#8221; or not &#8211; the NChain game is done after 1,000 steps</li>
<li>Debugging information &#8211; not relevant in this example</li>
</ul>
<p>As can be observed, starting in state 0 and taking step(1) action, the agent stays in state 0 and gets 2 for its reward. Next, I sent a series of action 0 commands. After every action 0 command, we would expect the progression of the agent along the chain, with the state increasing in increments (i.e. 0 -&gt; 1 -&gt; 2 etc.). However, you&#8217;ll observe after the first step(0) command, that the agent stays in state 0 and gets a 2 reward. This is because of the random tendency of the environment to &#8220;flip&#8221; the action occasionally, so the agent <em>actually</em> performed a 1 action. This is just unlucky.</p>
<p>Nevertheless, I persevere and it can be observed that the state increments as expected, but there is no immediate reward for doing so for the agent until it reaches state 4. When in state 4, an action of 0 will keep the agent in step 4 <em>and </em>give the agent a 10 reward. Not only that, the environment allows this to be done repeatedly, as long as it doesn&#8217;t produce an unlucky &#8220;flip&#8221;, which would send the agent back to state 0 &#8211; the beginning of the chain.</p>
<p>Now that we understand the environment that will be used in this tutorial, it is time to consider what method can be used to train the agent.</p>
<h2>A first naive heuristic for reinforcement learning</h2>
<p>In order to train the agent effectively, we need to find a good <em>policy $\pi$ </em>which maps states to actions in an optimal way to maximize reward. There are various ways of going about finding a good or optimal policy, but first, let&#8217;s consider a naive approach.</p>
<p>Let&#8217;s conceptualize a table, and call it a reward table, which looks like this:</p>
<p>$$<br />
\begin{bmatrix}<br />
r_{s_0,a_0} &amp; r_{s_0,a_1} \\<br />
r_{s_1,a_0} &amp; r_{s_1,a_1} \\<br />
r_{s_2,a_0} &amp; r_{s_2,a_1} \\<br />
r_{s_3,a_0} &amp; r_{s_3,a_1} \\<br />
r_{s_4,a_0} &amp; r_{s_4,a_1} \\<br />
\end{bmatrix}<br />
$$</p>
<p>Each of the rows corresponds to the 5 available states in the NChain environment, and each column corresponds to the 2 available actions in each state &#8211; forward and backward, 0 and 1. The value in each of these table cells corresponds to some measure of reward that the agent has &#8220;learnt&#8221; occurs when they are in that state and perform that action. So, the value $r_{s_0,a_0}$ would be, say, the sum of the rewards that the agent has received when in the past they have been in state 0 and taken action 0. This table would then let the agent choose between actions based on the summated (or average, median etc. &#8211; take your pick) amount of reward the agent has received in the past when taking actions 0 or 1.</p>
<p>This might be a good policy &#8211; choose the action resulting in the greatest previous summated reward. Let&#8217;s give it a try, the code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def naive_sum_reward_agent(env, num_episodes=500):
    # this is the table that will hold our summated rewards for
    # each action in each state
    r_table = np.zeros((5, 2))
    for g in range(num_episodes):
        s = env.reset()
        done = False
        while not done:
            if np.sum(r_table[s, :]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with highest cummulative reward
                a = np.argmax(r_table[s, :])
            new_s, r, done, _ = env.step(a)
            r_table[s, a] += r
            s = new_s
    return r_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the function definition, the environment is passed as the first argument, then the number of episodes (or number of games) that we will train the r_table on. We first create the r_table matrix which I presented previously and which will hold our summated rewards for each state and action. Then there is an outer loop which cycles through the number of episodes. The env.reset() command starts the game afresh each time a new episode is commenced. It also returns the starting state of the game, which is stored in the variable <em>s</em>.</p>
<p>The second, inner loop continues until a &#8220;done&#8221; signal is returned after an action is passed to the environment. The <em>if</em> statement on the first line of the inner loop checks to see if there are any existing values in the r_table for the current state &#8211; it does this by confirming if the sum across the row is equal to 0. If it is zero, then an action is chosen at random &#8211; there is no better information available at this stage to judge which action to take.</p>
<p>This condition will only last for a short period of time. After this point, there will be a value stored in at least one of the actions for each state, and the action will be chosen based on which column value is the largest for the row state <em>s.</em> In the code, this choice of the maximum column is executed by the numpy <em>argmax</em> function &#8211; this function returns the index of the vector / matrix with the highest value. For example, if the agent is in state 0 and we have the r_table with values [100, 1000] for the first row, action 1 will be selected as the index with the highest value is column 1.</p>
<p>After the action has been selected and stored in <em>a</em>, this action is fed into the environment with env.step(a). This command returns the new state, the reward for this action, whether the game is &#8220;done&#8221; at this stage and the debugging information that we are not interested in. In the next line, the r_table cell corresponding to state <em>s </em>and action <em>a </em>is updated by adding the reward to whatever is already existing in the table cell.</p>
<p>Finally the state <em>s </em>is updated to <em>new_s</em> &#8211; the new state of the agent.</p>
<p>If we run this function, the r_table will look something like:</p>
<p><img class="alignnone wp-image-788" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Naive-RL-model-r_table-1.png" alt="" width="177" height="101" /></p>
<p>Examining the results above, you can observe that the most common state for the agent to be in is the first state, seeing as any action 1 will bring the agent back to this point. The least occupied state is state 4, as it is difficult for the agent to progress from state 0 to 4 without the action being &#8220;flipped&#8221; and the agent being sent back to state 0. You can get different results if you run the function multiple times, and this is because of the stochastic nature of both the environment and the algorithm.</p>
<p>Clearly &#8211; something is wrong with this table. One would expect that in state 4, the most rewarding action for the agent would be to choose action 0, which would reward the agent with 10 points, instead of the usual 2 points for an action of 1. Not only that, but it has chosen action 0 for <em>all </em>states &#8211; this goes against intuition &#8211; surely it would be best to sometimes shoot for state 4 by choosing multiple action 0&#8217;s in a row, and that way reap the reward of multiple possible 10 scores.</p>
<p>In fact, there are a number of issues with this way of doing reinforcement learning:</p>
<ul>
<li>First, once there is a reward stored in one of the columns, the agent will always choose that action from that point on. This will lead to the table being &#8220;locked in&#8221; with respect to actions after just a few steps in the game.</li>
<li>Second, because no reward is obtained for most of the states when action 0 is picked, this model for training the agent has no way to encourage acting on <em>delayed reward</em> signal when it is appropriate for it to do so.</li>
</ul>
<p>Let&#8217;s see how these problems could be fixed.</p>
<h2>Delayed reward reinforcement learning</h2>
<p>If you want to be a medical doctor, you&#8217;re going to have to go through some pain to get there. You&#8217;ll be studying a long time before you&#8217;re free to practice on your own, and the rewards will be low while you are doing so. However, once you get to be a fully fledged MD, the rewards will be great. During your time studying, you would be operating under a delayed reward or delayed gratification paradigm in order to reach that greater reward. However, you might only be willing to undertake that period of delayed reward for a given period of time &#8211; you wouldn&#8217;t want to be studying forever, or at least, for decades.</p>
<p>We can bring these concepts into our understanding of reinforcement learning. Let&#8217;s say we are in state 3 &#8211; in the previous case, when the agent chose action 0 to get to state 3, the reward was zero and therefore r_table[3, 0] = 0. Obviously the agent would not see this as an attractive step compared to the alternative for this state i.e. r_table[3, 1] &gt;= 2. But what if we assigned to this state the reward the agent would received if it chose action 0 in state 4? It would look like this: r_table[3, 0] = r + 10 = 10 &#8211; a much more attractive alternative!</p>
<p>This idea of propagating possible reward from the best possible actions in future states is a core component of what is called <em>Q learning</em>. In Q learning, the Q value for each action in each state is updated when the relevant information is made available. The Q learning rule is:</p>
<p>$$Q(s, a) = Q(s, a) + \alpha (r + \gamma \max\limits_{a&#8217;} Q(s&#8217;, a&#8217;) &#8211; Q(s, a))$$</p>
<p>First, as you can observe, this is an <em>updating </em>rule &#8211; the existing Q value is added to, not replaced. Ignoring the $\alpha$ for the moment, we can concentrate on what&#8217;s inside the brackets. The first term, <em>r</em><em>, </em>is the reward that was obtained when action <em>a</em> was taken in state <em>s</em>. Next, we have an expression which is a bit more complicated. Ignore the $\gamma$ for the moment and focus on $\max\limits_{a&#8217;} Q(s&#8217;, a&#8217;)$. What this means is that we look at the next state <em>s&#8217;</em> after action <em>a</em> and return the maximum possible Q value in the next state. In other words, return the maximum Q value for the best possible action in the next state. In this way, the agent is looking forward to determine the best possible future rewards before making the next step <em>a</em>.</p>
<p>The $\gamma$ value is called the <em>discounting </em>factor &#8211; this decreases the impact of future rewards on the immediate decision making in state <em>s</em>. This is important, as this represents a limited patience in the agent &#8211; it won&#8217;t study forever to get that medical degree. So $\gamma$ will always be less than 1. The &#8211; Q(s, a) term acts to restrict the growth of the Q value as the training of the agent progresses through many iterations. Finally, this whole sum is multiplied by a <em>learning rate </em>$\alpha$ which restricts the updating to ensure it doesn&#8217;t &#8220;race&#8221; to a solution &#8211; this is important for optimal convergence (see my <em> </em><a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">neural networks tutorial</a> for more on learning rate).</p>
<p>Note that while the learning rule only examines the best action in the following state, in reality, discounted rewards still cascade down from future states. For instance, if we think of the cascading rewards from all the 0 actions (i.e. moving forward along the chain) and start at state 3, the Q reward will be $r + \gamma \max_a Q(s&#8217;, a&#8217;) = 0 + 0.95 * 10 = 9.5$ (with a $\gamma$ = 0.95). If we work back from state 3 to state 2 it will be 0 + 0.95 * 9.5 = 9.025. Likewise, the cascaded, discounted reward from to state 1 will be 0 + 0.95 * 9.025 = 8.57, and so on. Therefore, while the immediate updating calculation only looks at the maximum Q value for the next state, &#8220;upstream&#8221; rewards that have previously been discovered by the agent still cascade down into the present state and action decision. This is a simplification, due to the learning rate and random events in the environment, but represents the general idea.</p>
<p>Now that you (hopefully) understand Q learning, let&#8217;s see what it looks like in practice:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    lr = 0.8
    for i in range(num_episodes):
        s = env.reset()
        done = False
        while not done:
            if np.sum(q_table[s,:]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with largest q value in state s
                a = np.argmax(q_table[s, :])
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])
            s = new_s
    return q_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This function is almost exactly the same as the previous naive r_table function that was discussed. The additions and changes are:</p>
<ul>
<li>The variables <em>y</em> which specifies the discounting factor $\gamma$ and <em>lr</em> which is the Q table updating learning rate</li>
<li>The line:<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])</code></pre> <div class="code-embed-infos"> </div> </div></li>
</ul>
<p>This line executes the Q learning rule that was presented previously. The np.max(q_table[new_s, :]) is an easy way of selecting the maximum value in the q_table for the row new_s. After this function is run, an example q_table output is:</p>
<p><img class="alignnone wp-image-796" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table.png" alt="" width="248" height="107" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table.png 313w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/First-model-q_table-300x129.png 300w" sizes="(max-width: 248px) 100vw, 248px" /></p>
<p>This output is strange, isn&#8217;t it? Again, we would expect at least the state 4 &#8211; action 0 combination to have the highest Q score, but it doesn&#8217;t.  We might also expect the reward from this action in this state to have cascaded down through the states 0 to 3. Something has clearly gone wrong &#8211; and the answer is that there isn&#8217;t enough <em>exploration</em> going on within the agent training method.</p>
<h2>Q learning with $\epsilon$-greedy action selection</h2>
<p>If we think about the previous iteration of the agent training model using Q learning, the action selection policy is based solely on the maximum Q value in any given state. It is conceivable that, given the random nature of the environment, that the agent initially makes &#8220;bad&#8221; decisions. The Q values arising from these decisions may easily be &#8220;locked in&#8221; &#8211; and from that time forward, bad decisions may continue to be made by the agent because it can only ever select the maximum Q value in any given state, even if these values are not necessarily optimal. This action selection policy is called a <em>greedy </em>policy.</p>
<p>So we need a way for the agent to eventually always choose the &#8220;best&#8221; set of actions in the environment, yet at the same time allowing the agent to not get &#8220;locked in&#8221; and giving it some space to explore alternatives. What is required is the $\epsilon$-greedy policy.</p>
<p>The $\epsilon$-greedy policy in reinforcement learning is basically the same as the <em>greedy </em>policy, except that there is a value $\epsilon$ (which may be set to decay over time) where, if a random number is selected which is less than this value, an action is chosen completely at random. This step allows some random <em>exploration</em> of the value of various actions in various states, and can be scaled back over time to allow the algorithm to concentrate more on <em>exploiting </em>the best strategies that it has found. This mechanism can be expressed in code as:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def eps_greedy_q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    eps = 0.5
    lr = 0.8
    decay_factor = 0.999
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        done = False
        while not done:
            # select the action with highest cummulative reward
            if np.random.random() &lt; eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])
            # pdb.set_trace()
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])
            s = new_s
    return q_table</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code shows the introduction of the $\epsilon$ value &#8211; <em>eps</em>. There is also an associated <em>eps</em> decay_factor which exponentially decays eps with each episode <em>eps *= decay_factor</em>. The $\epsilon$-greedy based action selection can be found in this code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            if np.random.random() &lt; eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first component of the <em>if </em>statement shows a random number being selected, between 0 and 1, and determining if this is below <em>eps</em>. If so, the action will be selected randomly from the two possible actions in each state. The second part of the <em>if </em>statement is a random selection if there are no values stored in the q_table so far. If neither of these conditions hold true, the action is selected as per normal by taking the action with the highest q value.</p>
<p>The rest of the code is the same as the standard <em>greedy </em>implementation with Q learning discussed previously. This code produces a q_table which looks something like the following:</p>
<p><img class="alignnone wp-image-800" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table.png" alt="" width="242" height="102" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table.png 306w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/eps_greedy_q_table-300x126.png 300w" sizes="(max-width: 242px) 100vw, 242px" /></p>
<p>Finally we have a table which favors action 0 in state 4 &#8211; in other words what we would expect to happen given the reward of 10 that is up for grabs via that action in that state. Notice also that, as opposed to the previous tables from the other methods, that there are no actions with a 0 Q value &#8211; this is because the full action space has been explored via the randomness introduced by the $\epsilon$-greedy policy.</p>
<h2>Comparing the methods</h2>
<p>Let&#8217;s see if the last agent training model actually produces an agent that gathers the most rewards in any given game. The code below shows the three models trained and then tested over 100 iterations to see which agent performs the best over a test game. The models are trained as well as tested in each iteration because there is significant variability in the environment which messes around with the efficacy of the training &#8211; so this is an attempt to understand average performance of the different models. The main testing code looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def test_methods(env, num_iterations=100):
    winner = np.zeros((3,))
    for g in range(num_iterations):
        m0_table = naive_sum_reward_agent(env, 500)
        m1_table = q_learning_with_table(env, 500)
        m2_table = eps_greedy_q_learning_with_table(env, 500)
        m0 = run_game(m0_table, env)
        m1 = run_game(m1_table, env)
        m2 = run_game(m2_table, env)
        w = np.argmax(np.array([m0, m1, m2]))
        winner[w] += 1
        print(&quot;Game {} of {}&quot;.format(g + 1, num_iterations))
    return winner</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, this method creates a numpy zeros array of length 3 to hold the results of the winner in each iteration &#8211; the winning method is the method that returns the highest rewards after training and playing. The run_game function looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def run_game(table, env):
    s = env.reset()
    tot_reward = 0
    done = False
    while not done:
        a = np.argmax(table[s, :])
        s, r, done, _ = env.step(a)
        tot_reward += r
    return tot_reward</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here, it can be observed that the trained table given to the function is used for action selection, and the total reward accumulated during the game is returned. A sample outcome from this experiment (i.e. the vector w) is shown below:</p>
[13, 22, 65]
<p>&nbsp;</p>
<p>As can be observed, of the 100 experiments the $\epsilon$-greedy, Q learning algorithm (i.e. the third model that was presented) wins 65 of them. This is followed by the standard greedy implementation of Q learning, which won 22 of the experiments. Finally the naive accumulated rewards method only won 13 experiments. So as can be seen, the $\epsilon$-greedy Q learning method is quite an effective way of executing reinforcement learning.</p>
<p>So far, we have been dealing with explicit tables to hold information about the best actions and which actions to choose in any given state. However, while this is perfectly reasonable for a small environment like NChain, the table gets far too large and unwieldy for more complicated environments which have a huge number of states and potential actions.</p>
<p>This is where neural networks can be used in reinforcement learning. Instead of having explicit tables, instead we can train a neural network to predict Q values for each action in a given state. This will be demonstrated using <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras</a> in the next section.</p>
<h1>Reinforcement learning with Keras</h1>
<p>To develop a neural network which can perform Q learning, the input needs to be the current state (plus potentially some other information about the environment) and it needs to output the relevant Q values for each action in that state. The Q values which are output should approach, as training progresses, the values produced in the Q learning updating rule. Therefore, the loss or cost function for the neural network should be:</p>
<p>$$\text{loss} = (\underbrace{r + \gamma \max_{a&#8217;} Q'(s&#8217;, a&#8217;)}_{\text{target}} &#8211; \underbrace{Q(s, a)}_{\text{prediction}})^2$$</p>
<p>The reinforcement learning architecture that we are going to build in Keras is shown below:</p>
<figure style="width: 340px" class="wp-caption aligncenter"><img src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/Reinforcement-learning-Keras.png" alt="Reinforcement learning Python Keras - architecture" width="340" height="335" /><figcaption class="wp-caption-text">Reinforcement learning Keras architecture</figcaption></figure>
<p>The input to the network is the one-hot encoded state vector. For instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0] and state 3 is [0, 0, 0, 1, 0]. In this case, a hidden layer of 10 nodes with sigmoid activation will be used. The output layer is a linear activated set of two nodes, corresponding to the two Q values assigned to each state to represent the two possible actions. Linear activation means that the output depends only on the linear summation of the inputs and the weights, with no additional function applied to that summation. For more on neural networks, check out my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">comprehensive neural network tutorial</a>.</p>
<p>Building this network is easy in Keras &#8211; to learn more about how to use Keras, check out <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">my tutorial</a>. The code below shows how it can be done in a few lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = Sequential()
model.add(InputLayer(batch_input_shape=(1, 5)))
model.add(Dense(10, activation=&#039;sigmoid&#039;))
model.add(Dense(2, activation=&#039;linear&#039;))
model.compile(loss=&#039;mse&#039;, optimizer=&#039;adam&#039;, metrics=[&#039;mae&#039;])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>First, the model is created using the Keras Sequential API. Then an input layer is added which takes inputs corresponding to the one-hot encoded state vectors. Then the sigmoid activated hidden layer with 10 nodes is added, followed by the linear activated output layer which will yield the Q values for each action. Finally the model is compiled using a mean-squared error loss function (to correspond with the loss function defined previously) with the <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam optimizer</a> being used in its default Keras state.</p>
<p>To use this model in the training environment, the following code is run which is similar to the previous $\epsilon$-greedy Q learning methodology with an explicit Q table:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">    # now execute the q learning
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        if i % 100 == 0:
            print(&quot;Episode {} of {}&quot;.format(i + 1, num_episodes))
        done = False
        r_sum = 0
        while not done:
            if np.random.random() &lt; eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            s = new_s
            r_sum += r
        r_avg_list.append(r_sum / 1000)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first major difference in the Keras implementation is the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            if np.random.random() &lt; eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first condition in the <em>if </em>statement is the implementation of the $\epsilon$-greedy action selection policy that has been discussed already. The second condition uses the Keras model to produce the two Q values &#8211; one for each possible state. It does this by calling the model.predict() function. Here the numpy identity function is used, with vector slicing, to produce the one-hot encoding of the current state <em>s</em>. The standard numpy argmax function is used to select the action with the highest Q value returned from the Keras model prediction.</p>
<p>The second major difference is the following four lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line sets the target as the Q learning updating rule that has been previously presented. It is the reward <em>r</em> plus the discounted maximum of the predicted Q values for the new state, <em>new_s. </em>This is the value that we want the Keras model to learn to predict for state <em>s </em>and action <em>a</em> i.e. Q(s,a). However, our Keras model has an output for each of the two actions &#8211; we don&#8217;t want to alter the value for the other action, only the action <em>a </em>which has been chosen. So on the next line, <em>target_vec</em> is created which extracts both predicted Q values for state <em>s</em>. On the following line, only the Q value corresponding to the action <em>a</em> is changed to <em>target &#8211; </em>the other action&#8217;s Q value is left untouched.</p>
<p>The final line is where the Keras model is updated in a single training step. The first argument is the current state &#8211; i.e. the one-hot encoded input to the model. The second is our target vector which is reshaped to make it have the required dimensions of (1, 2). The third argument tells the fit function that we only want to train for a single iteration and finally the <em>verbose</em> flag simply tells Keras not to print out the training progress.</p>
<p>Running this training over 1000 game episodes reveals the following average reward for each step in the game:</p>
<figure id="attachment_811" style="width: 455px" class="wp-caption aligncenter"><img class=" wp-image-811" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training.png" alt="Reinforcement learning Python Keras - training improvement in reward" width="455" height="341" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training.png 640w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/03/RL-Keras-average-reward-improvement-during-training-80x60.png 80w" sizes="(max-width: 455px) 100vw, 455px" /><figcaption class="wp-caption-text">Reinforcement learning in Keras &#8211; average reward improvement over number of episodes trained</figcaption></figure>
<p>As can be observed, the average reward per step in the game increases over each game episode, showing that the Keras model is learning well (if a little slowly).</p>
<p>We can also run the following code to get an output of the Q values for each of the states &#8211; this is basically getting the Keras model to reproduce our explicit Q table that was generated in previous methods:</p>
<p>State 0 &#8211; action [[62.734287 61.350456]]
<p>State 1 &#8211; action [[66.317955 62.27209 ]]
<p>State 2 &#8211; action [[70.82501 63.262383]]
<p>State 3 &#8211; action [[76.63797 64.75874]]
<p>State 4 &#8211; action [[84.51073 66.499725]]
<p>This output looks sensible &#8211; we can see that the Q values for each state will favor choosing action 0 (moving forward) to shoot for those big, repeated rewards in state 4. Intuitively, this seems like the best strategy.</p>
<p>So there you have it &#8211; you should now be able to understand some basic concepts in reinforcement learning, and understand how to build Q learning models in Keras. This is just scraping the surface of reinforcement learning, so stay tuned for future posts on this topic (or check out the recommended course below) where more interesting games are played!</p>
<hr />
<p><strong>Recommended online course </strong>&#8211;<strong> </strong>If you&#8217;re more of a video based learner, I&#8217;d recommend the following inexpensive Udemy online course in reinforcement learning: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1080408&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fartificial-intelligence-reinforcement-learning-in-python%2F" target="new">Artificial Intelligence: Reinforcement Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/">Reinforcement learning tutorial using Python and Keras</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		</item>
		<item>
		<title>Keras LSTM tutorial &#8211; How to easily build a powerful deep learning language model</title>
		<link>http://adventuresinmachinelearning.com/keras-lstm-tutorial/</link>
		<comments>http://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments</comments>
		<pubDate>Sat, 03 Feb 2018 03:30:37 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Keras]]></category>
		<category><![CDATA[LSTMs]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=690</guid>
		<description><![CDATA[<p>In previous posts, I introduced Keras for building convolutional neural networks and performing word embedding. The next natural step is to talk about implementing recurrent <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/" title="Keras LSTM tutorial &#8211; How to easily build a powerful deep learning language model">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/">Keras LSTM tutorial &#8211; How to easily build a powerful deep learning language model</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In previous posts, I introduced Keras for building <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">convolutional neural networks</a> and performing <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">word embedding</a>. The next natural step is to talk about implementing recurrent neural networks in Keras. In a <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">previous tutorial of mine</a>, I gave a very comprehensive introduction to recurrent neural networks and long short term memory (LSTM) networks, implemented in TensorFlow. In this tutorial, I&#8217;ll concentrate on creating LSTM networks in Keras, briefly giving a recap or overview of how LSTMs work. In this Keras LSTM tutorial, we&#8217;ll implement a sequence-to-sequence text prediction model by utilizing a large text data set called the PTB corpus. All the code in this tutorial can be found on this <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">site&#8217;s Github repository</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, I&#8217;d recommend this inexpensive Udemy course to learn more about Keras and LSTM networks: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1140660&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fzero-to-deep-learning%2F" target="new">Zero to Deep Learning with Python and Keras</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1140660&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>A brief introduction to LSTM networks</h1>
<h2>Recurrent neural networks</h2>
<p>A LSTM network is a kind of recurrent neural network. A recurrent neural network is a neural network that attempts to model time or sequence dependent behaviour &#8211; such as language, stock prices, electricity demand and so on. This is performed by feeding back the output of a neural network layer at time <em>t</em> to the input of the same network layer at time <em>t + 1</em>. It looks like this:</p>
<figure id="attachment_537" style="width: 363px" class="wp-caption aligncenter"><img class="size-full wp-image-537" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg" alt="Recurrent LSTM tutorial - RNN diagram with nodes" width="363" height="229" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN.jpg 363w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Explicit-RNN-300x189.jpg 300w" sizes="(max-width: 363px) 100vw, 363px" /><figcaption class="wp-caption-text">Recurrent neural network diagram with nodes shown</figcaption></figure>
<p>Recurrent neural networks are &#8220;unrolled&#8221; programmatically during training and prediction, so we get something like the following:</p>
<figure id="attachment_541" style="width: 619px" class="wp-caption aligncenter"><img class=" wp-image-541" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" alt="Recurrent LSTM tutorial - unrolled RNN" width="619" height="202" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png 772w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-300x98.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network-768x251.png 768w" sizes="(max-width: 619px) 100vw, 619px" /><figcaption class="wp-caption-text">Unrolled recurrent neural network</figcaption></figure>
<p>Here you can see that at each time step, a new word is being supplied &#8211; the output of the previous <em>F</em> (i.e. <em>$h_{t-1}$</em>) is supplied to the network at each time step also. If you&#8217;re wondering what those example words are referring to, it is an example sentence I used in my <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">previous LSTM tutorial in TensorFlow</a>: “A girl walked into a bar, and she said ‘Can I have a drink please?’.  The bartender said ‘Certainly&#8217;”.</p>
<p>The problem with vanilla recurrent neural networks, constructed from regular neural network nodes, is that as we try to model dependencies between words or sequence values that are separated by a significant number of other words, we experience the vanishing gradient problem (and also sometimes  the exploding gradient problem) &#8211; to learn more about the vanishing gradient problem, see <a href="http://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">my post on the topic</a>. This is because small gradients or weights (values less than 1) are multiplied many times over through the multiple time steps, and the gradients shrink asymptotically to zero. This means the weights of those earlier layers won&#8217;t be changed significantly and therefore the network won&#8217;t learn long-term dependencies.</p>
<p>LSTM networks are a way of solving this problem.</p>
<h2>LSTM networks</h2>
<p>As mentioned previously, in this Keras LSTM tutorial we will be building an LSTM network for text prediction. An LSTM network is a recurrent neural network that has LSTM cell blocks in place of our standard neural network layers. These cells have various components called the input gate, the forget gate and the output gate &#8211; these will be explained more fully later. Here is a graphical representation of the LSTM cell:</p>
<figure id="attachment_564" style="width: 600px" class="wp-caption aligncenter"><img class=" wp-image-564" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png" alt="Recurrent neural network LSTM tutorial - LSTM cell diagram" width="600" height="289" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png 669w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram-300x144.png 300w" sizes="(max-width: 600px) 100vw, 600px" /><figcaption class="wp-caption-text">LSTM cell diagram</figcaption></figure>
<p>Notice first, on the left hand side, we have our new word/sequence value <em>$x_t$</em> being concatenated to the previous output from the cell <em>$h_{t-1}$</em>. The first step for this combined input is for it to be squashed via a <em>tanh</em> layer. The second step is that this input is passed through an <em>input gate</em>. An input gate is a layer of sigmoid activated nodes whose output is multiplied by the squashed input. These input gate sigmoids can act to &#8220;kill off&#8221; any elements of the input vector that aren&#8217;t required. A sigmoid function outputs values between 0 and 1, so the weights connecting the input to these nodes can be trained to output values close to zero to &#8220;switch off&#8221; certain input values (or, conversely, outputs close to 1 to &#8220;pass through&#8221; other values).</p>
<p>The next step in the flow of data through this cell is the internal state / forget gate loop. LSTM cells have an internal state variable <em>$s_t$</em>. This variable, lagged one time step i.e. <em>$s_{t-1}$</em> is <em>added</em> to the input data to create an effective layer of recurrence. This <em>addition</em> operation, instead of a multiplication operation, helps to reduce the risk of vanishing gradients. However, this recurrence loop is controlled by a forget gate &#8211; this works the same as the input gate, but instead helps the network learn which state variables should be &#8220;remembered&#8221; or &#8220;forgotten&#8221;.</p>
<p>Finally, we have an output layer <em>tanh</em> squashing function, the output of which is controlled by an <em>output </em><em>gate. </em>This gate determines which values are actually allowed as an output from the cell <em>$h_t$</em>.</p>
<p>The mathematics of the LSTM cell looks like this:</p>
<p><strong>Input</strong></p>
<p>First, the input is squashed between -1 and 1 using a <em>tanh</em> activation function. This can be expressed by:</p>
<p>$$g = tanh(b^g + x_tU^g + h_{t-1}V^g)$$</p>
<p>Where $U^g$ and $V^g$ are the weights for the input and previous cell output, respectively, and $b^g$ is the input bias. Note that the exponents <i>g</i> are not a raised power, but rather signify that these are the input weights and bias values (as opposed to the input gate, forget gate, output gate etc.).</p>
<p>This squashed input is then multiplied element-wise by the output of the <em>input gate, </em>which, as discussed above, is a series of sigmoid activated nodes:</p>
<p>$$i = \sigma(b^i + x_tU^i + h_{t-1}V^i)$$</p>
<p>The output of the input section of the LSTM cell is then given by:</p>
<p>$$g \circ i$$</p>
<p>Where the $\circ$ operator expresses element-wise multiplication.</p>
<p><strong>Forget gate and state loop</strong></p>
<p>The forget gate output is expressed as:</p>
<p>$$f = \sigma(b^f + x_tU^f + h_{t-1}V^f)$$</p>
<p>The output of the element-wise product of the previous state and the forget gate is expressed as $s_{t-1} \circ f$. The output from the forget gate / state loop stage is:</p>
<p>$$s_t = s_{t-1} \circ f + g \circ i$$</p>
<p><strong>Output gate</strong></p>
<p>The output gate is expressed as:</p>
<p>$$o = \sigma(b^o + x_tU^o + h_{t-1}V^o)$$</p>
<p>So the final output of the cell , with the <em>tanh </em>squashing, can be shown as:</p>
<p>$$h_t = tanh(s_t) \circ o$$</p>
<h2>LSTM word embedding and hidden layer size</h2>
<p>It should be remembered that in all of the mathematics above we are dealing with vectors i.e. the input <em>$x_t$</em> and <em>$h_{t-1}$</em> are not single valued scalars, but rather vectors of a certain length. Likewise, all the weights and bias values are matrices and vectors respectively. Now, you may be wondering, how do we represent words to input them to a neural network? The answer is word embedding. I&#8217;ve written about this extensively in previous tutorials, in particular <a href="http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/">Word2Vec word embedding tutorial in Python and TensorFlow</a> and <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/">A Word2Vec Keras tutorial</a>. Basically it involves taking a word and finding a vector representation of that word which captures some meaning of the word. In Word2Vec, this meaning is usually quantified by context &#8211; i.e. word vectors which are close together in vector space are those words which appear in sentences close to the same words.</p>
<p>The word vectors can be learnt separately, as in <a href="http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/" target="_blank" rel="noopener">this tutorial</a>, or they can be learnt during the training of your Keras LSTM network. In the example to follow, we&#8217;ll be setting up what is called an <em>embedding </em>layer, to convert each word into a meaningful word vector. We have to specify the size of the embedding layer &#8211; this is the length of the vector each word is represented by &#8211; this is usually in the region of between 100-500. In other words, if the embedding layer size is 250, each word will be represented by a 250-length vector i.e. [$x_1, x_2, x_3,\ldots, x_{250}$].</p>
<p><strong>LSTM hidden layer size</strong></p>
<p>We usually match up the size of the embedding layer output with the number of hidden layers in the LSTM cell. You might be wondering where the hidden layers in the LSTM cell come from. In my LSTM overview diagram, I simply showed &#8220;data rails&#8221; through which our input data flowed. However, each <em>sigmoid</em>, <em>tanh</em> or <em>hidden state</em> layer in the cell is actually a set of nodes, whose number is equal to the <em>hidden layer </em>size. Therefore each of the &#8220;nodes&#8221; in the LSTM cell is actually a cluster of normal neural network nodes, as in each layer of a <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">densely connected neural network</a>.</p>
<h2>The Keras LSTM architecture</h2>
<p>This section will illustrate what a full LSTM architecture looks like, and show the architecture of the network that we are building in Keras. This will further illuminate some of the ideas expressed above, including the embedding layer and the tensor sizes flowing around the network. The proposed architecture looks like the following:</p>
<p>&nbsp;</p>
<figure id="attachment_747" style="width: 522px" class="wp-caption aligncenter"><img class=" wp-image-747" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture.png" alt="Keras LSTM tutorial architecture" width="522" height="545" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture.png 642w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture-288x300.png 288w" sizes="(max-width: 522px) 100vw, 522px" /><figcaption class="wp-caption-text">Keras LSTM tutorial architecture</figcaption></figure>
<p>The input shape of the text data is ordered as follows : (batch size, number of time steps, hidden size). In other words, for each batch sample and each word in the number of time steps, there is a 500 length embedding word vector to represent the input word. These embedding vectors will be learnt as part of the overall model learning. The input data is then fed into two &#8220;stacked&#8221; layers of LSTM cells (of 500 length hidden size) &#8211; in the diagram above, the LSTM network is shown as unrolled over all the time steps. The output from these unrolled cells is still (batch size, number of time steps, hidden size).</p>
<p>This output data is then passed to a Keras layer called TimeDistributed, which will be explained more fully below. Finally, the output layer has a <em>softmax </em>activation applied to it. This output is compared to the training <em>y</em> data for each batch, and the error and gradient back propagation is performed from there in Keras. The training <em>y </em>data in this case is the input <em>x </em>words advanced one time step &#8211; in other words, at each time step the model is trying to predict the very next word in the sequence. However, it does this at <em>every </em>time step &#8211; hence the output layer has the same number of time steps as the input layer. This will be made more clear later.</p>
<h1>Building the Keras LSTM model</h1>
<p>In this section, each line of code to create the Keras LSTM architecture shown above will be stepped through and discussed. However, I&#8217;ll only briefly discuss the text preprocessing code which mostly uses the code found on the TensorFlow site <a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb" target="_blank" rel="noopener">here</a>. The complete code for this Keras LSTM tutorial can be found at <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">this site&#8217;s Github repository</a> and is called keras_lstm.py. Note, you first have to download the <a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz" target="_blank" rel="noopener">Penn Tree Bank (PTB)</a> dataset which will be used as the training and validation corpus. You&#8217;ll need to change the <em>data_path </em>variable in the Github code to match the location of this downloaded data.</p>
<h2>The text preprocessing code</h2>
<p>In order to get the text data into the right shape for input into the Keras LSTM model, each unique word in the corpus must be assigned a unique integer index. Then the text corpus needs to be re-constituted in order, but rather than text words we have the integer identifiers in order. The three functions which do this in the code are <em>read_words, build_vocab </em>and <em>file_to_word_ids. </em>I won’t go into these functions in detail, but basically, they first split the given text file into separate words and sentence based characters (i.e. end-of-sentence &lt;eos&gt;). Then, each unique word is identified and assigned a unique integer. Finally, the original text file is converted into a list of these unique integers, where each word is substituted with its new integer identifier. This allows the text data to be consumed in the neural network.</p>
<p>The <em>load_data</em> function which I created to run these functions is shown below:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def load_data():
    # get the data paths
    train_path = os.path.join(data_path, &quot;ptb.train.txt&quot;)
    valid_path = os.path.join(data_path, &quot;ptb.valid.txt&quot;)
    test_path = os.path.join(data_path, &quot;ptb.test.txt&quot;)

    # build the complete vocabulary, then convert text data to list of integers
    word_to_id = build_vocab(train_path)
    train_data = file_to_word_ids(train_path, word_to_id)
    valid_data = file_to_word_ids(valid_path, word_to_id)
    test_data = file_to_word_ids(test_path, word_to_id)
    vocabulary = len(word_to_id)
    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))

    print(train_data[:5])
    print(word_to_id)
    print(vocabulary)
    print(&quot; &quot;.join([reversed_dictionary[x] for x in train_data[:10]]))
    return train_data, valid_data, test_data, vocabulary, reversed_dictionary</code></pre> <div class="code-embed-infos"> </div> </div>
<p>To call this function, we can run:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The three outputs from this function are the training data, validation data and test data from the data set, respectively, but with each word represented as an integer in a list. Some information is printed out during the running of <em>load_data()</em>, one of which is <em>print(train_data[:5]) &#8211; </em>this produces the following output:</p>
<blockquote>[9970, 9971, 9972, 9974, 9975]</blockquote>
<p>As you can observe, the training data is comprised of a list of integers, as expected.</p>
<p>Next, the output <em>vocabulary</em> is simply the size of our text corpus. When words are incorporated into the training data, every single unique word is not considered &#8211; rather, in natural language processing, the text data is usually limited to a certain <em>N </em>number of the most common words. In this case <em>N = vocabulary = 10,000</em>.</p>
<p>Finally, <em>reversed_dictionary </em>is a Python dictionary where the key is the unique integer identifier of a word, and the associated value is the word in text. This allows us to work backwards from predicted integer words that our model will produce, and translate them back to real text. For instance, the following code converts the integers in <em>train_data </em>back to text which is then printed: <em>print(&#8221; &#8220;.join([reversed_dictionary[x] for x in train_data[100:110]]))</em>. This code snippet produces:</p>
<blockquote><p>workers exposed to it more than N years ago researchers</p></blockquote>
<p>That&#8217;s about all the explanation required with regard to the text pre-processing, so let&#8217;s progress to setting up the input data generator which will feed samples into our Keras LSTM model.</p>
<h2>Creating the Keras LSTM data generators</h2>
<p>When training neural networks, we generally feed data into them in small batches, called mini-batches or just &#8220;batches&#8221; (for more information on mini-batch gradient descent, see my tutorial <a href="http://adventuresinmachinelearning.com/stochastic-gradient-descent/" target="_blank" rel="noopener">here</a>). Keras has  some handy functions which can extract training data automatically from a pre-supplied Python iterator/generator object and input it to the model. One of these Keras functions is called <em>fit_generator. </em>The first argument to <em>fit_generator</em> is the Python iterator function that we will create, and it will be used to extract batches of data during the training process. This function in Keras will handle all of the data extraction, input into the model, executing gradient steps, logging metrics such as accuracy and executing <em>callbacks</em> (these will be discussed later). The Python iterator function needs to have a form like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">while True:
    #do some things to create a batch of data (x, y)
   yield x, y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this case, I have created a generator class which contains a method which implements such a structure. The initialization of this class looks like:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">class KerasBatchGenerator(object):

    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):
        self.data = data
        self.num_steps = num_steps
        self.batch_size = batch_size
        self.vocabulary = vocabulary
        # this will track the progress of the batches sequentially through the
        # data set - once the data reaches the end of the data set it will reset
        # back to zero
        self.current_idx = 0
        # skip_step is the number of words which will be skipped before the next
        # batch is skimmed from the data set
        self.skip_step = skip_step</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Here the <em>KerasBatchGenerator</em> object takes our data as the first argument. Note, this data can be either training, validation or test data &#8211; multiple instances of the same class can be created and used in the various stages of our machine learning development cycle &#8211; training, validation tuning, test. The next argument supplied is called <em>num_steps &#8211; </em>this is the number of words that we will feed into the time distributed input layer of the network. In other words (pun intended), this is the set of words that the model will learn from to predict the words coming after. The argument <em>batch_size </em>is pretty self-explanatory, and we&#8217;ve discussed <em>vocabulary</em> already (it is equal to 10,000 in this case). Finally <em>skip_steps </em>is the number of words we want to skip over between training samples within each batch. To make this a bit clearer, consider the following sentence:</p>
<p><em>&#8220;The cat sat on the mat, and ate his</em> <em>hat. </em><em>Then he jumped up and spat</em>&#8220;</p>
<p>If <em>num_steps </em>is set to 5, the data consumed as the input data for a given sample would be &#8220;The cat sat on the&#8221;. In this case, because we are predicted the very next word in the sequence via our model, for each time step, the matching output <em>y </em>or target data would be &#8220;cat sat on the mat&#8221;. Finally, the <em>skip_steps</em> is the number of words to skip over before the next data batch is taken. If, in this example, it is <em>skip_steps=num_steps</em> the next 5 input words for the next batch would be &#8220;mat and ate his hat&#8221;. Hopefully that makes sense.</p>
<p>One final item in the initialization of the class needs to be discussed. This is variable <em>current_idx</em> which is initialized at zero. This variable is required to track the extraction of data through the full data set &#8211; once the full data set has been consumed in the training, we need to reset <em>current_idx</em> to zero so that the data consumption starts from the beginning of the data set again. In other words it is basically a data set location pointer.</p>
<p>Ok, now we need to discuss the <em>generator </em>method that will be called during <em>fit_generator</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def generate(self):
    x = np.zeros((self.batch_size, self.num_steps))
    y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))
    while True:
        for i in range(self.batch_size):
            if self.current_idx + self.num_steps &gt;= len(self.data):
                # reset the index back to the start of the data set
                self.current_idx = 0
            x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]
            temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]
            # convert all of temp_y into a one hot representation
            y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)
            self.current_idx += self.skip_step
        yield x, y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first couple of lines our x and y output arrays are created. The size of variable <em>x </em>is fairly straight forward to understand &#8211; it&#8217;s first dimension is the number of samples we specify in the batch. The second dimension is the number of words we are going to base our predictions on. The size of variable <em>y</em> is a little more complicated. First it has the batch size as the first dimension, then it has the number of time steps as the second, as discussed above. However, <em>y</em> has an additional third dimension, equal to the size of our vocabulary, in this case 10,000.</p>
<p>The reason for this is that the output layer of our Keras LSTM network will be a standard <em>softmax </em>layer, which will assign a probability to each of the 10,000 possible words. The one word with the highest probability will be the predicted word &#8211; in other words, the Keras LSTM network will predict one word out of 10,000 possible <em>categories</em>. Therefore, in order to train this network, we need to create a training sample for each word that has a 1 in the location of the <em>true</em> word, and zeros in all the other 9,999 locations. It will look something like this: (0, 0, 0, &#8230;, 1, 0, &#8230;, 0, 0) &#8211; this is called a one-hot representation, or alternatively, a categorical representation. Therefore, for each target word, there needs to be a 10,000 length vector with only one of the elements in this vector set to 1.</p>
<p>Ok, now onto the <em>while True: yield x, y</em> paradigm that was discussed earlier for the generator. In the first line, we enter into a for loop of size <em>batch_size, </em>to populate all the data in the batch. Next, there is a condition to test regarding whether we need to reset the <em>current_idx </em>pointer. Remember that for each training sample we consume <em>num_steps</em> words. Therefore, if the current index point plus <em>num_steps </em>is greater than the length of the data set, then the <em>current_idx</em> pointer needs to be reset to zero to start over with the data set.</p>
<p>After this check is performed, the input data is consumed into the <em>x </em>array. The data indices consumed is pretty straight-forward to understand &#8211; it is the current index to the current-index-plus-<em>nu</em><em>m_steps </em>number of words. Next, a temporary <em>y </em>variable is populated which works in pretty much the same way &#8211; the only difference is that the starting point and the end point of the data consumption is advanced by 1 (i.e. + 1). If this is confusing, please refer to the &#8220;cat sat on the mat etc.&#8221; example discussed above.</p>
<p>The final step is converting each of the target words in each sample into the one-hot or categorical representation that was discussed previously. To do this, you can use the Keras <em>to_categorical </em>function. This function takes a series of integers as its first arguments and adds an additional dimension to the vector of integers &#8211; this dimension is the one-hot representation of each integer. It&#8217;s size is specified by the second argument passed to the function. So say we have a series of integers with a shape (100, 1) and we pass it to the <em>to_categorical </em>function and specify the size to be equal to 10,000 &#8211; the returned shape will be (100, 10000). For instance, let&#8217;s say the series / vector of integers looked like: (0, 1, 2, 3, &#8230;.), the <em>to_categorical</em> output would look like:</p>
<p>(1, 0, 0, 0, 0, &#8230;.)</p>
<p>(0, 1, 0, 0, 0, &#8230;.)</p>
<p>(0, 0, 1, 0, 0, &#8230;.)</p>
<p>and so on&#8230;</p>
<p>Here the &#8220;&#8230;&#8221; represents a whole lot of zeroes ensuring that the total number of elements associated with each integer is 10,000. Hopefully that makes sense.</p>
<p>The final two lines of the generator function are straight-forward &#8211; first, the <em>current_idx</em> pointer is incremented by <em>skip_step</em> whose role was discussed previously. The last line yields the batch of <em>x </em>and <em>y </em>data.</p>
<p>Now that the generator class has been created, we need to create instances of it. As mentioned previously, we can setup instances of the same class to correspond to the training and validation data. In the code, this looks like the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,
                                           skip_step=num_steps)
valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary,
                                           skip_step=num_steps)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now that the input data for our Keras LSTM code is all setup and ready to go, it is time to create the LSTM network itself.</p>
<h2>Creating the Keras LSTM structure</h2>
<p>In this example, the Sequential way of building deep learning networks will be used. This way of building networks was introduced in my <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras tutorial – build a convolutional neural network in 11 lines</a>. The alternate way of building networks in Keras is the Functional API, which I used in my <a href="http://adventuresinmachinelearning.com/word2vec-keras-tutorial/" target="_blank" rel="noopener">Word2Vec Keras tutorial</a>. Basically, the sequential methodology allows you to easily stack layers into your network without worrying too much about all the tensors (and their shapes) flowing through the model. However, you still have to keep your wits about you for some of the more complicated layers, as will be discussed below. In this example, it looks like the following:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = Sequential()
model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))
model.add(LSTM(hidden_size, return_sequences=True))
model.add(LSTM(hidden_size, return_sequences=True))
if use_dropout:
    model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(vocabulary)))
model.add(Activation(&#039;softmax&#039;))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first step involves creating a Keras model with the Sequential() constructor. The first layer in the network, as per the architecture diagram shown previously, is a word embedding layer. This will convert our words (referenced by integers in the data) into meaningful embedding vectors. This Embedding() layer takes the size of the vocabulary as its first argument, then the size of the resultant embedding vector that you want as the next argument. Finally, because this layer is the first layer in the network, we must specify the &#8220;length&#8221; of the input i.e. the number of steps/words in each sample.</p>
<p>It&#8217;s worthwhile keeping track of the Tensor shapes in the network &#8211; in this case, the input to the embedding layer is (batch_size, num_steps) and the output is (batch_size, num_steps, hidden_size). Note that Keras, in the Sequential model, always maintains the batch size as the first dimension. It receives the batch size from the Keras fitting function (i.e. <em>fit_generator</em> in this case), and therefore it is rarely (never?) included in the definitions of the Sequential model layers.</p>
<p>The next layer is the first of our two LSTM layers. To specify an LSTM layer, first you have to provide the number of nodes in the hidden layers within the LSTM cell, e.g. the number of cells in the forget gate layer, the <em>tanh </em>squashing input layer and so on. The next argument that is specified in the code above is the <em>return_sequences=True </em>argument. What this does is ensure that the LSTM cell returns all of the outputs from the unrolled LSTM cell through time. If this argument is left out, the LSTM cell will simply provide the output of the LSTM cell from the last time step. The diagram below shows what I mean:</p>
<figure id="attachment_737" style="width: 647px" class="wp-caption aligncenter"><img class=" wp-image-737" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/01/Keras-LSTM-return-sequences-diagram.png" alt="Keras LSTM tutorial - return sequences argument comparison" width="647" height="174" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/01/Keras-LSTM-return-sequences-diagram.png 1124w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/01/Keras-LSTM-return-sequences-diagram-300x81.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/01/Keras-LSTM-return-sequences-diagram-768x206.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/01/Keras-LSTM-return-sequences-diagram-1024x275.png 1024w" sizes="(max-width: 647px) 100vw, 647px" /><figcaption class="wp-caption-text">Keras LSTM return sequences argument comparison</figcaption></figure>
<p>As can be observed in the diagram above, there is only one output when <em>return_sequences=False</em> &#8211; $<em>h_t$ . </em>However, when <em>return_sequences=True</em> all of the unrolled outputs from the LSTM cells are returned <em>$h_0 &#8230; h_t$</em>. In this case, we want the latter arrangement. Why? Well, in this example we are trying to predict the very next word in the sequence. However, if we are trying to train the model, it is best to be able to compare the LSTM cell output at each time step with the very next word in the sequence &#8211; in this way we get <em>num_steps</em> sources to correct errors in the model (via <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/">back-propagation</a>) rather than just one for each sample.</p>
<p>Therefore, for both stacked LSTM layers, we want to return all the sequences. The output shape of each LSTM layer is (<em>batch_size, num_steps, hidden_size).</em></p>
<p>The next layer in our Keras LSTM network is a dropout layer to prevent overfitting. After that, there is a special Keras layer for use in recurrent neural networks called TimeDistributed. This function adds an independent layer for each time step in the recurrent model. So, for instance, if we have 10 time steps in a model, a TimeDistributed layer operating on a Dense layer would produce 10 independent Dense layers, one for each time step. The activation for these dense layers is set to be softmax in the final layer of our Keras LSTM model.</p>
<h2>Compiling and running the Keras LSTM model</h2>
<p>The next step in Keras, once you&#8217;ve completed your model, is to run the compile command on the model. It looks like this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model.compile(loss=&#039;categorical_crossentropy&#039;, optimizer=&#039;adam&#039;, metrics=[&#039;categorical_accuracy&#039;])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In this command, the type of loss that Keras should use to train the model needs to be specified. In this case, we are using &#8216;categorical_crossentropy&#8217; which is cross entropy applied in cases where there are many classes or categories, of which only one is true. Next, in this example, the optimizer that will be used is the <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam optimizer</a> &#8211; an effective &#8220;all round&#8221; optimizer with adaptive stepping. Finally, a metric is specified &#8211; &#8216;categorical_accuracy&#8217;, which can let us see how the accuracy is improving during training.</p>
<p>The next line of code involves creating a Keras <em>callback </em>&#8211; callbacks are certain functions which Keras can optionally call, usually after the end of a training epoch. For more on callbacks, see my <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras tutorial</a>. The callback that is used in this example is a model checkpoint callback &#8211; this callback saves the model after each epoch, which can be handy for when you are running long-term training.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">checkpointer = ModelCheckpoint(filepath=data_path + &#039;/model-{epoch:02d}.hdf5&#039;, verbose=1)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Note that the model checkpoint function can include the epoch in its naming of the model, which is good for keeping track of things.</p>
<p>The final step in training the Keras LSTM model is to call the aforementioned <em>fit_generator </em>function. The line below shows you how to do this:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,
                        validation_data=valid_data_generator.generate(),
                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first argument to <em>fit_generator</em> is our generator function that was explained earlier. The next argument is the number of iterations to run for each training epoch. The value given <em>len(train_data)//(batch_size*num_steps)</em> ensures that the whole data set is run through the model in each epoch. Likewise, a generator for the smaller validation data set is called, with the same argument for the number of iterations to run. At the end of each epoch, the validation data will be run through the model and the accuracy will be returned. Finally, the model checkpoint callback explained above is supplied via the callbacks argument in <em>fit_generator</em>. Now the model is good to go!</p>
<p>Before some results are presented &#8211; some caveats are required. First the PTB data set is a <em>serious </em>text data set &#8211; not a toy problem to demonstrate how good LSTM models are. Therefore, in order to get good results, you&#8217;ll likely have to run the model over many epochs, and the model will need to have a significant level of complexity. Therefore, it is likely to take a long time on a CPU machine, and I&#8217;d suggest running it on a machine with a good GPU if you want to try and replicate things. If you don&#8217;t have a GPU machine yourself, you can create an Amazon EC2 instance as shown in <a href="http://adventuresinmachinelearning.com/tensorflow-amazon-aws/" target="_blank" rel="noopener">my Amazon AWS tutorial</a>. I&#8217;m in the latter camp, and wasn&#8217;t looking to give <em>too </em>many dollars to Amazon to train, optimize learning parameters and so on. However, I&#8217;ve run the model up to 40 epochs and gotten some reasonable initial results. My model parameters for the results presented below are as follows:</p>
<blockquote><p><em>num_steps=30</em></p>
<p><em>batch_size=20</em></p>
<p><em>hidden_size=500</em></p></blockquote>
<p>After 40 epochs, training data set accuracy was around 40%, while validation set accuracy reached approximately 20-25%. This is the sort of output you&#8217;ll see while running the training session:</p>
<figure id="attachment_749" style="width: 1122px" class="wp-caption alignnone"><img class="size-full wp-image-749" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-training-output.png" alt="Keras LSTM tutorial - example training output" width="1122" height="48" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-training-output.png 1122w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-training-output-300x13.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-training-output-768x33.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-training-output-1024x44.png 1024w" sizes="(max-width: 1122px) 100vw, 1122px" /><figcaption class="wp-caption-text">Keras LSTM tutorial &#8211; example training output</figcaption></figure>
<h2>The Keras LSTM results</h2>
<p>In order to test the trained Keras LSTM model, one can compare the predicted word outputs against what the actual word sequences are in the training and test data set. The code below is a snippet of how to do this, where the comparison is against the predicted model output and the <em>training </em>data set (the same can be done with the <em>test_data </em>data).</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">model = load_model(data_path + &quot;\model-40.hdf5&quot;)
dummy_iters = 40
example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,
                                                     skip_step=1)
print(&quot;Training data:&quot;)
for i in range(dummy_iters):
    dummy = next(example_training_generator.generate())
num_predict = 10
true_print_out = &quot;Actual words: &quot;
pred_print_out = &quot;Predicted words: &quot;
for i in range(num_predict):
    data = next(example_training_generator.generate())
    prediction = model.predict(data[0])
    predict_word = np.argmax(prediction[:, num_steps-1, :])
    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + &quot; &quot;
    pred_print_out += reversed_dictionary[predict_word] + &quot; &quot;
print(true_print_out)
print(pred_print_out)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the code above, first the model is reloaded from the trained data (in the example above, it is the checkpoint from the 40th epoch of training). Then another KerasBatchGenerator class is created, as was discussed previously &#8211; in this case, a batch of length 1 is used, as we only want one <em>num_steps</em> worth of text data to compare. Then a loop of dummy data extractions from the generator is created &#8211; this is to control where in the data-set the comparison sentences are drawn from. The second loop, from 0 to <em>num_predict </em>is where the interesting stuff is happening.</p>
<p>First, a batch of data is extracted from the generator and this is passed to the <em>model.predict() </em>method. This returns <em>num_steps </em>worth of predicted words &#8211; however, each word is represented by a <em>categorical</em> or one hot output. In other words, each word is represented by a vector of 10,000 items, with most being zero and only one element being equal to 1. The index of this &#8220;1&#8221; is the integer representation of the actual English word. So to extract the index where this &#8220;1&#8221; occurs, we can use the<em> np.argmax() </em>function. This function identifies the index where the maximum value occurs in a vector &#8211; in this case the maximum value is 1, compared to all the zeros, so this is a handy function for us to use.</p>
<p>Once the index has been identified, it can be translated into an actual English word by using the <em>reverse_dictionary </em>that was constructed during the data pre-processing. This English word is then added to the predicted words string, and finally the actual and predicted words are returned.</p>
<p>The output below is the comparison between the actual and predicted words after 10 epochs of training on the <em>training</em> data set:</p>
<figure id="attachment_743" style="width: 885px" class="wp-caption aligncenter"><img class="size-full wp-image-743" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-10-epochs-training.png" alt="Keras LSTM tutorial - comparison on the training data set after 10 epochs" width="885" height="50" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-10-epochs-training.png 885w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-10-epochs-training-300x17.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-10-epochs-training-768x43.png 768w" sizes="(max-width: 885px) 100vw, 885px" /><figcaption class="wp-caption-text">Comparison on the training data set after 10 epochs of training</figcaption></figure>
<p>As can be observed, while some words match, after 10 epochs of training the match is pretty poor. By the way &#8220;&lt;unk&gt;&#8221; refers to words not included in the 10,000 length vocabulary of the data set. Alternatively, if we look at the comparison after 40 epochs of training (again, just on the <em>training </em>data set):</p>
<figure id="attachment_744" style="width: 869px" class="wp-caption aligncenter"><img class="size-full wp-image-744" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training.png" alt="Keras LSTM tutorial - comparison on the training data set after 40 epochs" width="869" height="57" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training.png 869w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training-300x20.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training-768x50.png 768w" sizes="(max-width: 869px) 100vw, 869px" /><figcaption class="wp-caption-text">Comparison on the training data set after 40 epochs of training</figcaption></figure>
<p>It can be observed that the match is quite good between the actual and predicted words in the <em>training</em> set.</p>
<p>However, when we look at the test data set, the match after 40 epochs of training isn&#8217;t quite as good:</p>
<figure id="attachment_745" style="width: 687px" class="wp-caption aligncenter"><img class=" wp-image-745" src="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training-test-set.png" alt="Keras LSTM tutorial - comparison on the test data set after 40 epochs" width="687" height="53" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training-test-set.png 749w, http://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-40-epochs-training-test-set-300x23.png 300w" sizes="(max-width: 687px) 100vw, 687px" /><figcaption class="wp-caption-text">Comparison on the test data set after 40 epochs of training</figcaption></figure>
<p>Despite there not being a perfect correspondence between the predicted and actual words, you can see that there is a rough correspondence and the predicted sub-sentence at least makes some grammatical sense. So not so bad after all. However, in order to train a Keras LSTM network which can perform well on this realistic, large text corpus, more training and optimization is required. I will leave it up to you, the reader, to experiment further if you desire. However, the current code is sufficient for you to gain an understanding of how to build a Keras LSTM network, along with an understanding of the theory behind LSTM networks.</p>
<p>I hope this (large) tutorial is a help to you in understanding Keras LSTM networks, and LSTM networks in general.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, I&#8217;d recommend this inexpensive Udemy course to learn more about Keras and LSTM networks: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1140660&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fzero-to-deep-learning%2F" target="new">Zero to Deep Learning with Python and Keras</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1140660&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/keras-lstm-tutorial/">Keras LSTM tutorial &#8211; How to easily build a powerful deep learning language model</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/keras-lstm-tutorial/feed/</wfw:commentRss>
		<slash:comments>10</slash:comments>
		</item>
		<item>
		<title>How to create a TensorFlow deep learning powerhouse on Amazon AWS</title>
		<link>http://adventuresinmachinelearning.com/tensorflow-amazon-aws/</link>
		<comments>http://adventuresinmachinelearning.com/tensorflow-amazon-aws/#respond</comments>
		<pubDate>Sat, 18 Nov 2017 09:53:10 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Amazon AWS]]></category>
		<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[GPUs]]></category>
		<category><![CDATA[Recurrent neural networks]]></category>
		<category><![CDATA[TensorFlow]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=703</guid>
		<description><![CDATA[<p>In my previous tutorial on recurrent neural networks and LSTM networks in TensorFlow, we weren&#8217;t able to get fantastic results. This is because I was <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/tensorflow-amazon-aws/" title="How to create a TensorFlow deep learning powerhouse on Amazon AWS">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-amazon-aws/">How to create a TensorFlow deep learning powerhouse on Amazon AWS</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>In my previous tutorial on recurrent neural networks and LSTM networks in TensorFlow, we weren&#8217;t able to get fantastic results. This is because I was running the code on my little ol&#8217; laptop CPU &#8211; not exactly the ideal setup for big deep learning networks. So what to do? I could fork out thousands on a specced up desktop with <a href="https://www.nvidia.com/en-us/deep-learning-ai/developer/" target="_blank" rel="noopener">NVIDIA GPUs</a>, but, you know, I have a family and bills to pay. So the best option, I think, is to hire out some GPUs on Amazon AWS. That&#8217;s just what I did, and I&#8217;m going to give you a how-to guide below on how to do it. Then I&#8217;m going to run the <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">sequence-to-sequence LSTM model</a> that I created in TensorFlow, and show you the improvements. So let&#8217;s get to it.</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, checkout the following highly rated and inexpensive Udemy course, which covers deep learning concepts and how to deploy on Amazon AWS too: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.772462&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdata-science-deep-learning-in-theano-tensorflow%2F" target="new">Modern Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.772462&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>Step 1 &#8211; Setup an Amazon AWS account and load up an instance</h1>
<p>The first thing to do is to head over to <a href="https://aws.amazon.com/" target="_blank" rel="noopener">Amazon AWS</a> and create an account. You&#8217;ll need to supply some credit card details, as the computing power isn&#8217;t free &#8211; but we&#8217;ll be using a cheap option here, so it shouldn&#8217;t cost you too much if you want to follow along (a few dollars). At this stage, you may have to request, via Amazon AWS support, for them to free up an EC2 instance for you in your region. To do this, log into your Amazon AWS account and go to the dashboard. At the top of the window you&#8217;ll see a &#8220;Services&#8221; drop down &#8211; click this and select the Support link on the left hand side. Once you&#8217;ve clicked this, on the next page click &#8220;Create Case&#8221;, again on the left hand side menu.</p>
<p>On this page, next to the heading &#8220;Regarding&#8221;, select &#8220;Service Limit Increase&#8221;. Then, under &#8220;Limit Type&#8221; select &#8220;EC2 Instances&#8221;. Select your closest region, and under &#8220;Primary Instance Type&#8221; select &#8220;p2.xlarge&#8221;. Leave the &#8220;Limit&#8221; field as &#8220;Instance Limit&#8221;, and put a &#8220;1&#8221; in the field &#8220;New limit value&#8221;. Put in a use case description i.e. &#8220;Deep learning computing&#8221; then submit the case. Amazon AWS will then free up an instance for you to use, which might take a little while for them to do. If the terms above like &#8220;EC2 instance&#8221; and &#8220;p2.xlarge&#8221; don&#8217;t make sense at this stage, don&#8217;t worry &#8211; they are explained more fully later.</p>
<p>Once you&#8217;re done that, head over to <a href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB?qid=1510042228622&amp;sr=0-3&amp;ref_=srh_res_product_title" target="_blank" rel="noopener">this link</a>. This page (see below) details a specifically setup Amazon Machine Instance (AMI) with all your favorite deep learning packages already loaded up &#8211; TensorFlow, Keras, PyTorch, CNTK, MXNet and more.</p>
<figure id="attachment_704" style="width: 1526px" class="wp-caption alignnone"><img class="size-full wp-image-704" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1.png" alt="Amazon AWS TensorFlow how-to: AMI selection" width="1526" height="798" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1.png 1526w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-300x157.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-768x402.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-1024x535.png 1024w" sizes="(max-width: 1526px) 100vw, 1526px" /><figcaption class="wp-caption-text">Amazon AWS &#8211; deep learning AMI selection</figcaption></figure>
<p>Scroll down and check out the EC2 instances available and the hourly prices on the right hand side. The EC2 instances are scale-able cloud computing services offered by Amazon AWS, and there are lots of different machine arrangements to choose from. In this case, we want to choose an AMI with at least 1 NVIDIA GPU. To do that, select your appropriate region on the right hand side and then hit the continue button.</p>
<p>You&#8217;ll then be taken to a launch page that looks like:</p>
<figure id="attachment_705" style="width: 1411px" class="wp-caption alignnone"><img class="size-full wp-image-705" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-instance-selection.png" alt="Amazon AWS TensorFlow - AMI instance selection" width="1411" height="805" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-instance-selection.png 1411w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-instance-selection-300x171.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-instance-selection-768x438.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-instance-selection-1024x584.png 1024w" sizes="(max-width: 1411px) 100vw, 1411px" /><figcaption class="wp-caption-text">AMI instance selection</figcaption></figure>
<p>Let&#8217;s go with the &#8220;1-Click Launch&#8221; option to make things nice and easy. Then, I&#8217;d suggest selecting the p2.xlarge EC2 instance under the &#8220;EC2 Instance Type&#8221; pane. This gives us 1 NVIDIA K80 GPU to play with. At the time of writing, this instance costs $1.54 / hour for an Asia Pacific (Sydney) deploy. Not too bad.</p>
<p>If you&#8217;re like me and haven&#8217;t done this before, scroll down to the bottom of the page and you&#8217;ll find this box:</p>
<figure id="attachment_706" style="width: 509px" class="wp-caption aligncenter"><img class=" wp-image-706" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-key-pair.png" alt="Amazon AWS TensorFlow - key pair creation" width="509" height="188" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-key-pair.png 850w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-key-pair-300x111.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-key-pair-768x284.png 768w" sizes="(max-width: 509px) 100vw, 509px" /><figcaption class="wp-caption-text">Key pair creation</figcaption></figure>
<p>Expand the Key Pair pane and follow the instructions &#8211; this Key Pair is a security measure that is required to perform the necessary secure encryption when you logon to your instance. Once you&#8217;ve done that, refresh the page again, and make sure that your region matches if you had to change it. Once you match the region correctly with your Key Pair, the &#8220;Launch with 1-click&#8221; button will become enabled, as shown below. Click this, and your instance will be created after a few minutes.</p>
<figure id="attachment_707" style="width: 327px" class="wp-caption aligncenter"><img class=" wp-image-707" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-launch-button-enabled.png" alt="Amazon AWS TensorFlow - launch button enabled" width="327" height="245" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-launch-button-enabled.png 586w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-launch-button-enabled-300x225.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-launch-button-enabled-326x245.png 326w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-launch-button-enabled-80x60.png 80w" sizes="(max-width: 327px) 100vw, 327px" /><figcaption class="wp-caption-text">Launch button enabled</figcaption></figure>
<p>Once you&#8217;ve hit the button above, you can go back to your <a href="http://console.aws.amazon.com/" target="_blank" rel="noopener">Amazon AWS dashboard</a>. Search or select the &#8220;EC2&#8221; service (under the &#8220;Compute&#8221; heading) in the AWS Services. This will take you to your EC2 dashboard, it should look something like this:</p>
<figure id="attachment_710" style="width: 670px" class="wp-caption aligncenter"><img class=" wp-image-710" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-EC2-console.png" alt="Amazon AWS TensorFlow - EC2 console" width="670" height="319" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-EC2-console.png 1469w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-EC2-console-300x143.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-EC2-console-768x366.png 768w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Step-1-EC2-console-1024x488.png 1024w" sizes="(max-width: 670px) 100vw, 670px" /><figcaption class="wp-caption-text">Amazon AWS EC2 console</figcaption></figure>
<p>Note that under the Resources heading, there should be &#8220;1 Running Instances&#8221; showing &#8211; this is your instance. To access your running AMI, on the left hand side select &#8220;Instances&#8221;. You&#8217;ll then see your p2.xlarge instance up and running on the main pane. Select the button &#8220;Connect&#8221;. You&#8217;ll be presented with a pop-up window &#8220;Connect To Your Instance&#8221; &#8211; select either option. I&#8217;m using &#8220;A standalone SSH client&#8221; (PuTTY on Windows) &#8211; but you can choose whichever method you like to connect. Just follow the instruction Amazon AWS gives you to setup.</p>
<p>If you&#8217;re using PuTTY, there is one final step to allow you to properly use a Linux text manager and terminal multiplexer called Byobu. In your PuTTY program, before you connect, go to the settings menu on the left hand side. Under Connections &#8211; Data, in the field &#8220;Terminal-type string&#8221; enter &#8220;putty-256color&#8221;. This allows you to hit Ctrl-F2 in Windows to create multiple screens in Linux, which will let us monitor our GPU performance while training &#8211; this will be discussed later.</p>
<p>Once you&#8217;ve done that &#8211; you&#8217;re all connected! You should see a command prompt that looks like:</p>
<figure id="attachment_712" style="width: 966px" class="wp-caption alignnone"><img class="size-full wp-image-712" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Remote-console.png" alt="Amazon AWS TensorFlow - remote Linux console" width="966" height="356" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Remote-console.png 966w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Remote-console-300x111.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Amazon-AWS-Remote-console-768x283.png 768w" sizes="(max-width: 966px) 100vw, 966px" /><figcaption class="wp-caption-text">AMI remote Linux console</figcaption></figure>
<h1>Step 2 &#8211; Exploring the instance and loading up the code</h1>
<p>The first thing you want to do when you have your instance running is update all the packages &#8211; you do this by running:</p>
<blockquote><p>sudo yum upgrade</p></blockquote>
<p>Next, let&#8217;s clone the Adventures in Machine Learning <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">github repo</a> by executing the following:</p>
<blockquote><p>git clone https://github.com/adventuresinML/adventures-in-ml-code</p></blockquote>
<p>Let&#8217;s also install a Python package called <em>gpustat </em>that we will use to monitor how our Nvidia GPU on the Amazon AWS instance is going as we train our recurrent neural network. Run:</p>
<blockquote><p>pip install gpustat</p></blockquote>
<p>Ok, so we&#8217;re not too far off being able to run the code using the GPU. However, first we&#8217;ll want to be able to monitor the GPU as we train. To do this on a Linux machine we need two screens, and we can use the package mentioned earlier called byobu to do this. To install it, we first need to go back to the root or administration privilege of our instance. Run this:</p>
<blockquote><p><span class="pln">sudo su </span><span class="pun">&#8211; </span></p></blockquote>
<p>Then to install byobu run this:</p>
<blockquote><p>yum install byobu</p></blockquote>
<p>Ok &#8211; now you can run byobu by simply typing &#8220;byobu&#8221; at the command prompt. To open up a new window, press Ctrl-F2. You&#8217;ll see this opens a new screen in your Linux session. To switch between the screens, press Ctrl-F3 and Ctrl-F4. Now, on one screen, we want to run the following to start a background monitoring process (which speeds up our gpustat package):</p>
<blockquote><p>sudo nvidia-smi daemon</p></blockquote>
<p>Then, on the same screen let&#8217;s setup our gpustat watch function, which will give us data about the GPU usage:</p>
<blockquote><p>watch -n1.0 gpustat -cp</p></blockquote>
<p>You should now see a utility printout with the GPU temperature, percentage usage and memory stats (see below for an example when we are actually running the code).</p>
<p>Now switch back to the other screen, using either Ctrl-F3 or Ctrl-F4.</p>
<h1>Step 3 &#8211; Download the data and run</h1>
<p>One final thing remains before we run the code &#8211; we first have to download the training data onto our instance. In the <a href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">TensorFlow recurrent neural network tutorial</a> we used a text data-set from the following link: http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz. You&#8217;ll need to download and extract this tarfile &#8211; to do this run the following:</p>
<blockquote><p>curl http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz | tar xvz</p></blockquote>
<p>You can navigate around this extracted file / folder by using the Linux commands ls (to list the contents of the current path) and cd (for change directory). You need to find the path to simple-examples/data/ &#8211; this is where our training data files are located. Once you&#8217;ve done this, we can finally run the following command to start training the LSTM network created in the aforementioned tutorial:</p>
<blockquote><p>python lstm_tutorial.py 1 &#8211;data_path /home/ec2-user/data/simple-examples/data/</p></blockquote>
<p>Once you run the above command, the program will start and, after it prints out some text data it will begin to train the network (note the dash before &#8220;data_path&#8221; is actually a double dash: &#8220;&#8211;&#8220;). After every 50 iterations, you can observe the loss, the accuracy on the training set and the average time it took to execute each iteration. You&#8217;ll see something like this:</p>
<figure id="attachment_722" style="width: 663px" class="wp-caption aligncenter"><img class=" wp-image-722" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Example-output-with-step-timing-Amazon-AWS-GPU.png" alt="Amazon AWS TensorFlow - GPU training times" width="663" height="149" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Example-output-with-step-timing-Amazon-AWS-GPU.png 909w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Example-output-with-step-timing-Amazon-AWS-GPU-300x67.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/Example-output-with-step-timing-Amazon-AWS-GPU-768x172.png 768w" sizes="(max-width: 663px) 100vw, 663px" /><figcaption class="wp-caption-text">Example output with GPU training times</figcaption></figure>
<p>As you can observe, each iteration takes an average of 0.14 seconds to execute. I&#8217;ve also run this oj my own Intel i5 CPUs and the average iteration time is around 3 seconds &#8211; so we get a greater than 20 times increase in performance with a single Amazon AWS Nvidia GPU. Not bad!</p>
<p>While it&#8217;s training, let&#8217;s take a look at what our GPU doing &#8211; hit Ctrl-F3 or Ctrl-F4 and you&#8217;ll return to your gpustat watch print-out. It should look something like this:</p>
<figure id="attachment_724" style="width: 608px" class="wp-caption aligncenter"><img class=" wp-image-724" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/GPU_status_during_run.png" alt="Amazon AWS TensorFlow - GPU status" width="608" height="51" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/GPU_status_during_run.png 926w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/GPU_status_during_run-300x25.png 300w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/11/GPU_status_during_run-768x65.png 768w" sizes="(max-width: 608px) 100vw, 608px" /><figcaption class="wp-caption-text">GPU status while training the LSTM network</figcaption></figure>
<p>So here we can see that the GPU is running close to maximum capacity &#8211; 81%. Good to see</p>
<p>WARNING: Remember, you have to shut down your instance on your EC2 console on Amazon AWS when you are complete. It&#8217;s not enough to shut down your PuTTY session or similar &#8211; you have to go an shut down your instance on the AWS dashboard. If you don&#8217;t you&#8217;ll be getting charged per hour with the instance sitting there doing nothing!</p>
<p>I hope that&#8217;s been helpful and will let you get your own Amazon AWS deep learning instance up and running. Enjoy your faster model training!</p>
<hr />
<p><strong>Recommended online course: </strong>If you are more of a video course learner, checkout the following highly rated and inexpensive Udemy course, which covers deep learning concepts and how to deploy on Amazon AWS too: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.772462&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fdata-science-deep-learning-in-theano-tensorflow%2F" target="new">Modern Deep Learning in Python</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.772462&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/tensorflow-amazon-aws/">How to create a TensorFlow deep learning powerhouse on Amazon AWS</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/tensorflow-amazon-aws/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>A PyTorch tutorial &#8211; deep learning in Python</title>
		<link>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/</link>
		<comments>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/#respond</comments>
		<pubDate>Thu, 26 Oct 2017 08:46:48 +0000</pubDate>
		<dc:creator><![CDATA[Andy]]></dc:creator>
				<category><![CDATA[Deep learning]]></category>
		<category><![CDATA[Neural networks]]></category>
		<category><![CDATA[PyTorch]]></category>

		<guid isPermaLink="false">http://adventuresinmachinelearning.com/?p=618</guid>
		<description><![CDATA[<p>So &#8211; if you&#8217;re a follower of this blog and you&#8217;ve been trying out your own deep learning networks in TensorFlow and Keras, you&#8217;ve probably <a class="mh-excerpt-more" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" title="A PyTorch tutorial &#8211; deep learning in Python">[...]</a></p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/">A PyTorch tutorial &#8211; deep learning in Python</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>So &#8211; if you&#8217;re a follower of this blog and you&#8217;ve been trying out your own deep learning networks in <a href="http://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">TensorFlow</a> and <a href="http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/" target="_blank" rel="noopener">Keras</a>, you&#8217;ve probably come across the somewhat frustrating business of debugging these deep learning libraries. Sure, they have Python APIs, but it&#8217;s kinda hard to figure out what exactly is happening when something goes wrong. They also don&#8217;t seem to play well with Python libraries such as numpy, scipy, scikit-learn, Cython and so on. Enter the <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> deep learning library &#8211; one of it&#8217;s <a href="http://pytorch.org/about/" target="_blank" rel="noopener">purported benefits</a> is that is a deep learning library that is more at home in Python, which, for a Python aficionado like myself, sounds great. It also has nifty features such as dynamic computational graph construction as opposed to the static computational graphs present in TensorFlow and Keras (for more on computational graphs, see below). It&#8217;s also on the up and up, with its development supported by companies such as Facebook, Twitter, NVIDIA and so on. So let&#8217;s dive into it in this PyTorch tutorial.</p>
<p>The first question to consider &#8211; is it better than TensorFlow? That&#8217;s a fairly subjective judgement &#8211; performance-wise there doesn&#8217;t appear to be a great deal of difference. Check out <a href="https://www.forbes.com/sites/quora/2017/07/10/is-pytorch-better-than-tensorflow/" target="_blank" rel="noopener">this article</a> for a quick comparison. In any case, its clear the PyTorch is here to stay and is likely to be a real contender in the &#8220;contest&#8221; between deep learning libraries, so let&#8217;s kick start our learning of it. I&#8217;ll leave it to you to decide which is &#8220;better&#8221;.</p>
<p>In this PyTorch tutorial we will introduce some of the core features of PyTorch, and build a fairly simple densely connected neural network to classify hand-written digits. To learn how to build more complex models in PyTorch, check out my post <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/" target="_blank" rel="noopener">Convolutional Neural Networks Tutorial in PyTorch</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video course learner, check out this inexpensive, highly rated, Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F" target="new">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<h1>A PyTorch tutorial &#8211; the basics</h1>
<p>In this section, we&#8217;ll go through the basic ideas of PyTorch starting at tensors and computational graphs and finishing at the Variable class and the PyTorch autograd functionality.</p>
<h2>Installing on Windows</h2>
<p>For starters, if you are a Windows user like myself, you&#8217;ll find that there is no straight-forward installation options for that operating system on the <a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch website</a>. However, there is a successful way to do it, check out <a href="https://www.superdatascience.com/pytorch/">this website</a> for instructions. It&#8217;s well worth the effort to get this library installed if you are a Windows user like myself.</p>
<h2>Computational graphs</h2>
<p>The first thing to understand about any deep learning library is the idea of a computational graph. A computational graph is a set of calculations, which are called <em>nodes</em>, and these nodes are connected in a directional ordering of computation. In other words, some nodes are dependent on other nodes for their input, and these nodes in turn output the results of their calculations to other nodes. A simple example of a computational graph for the calculation $a = (b + c) * (c + 2)$ can be seen below &#8211; we can break this calculation up into the following steps/nodes:</p>
<p>\begin{align}<br />
d &amp;= b + c \\<br />
e &amp;= c + 2 \\<br />
a &amp;= d * e<br />
\end{align}</p>
<figure id="attachment_158" style="width: 220px" class="wp-caption aligncenter"><img class=" wp-image-158" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png" alt="PyTorch tutorial - simple computational graph" width="220" height="253" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png 296w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example-260x300.png 260w" sizes="(max-width: 220px) 100vw, 220px" /><figcaption class="wp-caption-text">Simple computational graph</figcaption></figure>
<p>&nbsp;</p>
<p>The benefits of using a computational graph is that each node is like its own independently functioning piece of code (once it receives all its required inputs). This allows various performance optimizations to be performed in running the calculations such as threading and multiple processing / parallelism. All the major deep learning frameworks (TensorFlow, Theano, PyTorch etc.) involve constructing such computational graphs, through which neural network operations can be built and through which gradients can be back-propagated (if you&#8217;re unfamiliar with back-propagation, see my <a href="http://adventuresinmachinelearning.com/neural-networks-tutorial/" target="_blank" rel="noopener">neural networks tutorial</a>).</p>
<h2>Tensors</h2>
<p>Tensors are matrix-like data structures which are essential components in deep learning libraries and efficient computation. Graphical Processing Units (GPUs) are especially effective at calculating operations between tensors, and this has spurred the surge in deep learning capability in recent times. In PyTorch, tensors can be declared simply in a number of ways:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import torch
x = torch.Tensor(2, 3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This code creates a tensor of size (2, 3) &#8211; i.e. 2 rows and 3 columns, filled with zero float values i.e:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 0  0  0
 0  0  0
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>We can also create tensors filled random float values:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = torch.rand(2, 3)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Multiplying tensors, adding them and so forth is straight-forward:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = torch.ones(2,3)
y = torch.ones(2,3) * 2
x + y</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This returns:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 3  3  3
 3  3  3
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Another great thing is the numpy slice functionality that is available &#8211; for instance y[:, 1]
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">y[:,1] = y[:,1] + 1</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This returns:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code"> 2  3  2
 2  3  2
[torch.FloatTensor of size 2x3]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>Now you know how to create tensors and manipulate them in PyTorch, in the next step of this PyTorch tutorial let&#8217;s look at something a bit more complicated.</p>
<h2>Autograd in PyTorch</h2>
<p>In any deep learning library, there needs to be a mechanism where error gradients are calculated and back-propagated through the computational graph. This mechanism, called autograd in PyTorch, is easily accessible and intuitive. The Variable class is the main component of this autograd system in PyTorch. This Variable class wraps a tensor, and allows automatic gradient computation on the tensor when the .backward() function is called (more on this later). The object contains the data of the tensor, the gradient of the tensor (once computed with respect to some other value i.e. the loss) and also contains a reference to whatever function created the variable (if it is a user created function, this reference will be null).</p>
<p>Let&#8217;s create a Variable from a simple tensor:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">x = Variable(torch.ones(2, 2) * 2, requires_grad=True)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the Variable declaration above, we pass in a tensor of (2, 2) 2-values and we specify that this variable requires a gradient. If we were using this in a neural network, this would mean that this Variable would be trainable. If we set this flag to False, the Variable would not be trained. For this simple example we aren&#8217;t training anything, but we do want to interrogate the gradient for this Variable as will be shown below.</p>
<p>Next, let&#8217;s create another Variable, constructed based on operations on our original Variable <em>x</em>.</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">z = 2 * (x * x) + 5 * x</code></pre> <div class="code-embed-infos"> </div> </div>
<p>To get the gradient of this operation with respect to <em>x</em> i.e. <em>dz/dx</em> we can analytically calculate this to by 4x +5. If all elements of <em>x</em> are 2, then we should expect the gradient <em>dz/dx</em> to be a (2, 2) shaped tensor with 13-values. However, first we have to run the .backwards() operation to compute these gradients. Of course, to compute gradients, we need to compute them with respect to something. In this case, we can supply a (2,2) tensor of 1-values to be what we compute the gradients against &#8211; so the calculation simply becomes <em>d/dx</em>:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">z.backward(torch.ones(2, 2))
print(x.grad)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This produces the following output:</p>
<div class="code-embed-wrapper"> <pre class="language-markdown code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-markdown code-embed-code">Variable containing:
 13  13
 13  13
[torch.FloatTensor of size 2x2]</code></pre> <div class="code-embed-infos"> </div> </div>
<p>As you can observe, the gradient is equal to a (2, 2), 13-valued tensor as we predicted. Note that the gradient is stored in the <em>x </em>Variable, in the property .grad.</p>
<p>Now that we&#8217;ve covered the basics of tensors, Variables and the autograd functionality within PyTorch, we can move onto creating a simple neural network in PyTorch which will showcase this functionality further.</p>
<h1>Creating a neural network in PyTorch</h1>
<p>This section is the main show of this PyTorch tutorial. To access the code for this tutorial, check out this website&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>. Here we will create a simple 4-layer  fully connected neural network (including an &#8220;input layer&#8221; and two hidden layers) to classify the hand-written digits of the MNIST dataset. The architecture we&#8217;ll use can be seen in the figure below:</p>
<figure id="attachment_439" style="width: 413px" class="wp-caption aligncenter"><img class="size-full wp-image-439" src="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture.jpg" alt="PyTorch tutorial - fully connected neural network example architecture" width="413" height="334" srcset="http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture.jpg 413w, http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/CNTK-Dense-example-architecture-300x243.jpg 300w" sizes="(max-width: 413px) 100vw, 413px" /><figcaption class="wp-caption-text">Fully connected neural network example architecture</figcaption></figure>
<p>The input layer consists of 28 x 28 (=784) greyscale pixels which constitute the input data of the MNIST data set. This input is then passed through two fully connected hidden layers, each with 200 nodes, with the nodes utilizing a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener">ReLU</a> activation function. Finally, we have an output layer with ten nodes corresponding to the 10 possible classes of hand-written digits (i.e. 0 to 9). We will use a <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax</a> output layer to perform this classification.</p>
<p>Let&#8217;s create the neural network.</p>
<h2>The neural network class</h2>
<p>In order to create a neural network in PyTorch, you need to use the included class nn.Module. To use this base class, we also need to use Python <a href="https://docs.python.org/2/tutorial/classes.html" target="_blank" rel="noopener">class inheritance</a> &#8211; this basically allows us to use all of the functionality of the nn.Module base class, but still have overwriting capabilities of the base class for the model construction / forward pass through the network. Some actual code will help explain:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 10)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the class definition, you can see the inheritance of the base class nn.Module. Then, in the first line of the class initialization (def __init__(self):) we have the required Python super() function, which creates an instance of the base nn.Module class. The following three lines is where we create our fully connected layers as per the architecture diagram. A fully connected neural network layer is represented by the nn.Linear object, with the first argument in the definition being the number of nodes in layer <em>l</em> and the next argument being the number of nodes in layer <em>l+1</em>. As you can observer, the first layer takes the 28 x 28 input pixels and connects to the first 200 node hidden layer. Then we have another 200 to 200 hidden layer, and finally a connection between the last hidden layer and the output layer (with 10 nodes).</p>
<p>Now we&#8217;ve setup the &#8220;skeleton&#8221; of our network architecture, we have to define how data flows through out network. We do this by defining a <em>forward</em>() method in our class &#8211; this method overwrites a dummy method in the base class, and needs to be defined for each network:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">def forward(self, x):
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return F.log_softmax(x)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>For the <em>forward()</em> method, we supply the input data <em>x</em> as the primary argument. We feed this into our first fully connected layer <em>(self.fc1(x))</em> and then apply a ReLU activation to the nodes in this layer using <em>F.relu()</em>. Because of the hierarchical nature of this network, we replace <em>x</em> at each stage, feeding it into the next layer. We do this through our three fully connected layers, except for the last one &#8211; instead of a ReLU activation we return a log softmax &#8220;activation&#8221;. This, combined with the negative log likelihood loss function which will be defined later, gives us a multi-class cross entropy based loss function which we will use to train the network.</p>
<p>So that&#8217;s it &#8211; we&#8217;ve defined our neural network. Pretty easy right?</p>
<p>The next step is to create an instance of this network architecture:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">net = Net()
print(net)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>When we print the instance of the class Net, we get the following output:</p>
<blockquote><p>Net (<br />
(fc1): Linear (784 -&gt; 200)<br />
(fc2): Linear (200 -&gt; 200)<br />
(fc3): Linear (200 -&gt; 10)<br />
)</p></blockquote>
<p>This is pretty handy as it confirms the structure of our network for us.</p>
<h2>Training the network</h2>
<p>Next we have to setup an optimizer and a loss criterion:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># create a stochastic gradient descent optimizer
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)
# create a loss function
criterion = nn.NLLLoss()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>In the first line, we create a stochastic gradient descent optimizer, and we specify the learning rate (which I&#8217;ve passed to this function as 0.01) and a momentum of 0.9. The other ingredient we need to supply to our optimizer is all the parameters of our network &#8211; thankfully PyTorch make supplying these parameters easy by the .parameters() method of the base nn.Module class that we inherit from in the Net class.</p>
<p>Next, we set our loss criterion to be the negative log likelihood loss &#8211; this combined with our log softmax output from the neural network gives us an equivalent cross entropy loss for our 10 classification classes.</p>
<p>Now it&#8217;s time to train the network. During training, I will be extracting data from a data loader object which is included in the PyTorch utilities module. I won&#8217;t go into the details here (I&#8217;ll leave that for a future post), but you can find the code on this site&#8217;s <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">Github repository</a>. This data loader will supply batches of input and target data which we&#8217;ll supply to our network and loss function respectively. Here&#8217;s the full training code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># run the main training loop
for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)
        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)
        data = data.view(-1, 28*28)
        optimizer.zero_grad()
        net_out = net(data)
        loss = criterion(net_out, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print(&#039;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#039;.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The outer training loop is the number of epochs, whereas the inner training loop runs through the entire training set in batch sizes which are specified in the code as batch_size. On the next line, we convert <em>data</em> and <em>target</em> into PyTorch variables. The MNIST input data-set which is supplied in the <em>torchvision</em> package (which you&#8217;ll need to install using pip if you run the code for this tutorial) has the size (batch_size, 1, 28, 28) when extracted from the data loader &#8211; this 4D tensor is more suited to <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">convolutional neural network</a> architecture, and not so much our fully connected network. Therefore we need to flatten out the (1, 28, 28) data to a single dimension of 28 x 28 =  784 input nodes.</p>
<p>The .view() function operates on PyTorch variables to reshape them. If we want to be agnostic about the size of a given dimension, we can use the &#8220;-1&#8221; notation in the size definition. So by using <em>data.view(-1, 28*28)</em> we say that the second dimension must be equal to 28 x 28, but the first dimension should be calculated from the size of the original data variable. In practice, this means that <em>data</em> will now be of size (batch_size, 784). We can pass a batch of input data like this into our network and the magic of PyTorch will do all the hard work by efficiently performing the required operations on the tensors.</p>
<p>On the next line, we run <em>optimizer.zero_grad()</em> &#8211; this zeroes / resets all the gradients in the model, so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, but in PyTorch you have to remember to do it explicitly. Let&#8217;s single out the next two lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">net_out = net(data)
loss = criterion(net_out, target)</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line is where we pass the input data batch into the model &#8211; this will actually call the <em>forward()</em> method in our Net class. After this line is run, the variable <em>net_out</em> will now hold the log softmax output of our neural network for the given data batch. That&#8217;s one of the great things about PyTorch, you can activate whatever normal Python debugger you usually use and instantly get a gauge of what is happening in your network. This is opposed to other deep learning libraries such as TensorFlow and Keras which require elaborate debugging sessions to be setup before you can check out what your network is actually producing. I hope you&#8217;ll play around with how useful this debugging is, by utilizing the code for this PyTorch tutorial <a href="https://github.com/adventuresinML/adventures-in-ml-code" target="_blank" rel="noopener">here</a>.</p>
<p>The second line is where we get the negative log likelihood loss between the output of our network and our target batch data.</p>
<p>Let&#8217;s look at the next two lines:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">loss.backward()
optimizer.step()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>The first line here runs a back-propagation operation from the loss Variable backwards through the network. If you compare this with our review of the .backward() operation that we undertook earlier in this PyTorch tutorial, you&#8217;ll notice that we aren&#8217;t supplying the .backward() operation with an argument. Scalar variables, when we call .backward() on them, don&#8217;t require arguments &#8211; only tensors require a matching sized tensor argument to be passed to the .backward() operation.</p>
<p>The next line is where we tell PyTorch to execute a gradient descent step based on the gradients calculated during the .backward() operation.</p>
<p>Finally, we print out some results every time we reach a certain number of iterations:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">if batch_idx % log_interval == 0:
    print(&#039;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#039;.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This print function shows our progress through the epochs and also gives the network loss at that point in the training. Note how you access the loss &#8211; you access the Variable .data property, which in this case will be a single valued array. We access the scalar loss by executing loss.data[0].</p>
<p>Running this training loop you&#8217;ll get an output that looks something like this:</p>
<blockquote><p>Train Epoch: 9 [52000/60000 (87%)] Loss: 0.015086</p>
<p>Train Epoch: 9 [52000/60000 (87%)] Loss: 0.015086</p>
<p>Train Epoch: 9 [54000/60000 (90%)] Loss: 0.030631</p>
<p>Train Epoch: 9 [56000/60000 (93%)] Loss: 0.052631</p>
<p>Train Epoch: 9 [58000/60000 (97%)] Loss: 0.052678</p></blockquote>
<p>After 10 epochs, you should get a loss value down around the &lt;0.05 magnitude.</p>
<h2>Testing the network</h2>
<p>To test the trained network on our test MNIST data set, we can run the following code:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code"># run a test loop
test_loss = 0
correct = 0
for data, target in test_loader:
    data, target = Variable(data, volatile=True), Variable(target)
    data = data.view(-1, 28 * 28)
    net_out = net(data)
    # sum up batch loss
    test_loss += criterion(net_out, target).data[0]
    pred = net_out.data.max(1)[1]  # get the index of the max log-probability
    correct += pred.eq(target.data).sum()

test_loss /= len(test_loader.dataset)
print(&#039;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#039;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>This loop is the same as the previous training loop up until the <em>test_loss</em> line &#8211; here we extract the network loss using the .data[0] property as before, but all in the same line. Next, we have the <em>pred</em> line, where the data.max(1) method is used &#8211; this .max() method can return the index of the maximum value in a certain dimension of a tensor. Now, the output of our neural network will be of size (batch_size, 10), where each value of the 10-length second dimension is a log probability which the network assigns to each output class (i.e. it is the log probability of whether the given image is a digit between 0 and 9). So for each input sample/row in the batch, net_out.data will look something like this:</p>
[-1.3106e+01, -1.6731e+01, -1.1728e+01, -1.1995e+01, -1.5886e+01, -1.7700e+01, -2.4950e+01, -5.9817e-04, -1.3334e+01, -7.4527e+00]
<p>&nbsp;</p>
<p>The value with the highest log probability is the digit that the network considers to be the most probable given the input image &#8211; this is the best prediction of the class from the network. In the example of net_out.data above, it is the value -5.9817e-04 which is maximum, which corresponds to the digit &#8220;7&#8221;. So for this sample, the predicted digit is &#8220;7&#8221;. The .max(1) function will determine this maximum value in the second dimension (if we wanted the maximum in the first dimension, we&#8217;d supply an argument of 0) and returns both the maximum value that it has found, and the index that this maximum value was found at. It therefore has a size of (batch_size, 2) &#8211; in this case we are interested in the index where the maximum value is found at, therefore we access these values by calling .max(1)[1].</p>
<p>Now we have the prediction of the neural network for each sample in the batch determined, we can compare this with the actual target class from our training data, and count how many times in the batch the neural network got it right. We can use the PyTorch .eq() function to do this, which compares the values in two tensors and if they match, returns a 1. If they don&#8217;t match, it returns a 0:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">correct += pred.eq(target.data).sum()</code></pre> <div class="code-embed-infos"> </div> </div>
<p>By summing the output of the .eq() function, we get a count of the number of times the neural network has produced a correct output, and we take an accumulating sum of these correct predictions so that we can determine the overall accuracy of the network on our test data set. Finally, after running through the test data in batches, we print out the averaged loss and accuracy:</p>
<div class="code-embed-wrapper"> <pre class="language-python code-embed-pre line-numbers"  data-start="1" data-line-offset="0"><code class="language-python code-embed-code">test_loss /= len(test_loader.dataset)
print(&#039;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#039;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre> <div class="code-embed-infos"> </div> </div>
<p>After training the network for 10 epochs, we get the following output from the above code on the test data:</p>
<blockquote><p>Test set: Average loss: 0.0003, Accuracy: 9783/10000 (98%)</p></blockquote>
<p>A 98% accuracy &#8211; not bad!</p>
<p>So there you have it &#8211; this PyTorch tutorial has shown you the basic ideas in PyTorch, from tensors to the autograd functionality, and finished with how to build a fully connected neural network using the nn.Module. I hope it was helpful. If you&#8217;d like to learn more about PyTorch, check out my post on <a href="http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/" target="_blank" rel="noopener">Convolutional Neural Networks in PyTorch</a>.</p>
<hr />
<p><strong>Recommended online course: </strong>If you&#8217;re more of a video course learner, check out this inexpensive, highly rated, Udemy course: <a href="https://click.linksynergy.com/link?id=Jbc0N5ZkDzk&amp;offerid=323058.1259546&amp;type=2&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fpractical-deep-learning-with-pytorch%2F" target="new">Practical Deep Learning with PyTorch</a><img src="https://ad.linksynergy.com/fs-bin/show?id=Jbc0N5ZkDzk&amp;bids=323058.1259546&amp;type=2&amp;subid=0" width="1" height="1" border="0" /></p>
<hr />
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/">A PyTorch tutorial &#8211; deep learning in Python</a> appeared first on <a rel="nofollow" href="http://adventuresinmachinelearning.com">Adventures in Machine Learning</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
